# AI处理器架构与编程



## 从计算机科学角度

- 研究神经网络的计算特性以及计算实现
- 卷积神经网络
  - 结构定义
  - 网络模型训练
  - 推理计算
- 卷积层
  - 局部连接
  - 权重分享
  - 计算办法
    - 直接卷积
    - 矩阵乘法实现卷积：img2col
  - 优化加速
    - 突出并行
    - 神经元并行
    - 输入特征图并行
    - 输出特征图
    - 批处理并行



## 特定领域架构（DSA）

- 神经网络芯片加速理论
- GPU加速理论
- GPU加速神经网络的模式
  - 通用矩阵相乘
  - 核心：将计算展开成矩阵运算的形式



## 推理优化

- 计算图优化：和硬件无光的优化
  - 内存分配：模型参数、输入输出数据、中间表示
    - 静态分配：caffe
    - 动态分配：tensorflow
    - 指定批量的大小：roofline model
    - 通过精细的内存管理实现内存复用
    - 尽可能消除模型中的控制逻辑
    - 合理使用模型中的并行计算
  - 内核融合
    - 对计算图进行一些基本的优化，包括常量值计算、表达式优化等
    - 检测出计算图中可以融合的节点
    - 为给定的融合子图生成相应的内核的函数代码
    - 用融合的内核直接替换原计算图中的子图
  - 计算调度
- 内核优化
  - 数据排布
  - 循环展开
  - 硬件相关的优化



-------------------

## 深度学习对嵌入式的优化

算法软件优化

- 网络结构优化：基于一个初始版本，对网络结构进行调整，某些层的修改，参数的调整，使得它能够在不降低精度的情况下速度更快。
- 模型压缩剪枝：把一些不必要的分支给砍掉，在进行一个预测的时候，计算量相对会减少一些，速度变快。
- 定点化、二值化：深度学习模型的参数都是浮点数，相对来说它的计算比整数要复杂一些，特别在一些低端的芯片，乘法器都不够多的情况下，浮点的性能就会比较差。如果把它转成定点整数运算，那么在精度下降 1% 的情况下，它的速度将会带来几倍的提升。
  二值化比定点化更进一步，一个权重值只占用一个比特，并且可以将乘法运行转换为异或操作，在特定硬件上并行性会更高，执行速度会更快，非常适合在低端芯片上使用。
- SIMD，缓存，多线程：SIMD, 单指令多数据，一次一条指令做多个操作，增加缓存命中，减少内存访问。一些不同的算法如果放在不同的线程中去跑，对外提供的整体组合的效果会非常的快。
- 异构计算：与硬件相关比较大，根据我选择的不同的硬件、不同的方案、定制化指令的不同，硬件选择都会接触到异构计算。将高性能服务器上运行的算法，迁移到嵌入式平台实时运行，其难度非常大，除了算法软件层面的优化，还需要充分利用硬件提供的计算能力。在硬件选择方面，更是需要选取最适合的方案才能搭配出最优的性价比。

硬件选择

- 未来趋势：ASIC 专用芯片
- 价格和功耗过高：基于 GPU 的方案
- 价格和功耗过高：基于 FPGA 的方案
- ARM+DSP：基于 DSP 的方案
- 纯 CPU 方案

