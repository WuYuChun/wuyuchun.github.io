# 异构处理器OpenCL编程导论



## 前言

计算机科学中"并行处理"的定义:由若干处理单元协作完成某一计算任务,并且这些动作的完成过程一般在时间上有所重叠.



## GPU计算的发展历程

> 易于编程的高性能并行处理器体系结构和并行编程模型是计算机科学与技术史上面临的最大挑战之一-----<<量化计算机体系结构>>

GPU通用计算的典型应用:

- 油气勘探
- 教育科研
- 国防
- 生命科学
- 金融
- 制造业





## 现代图形处理器的体系结构

![](/images/posts/2020-11-21-OpenCL01.png)

![](/images/posts/2020-11-21-OpenCL02.png)



![](/images/posts/2020-11-21-OpenCL03.png)

图像处理的显著特点:

- 高度充分的内在并行性
- 高度规则的存储器访问模式

![](/images/posts/2020-11-21-OpenCL04.png)











## 异构系统体系结构和融合处理器

实现完美的融合处理需要解决一系列问题:

1. CPU和GPU的地址空间
2. 资源管理
3. 数据传输

异构系统体系结构的目标:**将多种异构计算资源整合在一个"一致"的处理环境中.**多种计算资源能够以统一方式进行编程和调度,即多个计算资源没有主从之分,可以有操作系统统一调度或者相互调用.多个处理器能够以统一和不矛盾方式看到存储空间.

HSA(异构系统体系结构)的目标:

1. 支持高效能计算
2. 改善异构处理器的可编程性
3. 最大化代码可移植性
4. 极大化可推广性



![](/images/posts/2020-11-21-OpenCL05.png)





## OpenCL基本概念



OpenCL与其他常见的CPU编程语言最大的区别就是引入了编程平台(platform)和设备(device)的概念.OpenCL要解决是跨平台多设备的兼容性问题.为了能够实现同一段代码在不同设备上的通用性,采用**运行时(runtime)**编译的策略.

程序员需要在程序中选定设备供应商所提供的OpenCL平台实现创建上下文,**OpenCL运行时调用平台的编译工具将Opencl代码编译成能够在设备上执行的机器码**

![](/images/posts/2020-11-21-OpenCL06.png)



```c++
_kernel void vec_add(_global const float *a, _global const float *b, _global float *c, const unsigned int n){
    int id = get_global_id(0); //获取全局线程索引值
    if(id < n){
        c[id] = a[id]+b[id];
    }
}
```

```c++
int main(int argc ,char *argv[]){
    cl_context context = 0;
    cl_command_queue commandQueue = 0;
    cl_program program = 0;
    cl_device_id device = 0;
    cl_kernel kernel = 0;
    cl_mem memObjects[3]={0,0,0};
    cl_int errNum;
    
    //在第一个可用的平台上建立OpenCLC的上下文
    context = CreateContext();
    if(context == NULL){
        cerr << "faile to create context!\n";
        return -1;
    }
    
    //在第一个可用设备上创建一个命令队列
    //将命令队列建立在刚刚创建的上下文中
    commandQueue = CreateCommandQueue(context,&device);
    if(commandQueue == NULL){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //使用helloWord.cl中kernel函数创建OpenCLC程序
    program = CreateProgram(context,device,"HelloWorld.cl");
    if(commandQueue == NULL){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //生成OpenCL kernel对象
    kernel = clCreateKernel(program,"hello_kernel",NULL);
    if(commandQueue == NULL){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //创建用来传递给kernel函数作为参数的存储对象
    //首先分配主机存储kernnel函数所需要参数的内存空间
    float result[ARRAY_SIZE];
    float a[ARRAY_SIZE];
    float b[ARRAY_SIZE];
    for(int i= 0; i < ARRAY_SIZE; ++i){
        a[i] = i;
        b[i] = i*2;
    }
    
    memObjects[0] = clCreateBUffer(context,CL_MEM_READ_ONLY,ARRAY_SIZE*sizeof(float),NULL,NULL);
    memObjects[1] = clCreateBUffer(context,CL_MEM_READ_ONLY,ARRAY_SIZE*sizeof(float),NULL,NULL);
    memObjects[2] = clCreateBUffer(context,CL_MEM_READ_ONLY,ARRAY_SIZE*sizeof(float),NULL,NULL);
    
    //将主机端的输入数据复制到设备端存储对象中
    errNum = clEnqueueWriteBUffer(commandQueue,memObjects[0],CL_TRUE,0,,ARRAY_SIZE*sizeof(float),a,0,NULL,NULL);
    errNum = clEnqueueWriteBUffer(commandQueue,memObjects[0],CL_TRUE,0,,ARRAY_SIZE*sizeof(float),b,0,NULL,NULL);
    if(errNum != CL_SUCCESS){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //为kernel函数设置参数
    errNum = clSetKernelArg(kernel,0,sizeof(cl_mem),&memObjects[0]);
    errNum = clSetKernelArg(kernel,1,sizeof(cl_mem),&memObjects[1]);
    errNum = clSetKernelArg(kernel,2,sizeof(cl_mem),&memObjects[2]);
    if(errNum != CL_SUCCESS){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //设置kernel函数的执行尺寸
    size_t globalWorksize[1]={ARRAY_SIZE};
    size_t localWorkSize[1] = {1};
    
    //将kernel函数提交到命令队列,对存储对象中数据进行操作
    errNum = clEnqueueNDRangeKernel(commandQueue,kernel,1,NULL,globalWorksize,loacalWorksize,0,NULL,NULL);
    if(errNum != CL_SUCCESS){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //将kernel的函数输出数据复制到主机端
    errNum = clEnqueueReadBuffer(commandQueue,memObjects[2],CL_TRUE,0,ARRAY_SIZE*sizeof(float),result,0,NULL,NULL);
    if(errNum != CL_SUCCESS){
        Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
        return -1;
    }
    
    //输出结果
    for(int i = 0; i <ARRAY_SIZE; ++i ){
        cout << reuslt[i] << " ";
    }
    Cleanup(context,commandQueue, program, kernel,memObjects); //清空所有已经创建的OpenCL对象
    return 0;
}
```

```c++
cl_context CreateContext(){
    cl_init errNum;
    cl_uint numPlatforms;
    cl_paltform_id fristPlatformId;
    cl_context context = NULL;
    
    //首先选择OpenCL平台,在选择系统中第一个可用平台
    //也可以列出所有平台,然后选择合适平台
    errNum = clGetPlatfromIDs(1,&firstPlatforId,&numPlatforms);
    
    //接下来,在平台上建立一个OpnenCL上下文
    //优先选择GPU平台上下文,如果没有,选择基于CPU的上下文
    cl_context_properite contextProperties[] = {
        CL_CONTEXT_PALTFORM,
        (cl_context_properites)firstPlatformId,
        0
    };
    context = clCreateContextFromType(contextProperties,CL_DEVICE_TYPE_GPU,NULL,NULL,&errNum);
    if(errNum != CL_SUCCESS){
        context = clCreateContextFromType(contextProperties,CL_DEVICE_TYPE_CPU,NULL,NULL,&errNum);
        if(rrNum != CL_SUCCESS){
            return NULL;
        }
    }
    return context;
}
```

```c++
cl_command_queue CreateCommandQueue(cl_context context,cl_device_id *device){
    cl_int errNum;
    cl_device_id *device;
    cl_command_queue commandQueue=NULL;
    size_t deviceBufferSize = -1;
    
    //首先获取保存设备ID上所需的存储空间大小
    errNum = clGetContextInfo(context,CL_CONTEXT_DEVICES,0,NULL,&deviceBufferSize);
    if(errNum != CL_SUCCESS){
        return NULL;
    }
    
    if(deviceBufferSize <= 0){
        return NULL;
    }
    
    //为保存设备ID的指针分配内存
    device = new cl_device_id[deviceBufferSize/sizeof(cl_device_id)];
    errNum = clGetContextInfo(context,CL_CONTEXT_DEVICES,deviceBufferSize,devices,NULL);
    if(errNum != CL_SUCCESS){
        return NULL;
    }
    
    //本程序选取第一个可用设备,实际应用中可以根据设备性能进行选择
    commandQueue = clCreateCommandQueue(context,devices[0],0,NULL);
    if(commandQueue == NULL){
        return NULL;
    }
    
    *device = devices[0];
    delete[] devices;
    
    return commandQueue;
}
```

```c++
cl_program CreateProgram(cl_context context, cl_device_id device,const char *fileName){
    cl_int errNum;
    cl_program program;
    
    //读入代码文件
    ifstream kernelFile(file,ios::in);
    ostringstream oss;
    oss << kernelFile.rdbuf();
    string strStdStr = oss.str();
    const char *srcStr = stcStdStr.c_str();
    program = clCreateProgramWithSource(context,1,(const char**)&srcStr,NULL,NULL);
    errNum = clBuildProgram(program,0,NULL,NULL,NULL,NULL);
    if(errNum != CL_SUCCESS){
        return NULL;
    }
    return program;
}
```





## OpenCL并行编程基础



OpenCL并行编程的核心概念:并行线程组织方式.**网格----工作组---工作项---波前(wavefront, 宽度由硬件决定**



存储器模型:

1. 私有存储器
2. 局部存储器
3. 常数存储器
4. 全局存储器



数据类型:

- 抽象数据类型:
  - cl_platform_id
  - cl_device_id
  - cl_context
  - cl_command_queue
  - cl_mem
  - cl_program
  - cl_kernel
  - cl_event
  - cl_sampler



运算符



函数

```c++
_kernel void mat_mul(const int M, const int N, const int K, const _global float* A, const _global float *B, _global float *C){
    const int i = get_global_id(0);//获取当前线程处理元素的行号
    const int j = get_global_id(1);//获取当前线程处理元素的列号
    float acc = 0.0f;
    for(int k = 0; k < K;++k){
        acc += A[k*M+i]*B[j*K+k];
    }
    C[j*M+i] = acc;
}
```









## OpenCL事件和队列

命令是Opencl的基本执行单元,包括核函数的执行,数据的读写操作等.

多个队列



**事件是与某条命令(对应某项任务)相关连的数据对象,表征该命令的状态**

- 该命令进入队列
- 命令已经被主机提交到该队列对应的设备上
- 设备正在执行该命令
- 该命令执行完毕
- 负的返回值,表示命令执行过程中出现错误了.



## OpenCL2.0高级特征

共享虚拟存储器

管道

嵌套并行

工作组函数

通用地址空间



## 并行程序设计方法

并行程序设计的复杂性:

- 检查并行程序的正确性非常困难
- 预测并行程序性能非常困难,由于缺乏全局程序性能模型,准确预测几乎是不可能的,只能通过大量的尝试和比较才能确定恰当的方案.
- 并行程序设计往往需要程序员直接操作具体硬件细节才能充分发挥硬件的性能.
- 并行程序设计时影响性能的因素极多,需要在众多因素中根据目标应用的特点进行选择.



程序性能剖析:

在以执行时间为优化目标的程序设计过程中,性能剖析是很重要的环节.

- [ ] 程序是否产生正确结果?测试有严重错误的程序是没有意义的
- [ ] 运行程序的机器的负载是否很高?在多个任务同时运行的情况下,很难确定某一程序的真正性能
- [ ] 编译时是否选择了正确的编译开关?
- [ ] 是否使用恰当的输入数据和执行配置?
- [ ] 执行时间是否可以复制?如果机器负载不高,但是执行时间波动很大,则表明程序具有某些不确定因素
- [ ] 室友要求顺序和并行代码的输出结果一致?



解读性能剖析结果

应用程序表现出异常丰富的模式,表现在性能剖析结果上可能出现以下情况:

- 平塔型:所有代码平等分摊执行时间
- 起伏型:多个代码端占据大部分执行时间
- 热点型:少量代码贡献比如超过80%的执行时间
- I/O主导型:大部分时间用来I/O操作



### 并行化设计方法学

1. 划分:既可以针对数据划分,也可以针对功能划分.并行性的划分决定了理论上有多少并行性可以利用.
2. 通信:确定任务间的通信方式.包括任务之间的数据传递,也包括任务之间的同步操作,还包括任务执行过程中对存储器的访问.
3. 聚集
4. 映射:前面划分\通信\聚集是对并行性进行发掘和组织,这步将并行工作分配到不同的处理器上,分配的目标是最大化程序性能.





