

[TOC]



# 简介

这里主要介绍TensorRT的定点化原理

本文引用之

[量化原理]: https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&amp;mid=2649038805&amp;idx=1&amp;sn=a39f593e10cdd9de6b968ca42ca6859f&amp;chksm=8712a1a8b06528bed3aab6e6fa8f4c2015997d01a5a5d04abb4f7cc7b665853cc7305a5b638f&amp;scene=21#wechat_redirect

tensorRT中的8bit属于对称量化方案

> 何为对称量化方案？

TensorRT[5]通过**最小化原始数据分布和量化后数据分布之间的KL散度来对激活值进行量化**，将FP32降为INT8的操作如下：
$$
FP32(T) = scalefactor(s) * 8-bit(t) +FP32bias(b)
$$
实验证明偏置实际上是不需要的，去掉偏置后就是T=s∗t。其中s就是比例因子（scaling factor），因此现在的问题就是如何确定比例因子，一个简单的示意图如下：

![](/images/posts/2020-06-28-10-31-30-tensorRT-bit1.png)

上图直接将一个tensor中的-|max|和|max|的FP32值线性映射为-127和127，这样的映射是对称的，并且是不饱和的，实验结果显示这样做会导致比较大的精度损失。主要的原因是max值可能会存在一些离散点噪声，如果直接进行线性缩放，可能就放大了这些噪声，TensorRT的改进做法是从127和|max|之间选择好一个阈值T，把大于这个阈值T的部分截断，示意图如下：

![](/images/posts/2020-06-28-10-32-55-tensorrt-1bit-2.png)

只要阈值选取得当，就能将不稳定的大激活值舍弃掉，并且使精度损失不至于降低太多。原始的数据是一个FP32的分布，现在我们要用INT8的分布来表达这个tensor，根据阈值|T|的取值不同有多种选择，只需要选择其中最接近FP32的即可，所以现在的问题是确定|T|。

我们需要一个衡量指标来**衡量不同的INT8分布与原来的FP32分布之间的差异程度，这就是相对熵，或者称为KL散度，它是衡量两个分布相似度的经典算法**，定义如下：
$$
KL-divergence(P,Q):= SUM(P[i]*log(P[i]/Q[i]),i)
$$
P，Q分别称为参考分布(reference_distribution)、量化分布(quantize_distribution)。**在应用的时候，每一层的|T|值都不同，确定每一层的|T|值的过程被称为校准(Calibration)。**

所以完整的流程如下：

![](/images/posts/2020-06-28-10-35-21-tensorRT-1bit-3.png)



1. 首先将FP32的模型在一个数据集(Calibration Dataset)上跑一遍记录下每一层的FP32激活值，这里没必要去跑整个训练集，比较现实的做法是从验证集中选取一个子集，当然它最好有代表性，多样性好。之后对网络的每一层收集激活值，得到直方图，bins的大小官方建议为2048，所以阈值就在128和2047之间。
2. 对于128和2047之间的不同阈值|T|进行遍历，选取使得KL散度取得最小值的|T|。对每一层计算出|T|，得到一个所谓的CalibrationTable。把0-T组的数值线性映射到0-128之间，超出T的直接映射到128。















