# 实战Google深度学习框架

[代码]: https://github.com/caicloud/tensorflow-tutorial
[demo]: https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650722700&amp;idx=1&amp;sn=4e14edc4b11cda185cb0788e197118a5



## 人工智能、机器学习、深度学习

- 深度学习解决核心问题之一：自动将简单的特征组合成 更复杂的特征
- 首个深度学习算法：AlexNet



## 神经网络

- 定义
- 如何训练神经网络
  - 权重
  - 偏差
- 为什么对于分类问题，softmax是一个不错的激活函数
- 梯度下降
- 为什么对于分类问题，交叉熵更有效
  - 交叉熵是一个关于权重、偏置、训练图像的像素和其已知标签的函数



## 环境搭建

- 主要依赖包：protocol buffer
  - 序列化：将结构化的数据变成数据流的格式
  - 与xml、json的区别
    - protocol buffer序列化之后的数据是不可读字符串，二进制流
    - xml\json不需要任何额外的信息就能还原数据
- 主要依赖包：Bazel
  - 自动化构建工具



## 深度学习基础框架

- 2013:层结构模型：Caffe\Theano 
- 2015:操作结构模型：Tensorflow、caffe2
- 2016:没有模型：Tensorflow、PyTorch



## 入门



- 计算模型
  - 计算图
    - 隔离张量和计算
    - 管理张量和计算的机制
      - 指定运行计算设备，GPU
    - 整理程序中资源
      - 张量：变量也是一种张量
      - 变量：神经网络优化算法将GraphKeys.Trainable_variables集合中的变量作为默认的优化对象
      - 队列资源
  - 每一个计算都是计算图中的一个节点
  - 节点之间的边描述计算之间的关系
  - 不同计算图上的张量和运算都不会共享
- 数据模型
  - 张量：管理数据的形式
    - 张量中并没有保存真正的数字，保存的是如何得到这些数据的计算过程
    - 保存三个属性：名字、维度、类型
    - 使用总结
      - 对中间计算结果的引用
      - 当计算图构造完成，张量可以用来获取计算结果
- 运行模型
  - 会话
    - 执行定义好的计算，管理运行所需要的资源
    - 使用方式
- 程序以便分为两个步骤
  - 定义计算图中所有的计算
  - 执行计算
- 前向传播算法
  - 不同的神经网络结构前向传播方式也不一样
  - 神经元的结构
  - 三个信息
    - 神经网络的输入
    - 神经网络的链接结构
    - 每一个神经元的参数：实现分类或者回归问题的重要部分
    - 变量在使用前，初始化过程必须明确调用
- 训练过程
  - 使用placeholder机制提供输入数据
    - 数据类型需要指定
    - 数据的维度信息可以不指定，通关数据进行推导
  - 定义神经网络的结构和前向传播的输出结果
  - 定义损失函数以及选择反向传播优化的算法
  - 生成会话，并在训练数据上反复运行反向传播算法
- 变量管理
  - 神经网络复杂，那么变量也会变成很多，如何管理
  - 提供通过变量名称来创建或获取一个变量的机制
  - 通过这样的机制，在不同的函数可以通过变量的名字来使用变量，而不是参数的形式



## 基本概念

- 变量
  - 为训练算法为你确定的所有参数：例如权重和偏差
- 占位符
  - 在训练期间填充实际数据的参数：例如训练图像
- 模型
- 损失函数
  - 交叉熵
- 适应器
  - 最小化损失函数
  - 梯度下降
- 计算图
  - 延时执行：为分布式计算构建，必须知道你要计算的是什么
  - 执行图才开始发送计算任务到各种计算机
- API





## 深层神经网络

- 一类通过多层非线性变换对高复杂性数据建模算法的合集
  - 多层
  - 非线性
- 激活函数实现去线性化
- 损失函数
  - 神经网络模型的效果以及优化目标是通过损失函数来定义的
  - 交叉熵
    - 交叉熵刻画两个概率分布之间的距离
    - 然而神经网络的输出并不一定是概率分布
      - 如何将神经网络前向传播得到的结果变成概率分布
      - softmax
  - 自定义损失函数
  - 对于相同的神经网络，不同的损失函数会对训练得到的模型产生重要的影响
  - 只有该函数为凸函数的时候，梯度下降算法才能保证达到全局最优解
- 神经网络优化算法
  - 梯度下降算法:用于优化单个参数的取值
  - 反向传播算法:给出一个高效的方式在所有的参数上使用梯度下降算法
  - 优化过程分为两个阶段
    - 通过前向传播算法计算预测值，并于真实值进行比较
    - 通过反向传播算法计算损失函数对每一个参数的梯度，更新参数
  - 学习率设置:指数衰减法
  - 过拟合问题
    - 正则化手段
      - 思想：在损失函数中加入刻画模型复杂程度的指标
      - L1正则化
        - 会有更多的参数变为0（参数变得更稀疏）
        - L1正则化公式不可导
      - L2正则化
        - 不会使得参数变得稀疏
        - L2正则化公式可导
      - 一般来说，模型的复杂度只有权重决定
      - 希望通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音
  - 滑动平均模型



## 如何构建卷积网络

- 如果采用大量数据以及各种过拟技术后，性能一直提升不上，说明当前的神经网络已经无法从你提供的数据抽取更多的信息
- 卷积网络可以利用形状的信息
- 与全链接层神经网络的最大区别：卷积网络的每一个神经元重复使用相同的权重，而不是每一个神经元都有自己的权重
- 如何做卷积计算：
  - 卷积计算：就是两个（多维）矩阵相乘，相加的和，彩色图像是一个三维矩阵，那么相乘的矩阵也是一个三维矩阵，和就是一个数



## 训练提示和技巧

- 过拟合
- dropout
- 学习速率衰减



## 模型持久化

- 如何模型持久化
  - tf.train.saver:保存类
  - xxx.ckpt.meta:保存计算图的结构
  - xxx.ckpt.data-000\cpkt.inde:保存变量的值
  - checkpoint:保存目录下所有的模型文件列表
- 如何加载模型
  - tf.train.saver
  - 保存时候变量的名称和加载的变量的名称不一致问题
    - 通过字典的方式
    - 方便使用平均滑动模型:在Tensorflow中，每一个变量的滑动平均值是通过影子变量维护的，所以要获取变量的滑动平均值实际上就是获取这个影子变量的值。如果在加载模型时直接将影子变量映射到变量自身，那么在使用训练好的模型就不需要在调用函数获取变量的滑动平均值。
- 持久化原理以及数据格式
  - 元图:MetaInfoDef
  - 参数的变量



## 图像识别与卷积神经网络

- 卷积神经网络

  - 卷积神经网络和全链接神经网络的唯一区别就是神经网络中相邻两层的链接方式

  - 卷积层中每一个节点的输入只是上一层神经网络的一小块

  - 卷积层:如何计算卷积层的参数个数

    - [理解]: https://blog.csdn.net/v_july_v/article/details/51812459

  - 感受野的机制：卷积计算，每一次只计算一部分

  - 共享机制：卷积核是固定的，说明参数也是固定的

  - 卷积神经网络结构设计的一些模式

  - inception-v3模型：

    - 常见模型结构是不同卷积层通过串联的方式链接在一起
    - 这种模型是将不同的卷积层通过并联的方式结合起来

  - 迁移学习：将一个问题上训练好的模型通过简单的调整使其适适用于一个新的问题



## 图像数据处理

- 输入数据格式：tfRecord
- 图像在存储时并不是直接记录这些矩阵的数字，而是记录经过压缩编码之后的结果
- 神经网络输入节点的个数是固定的，因此在将图像的像素作为输入提供给神经网络之前，需要将图像的大小进行统一
  - 通过算法使得新的图像尽量保存原始图像的所有信息
    - 双线性插值法
    - 最近邻居法
    - 双三次插值法
    - 面积插值法
  - 通过裁剪或者填充
  - 随机翻转训练图像是一种很常用的图像预处理方式
  - 在解决真实的图像识别问题时，一般会同时使用多种预处理方法（调整亮度、翻转、裁剪等）
- 多线程输入数据处理框架
  - 在tesorflow中，队列不仅是一种数据结构，还是一种多线程的机制
  - 队列也是计算图上有状态的节点
- 输入数据处理框架



## TensorBoard可视化

- 通过tensorflow程序运行过程中输出的日志文件可视化Tensorflow程序的运行状态
- tensorBoard和tensorflow程序跑在不同的进程中
- 边的含义
  - 实线
    - 刻画数据传输，边上箭头方向表达数据传输的方式
    - 双向：表示数据会互相修改
  - 边的粗细：表示两个节点之间传输的标量维度的总大小
  - 虚线：虚线表示计算之间的依赖关系
- 主图和辅助图
- 展示每一个节点的基本信息以及运行时消耗的时间和空间



## 如何解决深度神经网络问题

- 修正线性单元ReLU激活函数
- 使用更好的优化器
- 随机初始化
- 不定值
- 学习概率如何合适呢？
- droput













