[TOC]



# 专业的CUDA®C编程
编程的未来是异构并行编程





## 并行计算之cuda历史

本小节的关键点：

- 了解异构计算架构
- 认识到并行编程的范式转变
- 掌握GPU编程的基本要素
- 了解CPU和GPU编程之间的差异

从纯粹的计算角度来看，并行计算可以定义为一种计算形式，其中许多计算可以同时进行，其原理是大问题通常可以分为较小的问题，然后并行解决。

实际上，并行计算通常涉及两个计算技术的不同领域：

➤计算机体系结构（硬件方面）
➤并行编程（软件方面）

![](/images/posts/2020-07-29-17-14-49-cuda专家编程00.png)

分析数据依存关系是实现并行算法的一项基本技能，因为依存关系是并行性的主要抑制因素之一，而了解它们对于获得现代编程世界中的应用程序加速是必不可少的。 在大多数情况下，相关任务的多个独立链为并行化提供了最佳机会。

两种并行性：

- 任务并行
- 数据并行

**CUDA编程特别适合解决可以表示为数据并行计算的问题。** 本书的主要重点是如何使用CUDA编程解决数据并行问题。 许多处理大型数据集的应用程序可以使用数据并行模型来加快计算速度。 数据并行处理将数据元素映射到并行线程。

设计数据并行程序的第一步是跨线程对数据进行分区，每个线程对一部分数据进行处理。 **通常，有两种分区数据的方法：块分区和循环分区。 在块分区中，许多连续的数据元素被分块在一起。 每个块以任何顺序分配给单个线程，并且线程通常一次仅处理一个块。 在循环分区中，更少的数据元素被分块在一起。 相邻线程接收相邻的块，并且每个线程可以处理多个块。 为要处理的线程选择一个新的块意味着向前跳与线程数一样多的块。**



![](/images/posts/2020-07-29-17-27-26-cuda专家编程01.png)

确定如何在线程之间分配数据与该数据的物理存储方式以及每个线程的执行顺序如何紧密相关。 组织线程的方式会对程序的性能产生重大影响。

在体系结构级别上，已经取得了许多进步，以实现以下目标：

- 减少延迟

  > 延迟定义：一个操作完成的时间

- 增加带宽

  > 带宽定义：一秒钟能传输多少字节

- 增加吞吐量

  > 吞吐量定义：一秒中能支持多个操作（ops/s）

计算机体系结构也可以按其内存组织来细分，该组织通常分为以下两种类型：

➤具有分布式内存的多节点
➤具有共享内存的多处理器

**GPU代表了一个多核架构，并且几乎具有前面描述的每种并行性：多线程，MIMD，SIMD和指令级并行性。 NVIDIA为这种架构创造了单指令多线程（SIMT）一词。**

从同类系统到异构系统的转变是高性能计算历史上的一个里程碑。 同质计算使用相同处理器的一个或多个处理器
执行应用程序的体系结构。 相反，异构计算使用一套处理器体系结构来执行应用程序，将任务应用到适合它们的体系结构，从而提高性能。

CPU计算适用于控制密集型任务，而GPU计算适用于数据并行计算密集型任务。

![](/images/posts/2020-07-30-08-32-18-cuda专家编程02.png)

CUDA nvcc编译器基于广泛使用的LLVM开源编译器基础结构。 您可以使用以下命令创建或扩展支持GPU加速的编程语言：CUDA编译器SDK。

![](/images/posts/2020-07-30-08-37-49-cuda专家编程03.png)



```c++
which nvcc
-> /usr/local/bin/nvcc
```

```
ls -l /dev/nv*
crw-rw-rw- 1 root root 195,   0 7月  22 09:18 /dev/nvidia0
crw-rw-rw- 1 root root 195, 255 7月  22 09:18 /dev/nvidiactl
crw-rw-rw- 1 root root 195, 254 7月  22 09:18 /dev/nvidia-modeset
crw-rw-rw- 1 root root 236,   0 7月  22 09:18 /dev/nvidia-uvm
crw-rw-rw- 1 root root 236,   1 7月  22 09:18 /dev/nvidia-uvm-tools
crw------- 1 root root  10, 144 7月  22 09:18 /dev/nvram
```

说明只有一张显卡已经安装OK

## cuda编程模型

使用CUDA，您可以轻松地实现并行算法

编程模型提供了计算机体系结构的抽象，该体系结构充当应用程序及其在可用硬件上的实现之间的桥梁。 图2-1说明程序和编程模型实现之间重要的抽象层。 通信抽象是程序与编程模型实现之间的边界，它是通过使用特权硬件原语和操作系统的编译器或库来实现的。 该程序，用于编程模型，指示程序的组件如何共享信息并协调其活动。 编程模型提供了特定计算体系结构的逻辑视图。 通常，它是体现在编程语言或编程环境中。

在进行程序和算法设计时，您主要关注的领域是：如何分解数据和函数，以便在并行环境中运行时正确，有效地解决问题。 当您进入编程阶段时，您的关注点将转向如何组织并发线程。 在此阶段，您正在考虑逻辑级别，以确保您的线程和计算能够正确解决问题。

注意cuda的运行时配置

- kernel的线程总数
- 要用于kernel的线程的布局

调用内核时，许多不同的CUDA线程并行执行相同的计算。

以下限制适用于所有内核：
➤仅访问设备内存
➤必须具有void的返回类型
➤不支持可变数量的参数
➤不支持静态变量
➤不支持函数指针
➤展示异步行为

除了许多有用的调试工具外，还有两种非常基本但有用的方法：您可以验证您的内核代码。
首先，您可以在内核中将printf用于Fermi和更高版本的设备。
其次，您可以将执行配置设置为<<< 1,1 >>>，因此您可以强制执行内核只能运行一个块和一个线程。 这模拟了一个顺序实施。 这对于调试和验证正确的结果很有用。



### 比较应用程序性能以最大限度地提高理论上的限制

在执行应用程序优化时，重要的是确定应用与理论极限相比。 从nvprof收集的计数器可以帮助您导出应用程序的指令和内存吞吐量。 如果你将应用测量值与理论峰值进行比较，可以确定如果您的应用程序受算术或内存带宽限制。

理论算力：

?

带宽：

?

指令比例：理论算力和带宽支架的比值

> For Tesla K10, if your application issues more than 13.6 instructions for every byte accessed, then your application is bound by arithmetic performance. Most HPC workloads are bound by memory bandwidth.

![](/images/posts/2020-07-30-19-38-54-cuda专家编程04.png)

```shell
nvidia-smi -L
GPU 0: GeForce GTX 1080 (UUID: GPU-24749d2b-1f2f-c467-5369-44f864b124e4)
```

```shell
nvidia-smi -q -i 0
Process ID                  : 1900
            Type                    : G
            Name                    : /usr/lib/xorg/Xorg
            Used GPU Memory         : 18 MiB
        Process ID                  : 2031
            Type                    : G
            Name                    : /usr/bin/gnome-shell
            Used GPU Memory         : 48 MiB

```



```shell
nvidia-smi -q -i 0 -d MEMORY

==============NVSMI LOG==============

Timestamp                           : Fri Jul 31 10:19:44 2020
Driver Version                      : 440.33.01
CUDA Version                        : 10.2

Attached GPUs                       : 1
GPU 00000000:01:00.0
    FB Memory Usage
        Total                       : 8116 MiB
        Used                        : 1044 MiB
        Free                        : 7072 MiB
    BAR1 Memory Usage
        Total                       : 256 MiB
        Used                        : 7 MiB
        Free                        : 249 MiB
```

```shell
nvidia-smi -q -i 0 -d UTILIZATION

==============NVSMI LOG==============

Timestamp                           : Fri Jul 31 10:21:49 2020
Driver Version                      : 440.33.01
CUDA Version                        : 10.2

Attached GPUs                       : 1
GPU 00000000:01:00.0
    Utilization
        Gpu                         : 0 %
        Memory                      : 0 %
        Encoder                     : 0 %
        Decoder                     : 0 %
    GPU Utilization Samples
        Duration                    : 16.38 sec
        Number of Samples           : 99
        Max                         : 73 %
        Min                         : 0 %
        Avg                         : 2 %
    Memory Utilization Samples
        Duration                    : 16.38 sec
        Number of Samples           : 99
        Max                         : 18 %
        Min                         : 0 %
        Avg                         : 0 %
    ENC Utilization Samples
        Duration                    : 16.38 sec
        Number of Samples           : 99
        Max                         : 0 %
        Min                         : 0 %
        Avg                         : 0 %
    DEC Utilization Samples
        Duration                    : 16.38 sec
        Number of Samples           : 99
        Max                         : 0 %
        Min                         : 0 %
        Avg                         : 0 %

```

## cuda执行模型

➤学习各种CUDA性能指标和事件
➤探索动态并行性和嵌套执行

通常，执行模型提供了如何在特定计算体系结构上执行指令的操作视图。 CUDA执行模型公开了GPU并行体系结构的抽象视图，使您可以推理线程并发。 在第2章中，您了解了CUDA编程模型公开了两个主要的抽象：一个内存层次结构和一个线程层次结构，使您可以控制大规模并行GPU。 因此，CUDA执行模型提供了洞见，对于指令吞吐量和内存访问而言，这些见解对于编写高效的代码很有用。

SIMT和SIMD的区别

**SIMD和SIMT都通过将同一条指令广播到多个执行单元来实现并行性。主要区别在于SIMD要求向量中的所有向量元素在统一的同步组中一起执行，而SIMT允许同一线程束中的多个线程独立执行。即使warp中的所有线程都从同一程序地址一起开始，但各个线程可能具有不同的行为。**

SIMT模型包含SIMD不具备的三个关键功能：
➤每个线程都有自己的指令地址计数器。
➤每个线程都有其自己的寄存器状态。
➤每个线程可以有一个独立的执行路径。

Event和metrics的区别和定义

在CUDA分析中，事件是可计数的活动，与内核执行期间收集的硬件计数器相对应。 度量是从一个或多个事件计算得出的内核的特征。

- 大多数计数器是按流多处理器报告的，而不是整个GPU。
- 一次运行只能收集几个计数器。 一些计数器的集合是互斥的。 通常需要多次分析运行以收集所有相关计数器。
- 由于GPU执行的变化（例如线程块和扭曲调度顺序），在重复运行期间，计数器值可能不完全相同。



### 理解warp执行的本质

启动内核时，从软件角度您会看到什么？ 在您看来，**内核中的所有线程似乎都是并行运行的。 从逻辑的角度来看，这是正确的**，**但从硬件的角度来看，并非所有线程都可以在物理上同时并行执行。** 本章已经介绍了将32个线程分组为一个执行单元（warp）的概念。 现在，您将从硬件角度仔细研究warp执行，并获得有助于指导内核设计的见解。

![](/images/posts/2020-08-04-08-52-50-cuda专家编程05.png)

**逻辑视图和硬件视图**

### warp分散

执行不同指令的同一线程中的线程称为线程发散。

![](/images/posts/2020-08-04-09-34-34-cuda专家编程06.png)

Branch Efficiency：为非分支分支与总分支的比率，可以使用以下公式计算：

$$
bf=100*((branchs-divergen)/branches)
$$

```c++
__global__ void mathKernel1(float *c)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    if (tid % 2 == 0)
    {
        ia = 100.0f;
    }
    else
    {
        ib = 200.0f;
    }

    c[tid] = ia + ib;
}
__global__ void mathKernel2(float *c)
{
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    float ia, ib;
    ia = ib = 0.0f;

    if ((tid / warpSize) % 2 == 0)
    {
        ia = 100.0f;
    }
    else
    {
        ib = 200.0f;
    }

    c[tid] = ia + ib;
}
```

测试分支效率的性能

```shell
 Kernel: mathKernel1(float*)
          1                         branch_efficiency                         Branch Efficiency      83.33%      83.33%      83.33%
    Kernel: mathKernel2(float*)
          1                         branch_efficiency                         Branch Efficiency     100.00%     100.00%     100.00%

```

```shell
Device "GeForce GTX 1080 (0)"
    Kernel: mathKernel1(float*)
          1                                    branch          12          12          12          12
          1                          divergent_branch           2           2           2           2
    Kernel: mathKernel2(float*)
          1                                    branch          11          11          11          11
          1                          divergent_branch           0           0           0           0

```

### 资源分配

warp的本地执行上下文主要包括以下资源：
➤程序计数器
➤寄存器
➤共享内存

warp的阶段

SM上的warp调度程序在每个周期中选择活动的warp，并将其分配给执行单元。 主动执行的warp称为选定warp。 如果活动的warp准备好执行但当前未执行，则它是合格的warp。 如果warp尚未准备好执行，则它是停滞的warp。 如果同时满足以下两个条件，则可以执行warp：

### lancy延迟

lancy的定义

**考虑指令情况，指令可以分为两种基本类型：
➤算术指令：10-20cycle的延迟
➤内存指令：400~800cycly的延迟**

![](/images/posts/2020-08-05-09-48-38-cuda专家编程07.png)

可能想知道如何估算隐藏延迟所需的活动扭曲数。 利特尔定律可以提供一个合理的近似值。 最初是队列理论中的一个定理，它也可以应用于GPU：

$$
numsOfRequiredWarp = latency * throughput
$$
![](/images/posts/2020-08-05-09-52-44-cuda专家编程08.png)



**吞吐率表示每一个sm在每一个cycle中执行指令的数量，** **并且每一个warp执行一条指令32次**，假设吞吐率是640（每一个cycle能执行640次指令），那么一个sm需要640/32=20个warp活动

因此，需要考虑两种并行性：

- 在一个线程中指令之间的并行性
- 进程之间的并行性

对于内存操作，所需的并行度表示为隐藏内存等待时间所需的每个周期的字节数。

内存指令的吞吐率表示每一秒多少个大biytes。

如何检查显卡的显存的频率

```shell
 nvidia-smi -a -q -d CLOCK | fgrep -A 3 "Max Clocks" | fgrep "Memory"
        Memory                      : 5005 MHz

```

| gpu_model | 指令lancy | 带宽(GB/sec) | 频率     | 带宽(B/cycle)     | 并行度      |
| --------- | --------- | ------------ | -------- | ----------------- | ----------- |
| fermi     | 800       | 144          | 1.566GHZ | 144/1.566=92Bytes | 800*92=74kb |

**注意：1hz表示一秒一个cycle**，lancy * thoughtput = 并行度

假设一个thread需要4byte从显存到sm，那么74kb/4bytes=185000，需要185000个线程，那么就是579个warp，fermi有16个sm，那么一个sm就需要36个warp。

> A simple formula for calculating the required parallelism is to multiply the number of cores per SM by the latency of one arithmetic instruction on that SM. For example, Fermi has 32 single-precision floating-point pipeline lanes and the latency of one arithmetic instruction is 20 cycles, so at minimum 32 x 20 = 640 threads per SM are required to keep your device busy. However, this is a lower bound.

The registers per thread and shared memory per block resource usage can be obtained from nvcc
with the following compiler flag:

```shell
--ptxas-options=-v
```

线程块的大小配置的影响

➤小线程块：每个块中的线程太少会导致硬件限制在充分利用所有资源之前要达到的每个SM的warp数。
➤大线程块：每个块过多的线程导致减少每个线程可用每个SM硬件资源。

block数和线程数的配置指导方针

- 每一个block中的thread数量都是warp（32）的倍数
- 避免小的block大小，最少也要128或者256开始
- 根据kernel的资源要求调整block size的大小
- 保持块数远大于SM数，以向设备提供足够的并行性。
- 进行实验以发现最佳执行配置和资源使用情况。

占用率专门关注每个SM的并发线程数或扭曲数。 但是，完全占用并不是性能优化的唯一目标。 一旦内核达到一定程度的占用率，进一步增加可能不会导致性能提高。 您还需要检查许多其他因素以进行性能调整。

### 同步

两种同步

- 全局同步
- block内线程的同步

注意：没有block之间的同步

### 可扩展性

**可伸缩性是任何并行应用程序的理想功能。** 可伸缩性意味着向并行应用程序提供额外的硬件资源会相对于所添加的数量产生加速资源。 例如，如果在两个SM上运行，则CUDA应用程序可扩展到两个SM，相对于在一个SM上运行，其执行时间减少了一半。 可伸缩的并行程序有效地使用所有计算资源来提高性能。 可伸缩性意味着可以通过添加计算核心来提高性能。 串行代码本质上是不可扩展的，因为在数千个内核上运行顺序的单线程应用程序不会影响性能。 并行代码具有可伸缩性的潜力，但实际的可伸缩性取决于算法设计和硬件功能。

![](/images/posts/2020-08-08-14-38-51-cuda专家编程10.png)



### 思考（非常重要）

考虑下面的情况，对两个矩阵相加（数据量为16384个float）

```
 ./sumMatrix 32 32
sumMatrixOnGPU2D <<<(512,512), (32,32)>>> elapsed 31.655000 ms
./sumMatrix 32 16
sumMatrixOnGPU2D <<<(512,1024), (32,16)>>> elapsed 26.925000 ms
./sumMatrix 16 32
sumMatrixOnGPU2D <<<(1024,512), (16,32)>>> elapsed 27.483000 ms
./sumMatrix 16 16
sumMatrixOnGPU2D <<<(1024,1024), (16,16)>>> elapsed 27.726000 ms
```

其中第二种情况的耗时最小，按理说是性能最好的

#### 使用nvprof检查GPU利用率

```shell
/usr/local/cuda/bin/nvprof --metrics achieved_occupancy ./sumMatrix 32 32
 	Achieved Occupancy    0.676593    0.676593    0.676593
/usr/local/cuda/bin/nvprof --metrics achieved_occupancy ./sumMatrix 32 16
	Achieved Occupancy    0.778135    0.778135    0.778135
/usr/local/cuda/bin/nvprof --metrics achieved_occupancy ./sumMatrix 16 32
	Achieved Occupancy    0.780214    0.780214    0.780214
/usr/local/cuda/bin/nvprof --metrics achieved_occupancy ./sumMatrix 16 16
 	Achieved Occupancy    0.798498    0.798498    0.798498	
```

**从上面可以看出，对比第一个和第二个，占用率比第一个高，但是后面两个比第二个高，但是性能（耗时）上看并不最短**，**高的占用率并不代表最好的性能**

#### 使用nvprotf检查显存操作

```c++
c[idx]=a[idx]+b[idx];
```

对于GPU来说，这条语句含有的显存操作：load和store

这里是可以通过nvprof来检查这些操作的效率的。

先看读取数据的效率

```c++
/usr/local/cuda/bin/nvprof --metrics gld_throughput ./sumMatrix 32 32
     gld_throughput      Global Load Throughput  14.565GB/s  14.565GB/s  14.565GB/s
/usr/local/cuda/bin/nvprof --metrics gld_throughput ./sumMatrix 32 16
 	gld_throughput       Global Load Throughput  15.714GB/s  15.714GB/s  15.714GB/s
/usr/local/cuda/bin/nvprof --metrics gld_throughput ./sumMatrix 16 32
    gld_throughput       Global Load Throughput  15.448GB/s  15.448GB/s  15.448GB/s
/usr/local/cuda/bin/nvprof --metrics gld_throughput ./sumMatrix 16 16    
	gld_throughput       Global Load Throughput  16.048GB/s  16.048GB/s  16.048GB/s
```

**尽管第四种情况具有最高的负载吞吐量，但它比第二种情况慢（后者仅演示了大约一半的负载吞吐量）。 由此可见，更高的负载吞吐量并不总是等同于更高的性能。 **

gld_efficiency的含义：请求的全局负载吞吐量与所需的全局负载吞吐量之比。**它衡量应用程序的加载操作使用设备内存带宽的程度。**

```
/usr/local/cuda/bin/nvprof --metrics gld_efficiency ./sumMatrix 32 32
gld_efficiency   Global Memory Load Efficiency     100.00%     100.00%     100.00%
/usr/local/cuda/bin/nvprof --metrics gld_efficiency ./sumMatrix 32 16
gld_efficiency   Global Memory Load Efficiency     100.00%     100.00%     100.00%
/usr/local/cuda/bin/nvprof --metrics gld_efficiency ./sumMatrix 16 32
gld_efficiency   Global Memory Load Efficiency     100.00%     100.00%     100.00%
usr/local/cuda/bin/nvprof --metrics gld_efficiency ./sumMatrix 16 16
gld_efficiency   Global Memory Load Efficiency     100.00%     100.00%     100.00%
```

***您可以得出结论，线程块最里面的尺寸的大小(block.x)在性能中起着关键作用***

```shell
$ ./sumMatrix 64 2
sumMatrixOnGPU2D <<<(256,8192), (64,2) >>> elapsed 0.033567 sec
$ ./sumMatrix 64 4
sumMatrixOnGPU2D <<<(256,4096), (64,4) >>> elapsed 0.034908 sec
$ ./sumMatrix 128 4
sumMatrixOnGPU2D <<<(128,4096), (128,4)>>> elapsed 0.034786 sec
$ ./sumMatrix 128 8
sumMatrixOnGPU2D <<<(128,2048), (128,8)>>> elapsed 0.046157 sec
$ ./sumMatrix 256 2
```

**请注意，您发现的最佳执行配置没有达到最高的占用率或记录的最高负载吞吐量。 从这些测试中，您可以得出结论，没有单个指标是直接等同于改进的性能。 您需要寻找几个相关的平衡点指标以达到最佳整体效果。**

### 归约计算

![](/images/posts/2020-08-08-17-59-18-cuda专家编程09.png)



#### *思考（如何优化一个kernel函数，从哪些指标去看一个kerenel函数）*

对于归约计算和，从CPU的统计耗时来看：**43.38ms**

结论：？？？？？？？？？？？？？？？？？？？？？？？？？？？？？？

|                    | bored  | boredLess | leaved | rolling2 | rolling4 | rolling8 | Warps8 | Urolrp8 |
| ------------------ | ------ | --------- | ------ | -------- | -------- | -------- | ------ | ------- |
| 耗时               | 29.79  | 15.68     | 5.88   | 3.60     | 2.16     | 1.32     | 1.20   | 1.04    |
| 加速比             | 31%    | 64%       | 86%    | 91.7%    | 95%      | 96.95%   | 97.23% | 97.60%  |
| warp执行效率       | 75.6%  | 95.8%     | 96.3%  | 96.8%    | 97.2%    | 97.7%    | 99.6%  | 99.6%   |
| 非谓词warp执行效率 | 74.3%  | 92.7%     | 93%    | 93.7%    | 94.5%    | 95.4%    | 98.3%  | 98.2%   |
| 占用率             | 98.6%  | 98.1%     | 96.4%  | 97%      | 97.3%    | 97.4%    | 74.2%  | 73.8%   |
| 读取效率           | 25%    | 25%       | 96.1%  | 98%      | 98.7%    | 99.2%    | 99.4%  | 99.4%   |
| 写入效率           | 24.8%  | 25%       | 95.5%  | 97.7%    | 97.7%    | 97.7%    | 99.4%  | 99.4%   |
| 读带宽GB/s         | 6.634  | 13.17     | 14.11  | 20.85    | 31.52    | 51.81    | 76.56  | 81.54   |
| 写带宽GB/s         | 8.37   | 16.04     | 6.44   | 10.44    | 8.04     | 6.97     | 16.51  | 17.45   |
| inst_per—warp      | 794    | 326       | 305    | 352      | 404      | 484      | 385    | 351     |
| deviece r_带宽     | 2.73   | 4.94      | 12.67  | 19.89    | 33.18    | 53.88    | 59.76  | 69.9    |
| deviece w_带宽     | 2.28   | 4.59      | 6.12   | 9.96     | 8.49     | 7.22     | 7.89   | 9.21    |
| stall_syn          | 28.47% | 41.63%    | 51.74% | 49.36%   | 45.19%   | 40.30%   | 24.16% | 25.79%  |

注意：

- warp执行效率（warp execution efficiency)：表示warp执行的时候没有分散
- 非谓词执行效率（Not-prdicated-off warp execution):没有谓词的时候，warp执行的效率？？？？？
- 读取效率：（gst_efficiency）
- 写入效率：（gld_efficiency）
- warp执行效率：（warp_execution_efficiency）
- 非谓词warp执行效率：（warp_nonpred_execution_efficiency）
- 每一个warp执行指令（inst_per_warp：Instructions per warp）
- 读带宽（写带宽）：数值越大，在相同数量的I/O的情况下，所搬运数据的时间越少。
- Device Memory Read Throughput(dram_read_throughput)
- Device Memory Write Throughput(dram_write_throughput)

**在CUDA中循环展开可能意味着很多事情。 但是，目标仍然是相同的：通过减少指令开销并创建更多独立的指令进行调度来提高性能。结果，更多的并发操作被添加到管道中，从而导致更高的饱和度。指令和内存带宽。 这为warp调度程序提供了更多合格的warp，可以帮助隐藏指令或内存延迟**



### 动态并行

相同的内核调用语法用于在内核中启动新内核。

在动态并行中，内核执行分为两类：父级和子级。 父线程，父线程块或父网格已启动新的网格，即子网格。 父级已启动了子线程，子线程块或子网格。 子网格必须先完成，然后才能将父线程，父线程块或父网格视为已完成。 直到父级的所有子网格都完成后，父级才被视为完整。

> 这里是之前没有看到的

## cuda内存模型-全局内存



### cuda显存模型

对了编程者来说，存储有两种类别：

- 可编程的：您可以明确控制将哪些数据放置在可编程存储器中。
- 不可编程：您无法控制数据放置，并且依靠自动技术来获得良好的性能。
  - L1 cache
    - 每一个SM含有一个L1 cache
  - L2 chache
    - 所有的sm含有一个L2 cache
  - 常量内存
  - 文本内存

对于CPU中的内存中，L1 cache和L2 cache 是不可编程存储

对于GPU中的显存中，以下是可以编程的存储

1. 寄存器

   1. 如果内核使用的寄存器超出硬件限制，则多余的寄存器将溢出到本地内存。

2. 共享存储

   1. 可以通过运行时动态调整L1 cache和共享内存，如下是配置

      ```
      cudaFuncCachePreferNone: no preference (default)
      cudaFuncCachePreferShared: prefer 48KB shared memory and 16KB L1 cache
      cudaFuncCachePreferL1: prefer 48KB L1 cache and 16KB shared memory
      cudaFuncCachePreferEqual: Prefer equal size of L1 cache and shared memory,
      both 32KB
      ```

      

3. 局部存储（本地内存）

   1. “本地内存”的名称具有误导性：溢出到本地内存的值与全局内存位于相同的物理位置，因此本地内存访问的特点是高延迟和低带宽

4. 文本存储

5. 常量存储

   1. 如果warp中的每个线程都从一个不同的地址读取并且只能读取一次，则常量内存不是最佳选择，因为对常量存储器的单次读取会广播到warp中的所有线程。

6. 全局存储

   1. 优化内存事务对于获得最佳性能至关重要。 当warp执行内存加载/存储时，满足该请求所需的事务数通常取决于以下两个因素：
      1. 在该warp线程之间分配内存地址。
      2. 每个事务的内存地址对齐。
   2. 通常，满足存储器请求所需的事务越多，未使用字节被传输的可能性就越高，从而导致吞吐量效率降低。



### 如何优化全局存储以及如何最大化全局存储的带宽效率

在CPU上，可以缓存内存加载和存储。 但是，在GPU上，只能缓存内存加载操作。 内存存储操作无法缓存。



| 限定词          | 变量名         | 存储     | 范围   | 生命周期   |
| --------------- | -------------- | -------- | ------ | ---------- |
|                 | float var      | register | thread | thread     |
|                 | float var[100] | local    | thread | thread     |
| ___ shared _ __ | float var      | shared   | block  | block      |
| device          | float var      | global   | global | appication |
| constant        | float var      | constant | global | appication |

![](/images/posts/2020-08-17-10-25-52-cuda专家编程12.png)



### 设备内存的管理

申请和传输

![](/images/posts/2020-08-18-08-36-26-cuda专家编程13.png)



![](/images/posts/2020-08-18-08-46-53-cuda专家编程14.png)



### PINNING 内存的好处



### 零拷贝内存

- 在设备内存不足时利用主机内存
- 避免主机和设备之间的显式数据传输
- 提高PCIe传输速率

> 零拷贝内存的优势劣势

两种常见的异型架构

- 分离
- 合并

在集成架构中，CPU和GPU融合在单个裸片上，并在物理上共享主内存。 在这种体系结构中，零拷贝内存更可能会同时提高性能和可编程性，因为不需要通过PCIe总线进行拷贝。

对于具有通过PCIe总线连接到主机的设备的离散系统，仅在特殊情况下零复制内存才是有利的。

注意不要过度使用零拷贝内存。 从零拷贝内存读取的设备内核由于其高延迟而可能非常慢。



### UVA（统一虚拟内存）

![](/images/posts/2020-08-18-09-46-52-cuda专家编程14.png)





### Unified Memory

在CUDA 6.0中，引入了一项称为统一内存的新功能，以简化CUDA编程模型中的内存管理。 统一内存创建托管内存池，在相同的CPU和GPU上都可以访问此内存池中的每个分配内存地址（即指针）。 基础系统自动在统一环境中迁移数据主机和设备之间的内存空间。 这种数据移动对应用程序是透明的，大大简化了应用程序代码。

**UVA为系统中的所有处理器提供单个虚拟内存地址空间。 但是，UVA不会自动将数据从一个物理位置迁移到另一个物理位置。 这是统一内存独有的功能。**

对全局内存的所有访问都通过L2缓存进行。 许多访问也会通过L1缓存进行传输，具体取决于访问的类型和GPU的体系结构。 如果同时使用了L1和L2高速缓存，则通过128字节的内存事务为内存访问提供服务。 如果仅使用L2高速缓存，则通过32字节的内存事务为内存访问提供服务。 在允许将L1缓存用于全局内存缓存的体系结构上，可以在编译时显式启用或禁用L1缓存。

![](/images/posts/2020-08-18-10-16-03-cuda专家编程17.png)

当设备内存事务的首地址是用于服务事务的高速缓存粒度的偶数倍时（L2高速缓存为32字节或L1高速缓存为128字节），就会发生对齐的内存访问。 执行未对齐的负载将导致带宽浪费。

当warp中的所有32个线程都访问连续的内存块时，就会发生合并的内存访问。

**通常，您应该针对内存事务效率进行优化：使用最少的事务数来服务最大数量的内存请求。**



### 全局内存的读取（重点）

在SM中，数据将通过以下三个高速缓存/缓冲区路径之一进行流水线传输，具体取决于所引用的设备内存类型：

- L1/L2 cache
- 常量cache
- 只读cache

全局内存加载操作是否通过L1缓存取决于两个因素：

- 设备能力
- 编译选项

下面的编译选项提示L1 cache关闭

```shell
-Xptxas -dlcm=cg
```

在禁用L1缓存的情况下，所有对全局内存的加载请求都直接进入L2缓存。 当发生L2丢失时，请求由DRAM处理。 每个存储器事务可以由一个，两个或四个段进行，其中一个段为32个字节。

下面强制开启L1 cache

```shell
-Xptxas -dlcm=ca
```

设置此标志后，全局内存加载请求首先尝试命中L1缓存。 在L1未命中时，请求将转到L2。 在L2丢失时，请求由DRAM处理。 在这种模式下，加载内存请求由128字节的设备内存事务处理。

内存负载的访问模式可以通过以下组合来表征：
➤缓存与未缓存：如果启用了L1缓存，则缓存负载
➤对齐与未对齐：如果内存访问的第一个地址是32字节的倍数，则加载将对齐
➤合并与未合并：如果扭曲访问连续的数据块，则合并访存

#### cache load

> CPU L1高速缓存针对空间和时间局部性进行了优化。 GPU L1缓存专为空间而非时间局部性而设计。 频繁访问缓存的L1内存位置不会增加数据保留在缓存中的可能性。

![](/images/posts/2020-08-19 09-51-50-cuda专家编程16.png)

![](/images/posts/2020-08-19-09-52-35-cuda专家编程15.png)



![](/images/posts/2020-08-19-09-53-38-cuda专家编程18.png)



![](/images/posts/2020-08-19-09-54-28-cuda专家编程19.png)



![](/images/posts/2020-08-19-09-55-08-cuda专家编程21.png)



#### 非cached load

未缓存的负载不会通过L1缓存，而是以内存段（32字节）而不是缓存行（128字节）的粒度执行。 这些是更细粒度的负载，并且可以提高总线利用率，以解决未对齐或未经合并的内存访问。

![](/images/posts/2020-08-19-10-09-04-cuda专家编程20.png)

![](/images/posts/2020-08-19-10-10-00-cuda专家编码２２.png)

![](/images/posts/2020-08-19-10-11-26-cuda专家编程23.png)

![](/images/posts/2020-08-19-10-12-54-cua专家编程24.png)

![](/images/posts/2020-08-19-10-13-48-cuda专家编程25.png)

#### 只读缓存

只读缓存最初保留给纹理内存加载使用。 对于计算能力为3.5和更高版本的GPU，只读缓存还可以支持全局内存负载，以替代L1缓存。

**通过只读缓存加载的粒度为32个字节。 通常，这些细粒度的负载比L1缓存更适合分散读取。**

有两种方法可以通过只读高速缓存引导内存读取：
➤使用功能__ldg
➤在要取消引用的指针上使用声明限定符

一般的写法

```c++
__global__ void copyKernel(int *out, int *in) {
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	out[idx] = in[idx];
}
```

使用只读缓存

```c++
__global__ void copyKernel(int *out, int *in) {
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	out[idx] = __ldg(&in[idx]);
}
```

```
__global__ void copyKernel(int * __restrict__ out,const int * __restrict__ in) {
	int idx = blockIdx.x * blockDim.x + threadIdx.x;
	out[idx] = in[idx];
}
```



### 全局存储的写（重点）

内存存储操作相对简单。 L1高速缓存既不用于任何一个上的存储操作Fermi或Kepler GPU，存储操作仅在发送到设备之前才缓存在L2缓存中记忆。 存储以32字节的段粒度执行。 内存事务可以是一个一次两个或四个细分。 例如，如果两个地址属于同一128字节区域但不在对齐的64字节区域内，将发出一个四段交易（即，单个四段交易的效果要好于发布两个单段交易的效果。



### 结构数组vs数组结构

两种数据排布形式：

- 结构数组（AOS)
- 数组结构  (SoA)

![](/images/posts/2020-08-20-09-49-34-cuda专家编程27.png)



### 性能提升

优化设备内存带宽利用率时，有两个目标需要努力：
➤对齐并合并的内存访问可减少带宽浪费
➤足够的并发内存操作以隐藏内存延迟



#### 循环展开

> 为什么要循环展开？因为**足够的并发内存操作以隐藏内存延迟**



#### 如何最大化带宽利用率（重点）

影响设备内存操作性能的主要因素有两个：
➤有效使用在设备DRAM和SM片上存储器之间移动的字节：为避免浪费设备存储器带宽，应对齐并合并存储器访问模式。
➤并发运行中的内存操作数：可以通过以下方式最大化运行中的内存操作数：1）展开，使每个线程产生更多独立的内存访问，或者2）修改内核启动的执行配置以暴露更多的并行性 每个SM。



### 带宽

有两种带宽：

- 理论带宽
- 有效带宽

![](/images/posts/2020-08-21-09-39-39-cuda专业编程30.png)

**性能提高的原因与DRAM的并行访问有关。 要求DRAM分区为全局存储器提供服务。 设备存储器的连续256字节区域被分配给连续的分区。 使用笛卡尔坐标将线程块映射到在数据块中，全局内存访问可能不会在DRAM分区之间平均分配，并且可能会发生称为分区预占的现象。 在分区预占中，内存请求是在某些分区排队，而其他分区未使用。 因为对角坐标映射导致从线程块到它们处理的数据块的非线性映射访问不太可能属于一个分区，因此性能会提高。**

在改进合并访问时，您的重点是扭曲内的内存访问模式。 另一方面，删除分区驻留区时，您的重点是所有活动扭曲的访问模式。 对角块坐标映射是一种调整块执行顺序以避免分区驻留的方法。



## 共享内存和常量内存

根据算法的性质和相应的访问模式，非强制访问可能是不可避免的。 但是，在许多情况下，可以使用共享内存来改善全局内存合并访问。 共享内存是许多高性能计算应用程序的关键推动力。

您已经了解到GPU配备了两种类型的内存：
➤板载内存
➤片上存储器

全局内存是大型的板载内存，具有相对较高的延迟。 共享内存较小，低延迟的片上内存比全局内存提供更高的带宽。 您可以将其视为程序管理的缓存。 共享内存通常可用于：
➤块内线程通信通道
➤用于全局内存数据的程序管理的缓存
➤临时填充内存以转换数据以改善全局内存访问模式

共享内存在SM上的所有驻留线程块之间分配； 因此，共享内存是限制设备并行性的关键资源。 内核使用的共享内存越多，可能并发活动的线程块就越少。



### 共享内存的分配

可以静态或动态分配共享内存变量。

动态分配共享内存变量

```c++
extern __shared__ int tile[];
kernel<<<grid, block, isize * sizeof(int)>>>(...)
```

其中注意的地方：**请注意，您只能动态声明一维数组。**

**优化内存性能时需要衡量两个关键属性：延迟和带宽。** 第4章介绍了延迟和带宽对内核性能的影响通过不同的全局内存访问模式。 共享内存可用于隐藏性能全局内存延迟和带宽的影响。 要充分利用这些资源，有助于了解共享内存的排列方式。

#### memory bank

为了获得高存储带宽，共享内存被分为32个大小相等的存储模块，称为存储体，可以同时访问它们。

#### bank冲突

当共享内存请求中的多个地址落入同一内存库中时，将发生库冲突，从而导致请求被重播。 

通过warp发出对共享内存的请求时，会出现三种典型情况：

- 并行访问：跨多个存储库访问多个地址
- 串行访问：在同一存储库中访问多个地址
- 广播访问：在单个存储区中读取一个地址

并行访问是最常见的模式：warp访问的多个地址属于多个存储体。 这种模式意味着，即使不是全部，某些地址也可以在一个地址中提供内存事务。 最佳地，当每个地址都在单独的存储区中时，执行无冲突的共享内存访问。

串行访问是最糟糕的模式：当多个地址属于同一存储区时，必须将请求序列化。 如果warp中的所有32个线程访问单个存储体中的不同内存位置，则将需要32个内存事务，而满足这些访问所需的时间将是单个请求的32倍。

在广播访问的情况下，经线中的所有线程在单个存储体中读取相同的地址。 执行一次内存事务，并将访问的字广播到所有请求线程。 虽然广播访问只需要单个内存事务，但是带宽利用率很差，因为只能读取少量字节。

以下是并行访问

![](/images/posts/2020-08-22-14-55-59-cuda专家编程0501.png)

![并行访问](/images/posts/2020-08-22-14-57-30-cuda专家编程0502.png)

以下是另外的两种情况
➤如果线程访问bank中的相同地址，则无冲突的广播访问
➤如果线程访问bank中的其他地址，则bank冲突访问

![](/images/posts/2020-08-22-14-59-16-cuda专家编程0503.png)

公式：
$$
bank index = (byte address ÷ 4 bytes/bank) % 32 banks
$$
![](/images/posts/2020-08-24-09-27-31-cuda专家编程0504.png)

对于有些架构来说，有两种模式：

- 32bits
- 64bits

对于64bits模式，计算公式如下：
$$
bank index = (byte address ÷ 8 bytes/bank) % 32 banks
$$
对于64bits模式（每一个cycle读取64bit），就是下面的样子：从下图中可以，可以看出来，读取０和32的时候，并不会引起bank冲突，因为每一个cycle读取64位

![](/images/posts/2020-08-24-10-31-19-cuda专家编程0504.png)

下面的就是会引起bank冲突的访问：

1）这个例子虽然每一个thread访问不同的bank，也会导致冲突

![](/images/posts/2020-08-24-10-34-21-cuda专家编程0505.png)

２）不同的线程访问同一个bank中相同的地址（以8bytes，来访问）

![](/images/posts/2020-08-24-10-35-50-cuda专家编程0506.png)

３）很传统的bank冲突

![](/images/posts/2020-08-24-10-37-50-cuda专家编程0507.png)

![](/images/posts/2020-08-24-10-40-30-cuda专家编程0508.png)



#### memory padding

> 为什么要做memory padding？

memory padding是避免bank冲突的一种方法．例如如下所示：

假设你只有５个share memory bank，如果所有线程都访问bank0中的地址，这时候进行如左图，进行padding，那么变成右图，把之前的数据错开

![](/images/posts/2020-08-24-10-48-08-cuda专家编程0509.png)

填充的内存永远不会用于数据存储。**它的唯一功能是移动数据元素**，以使最初驻留在同一存储库中的数据在存储库之间分配。 结果，线程块可用的有用共享内存总量将减少。 填充后，您还需要重新计算数组索引，以确保访问正确的数据元素。

费米和开普勒都有32个bank，但bank的宽度(32bits/64bits)不同。 在这些不同体系结构上填充共享内存时，必须小心。 Fermi中的某些内存填充模式可能会导致Kepler中的bank冲突。

##### 查看和设置每个共享存储器每个bank的访问模式

```c++
  cudaSharedMemConfig config;
  cudaDeviceGetSharedMemConfig(&config);
  printf(">>debug %d\n",config);
```

结果是：

```c++
enum __device_builtin__ cudaSharedMemConfig {
     cudaSharedMemBankSizeDefault   = 0,
     cudaSharedMemBankSizeFourByte  = 1,   //4bits ,在xavier上运行的默认结果
     cudaSharedMemBankSizeEightByte = 2   //8bits
};

```

在内核启动之间更改共享内存配置可能需要隐式设备同步点。 更改共享内存库的大小不会增加共享内存的使用或影响内核的占用率，但可能会对性能产生重大影响。 较大的存储体大小可能会为共享内存访问提供更高的带宽，但可能会导致更多的存储体冲突，具体取决于应用程序的共享内存访问模式。

##### 配置共享内存的大小

每个SM具有64 KB的片上存储器。 共享内存和L1缓存共享此硬件资源。cuda提供两种方式来设置：

- 整块硬件设置
- 每一个kernel设置

```c++
 cudaFuncCache cacheConfig;
 cudaDeviceGetCacheConfig(&cacheConfig);
```

```c++
/**
 * CUDA function cache configurations
 */
enum __device_builtin__ cudaFuncCache
{
  cudaFuncCachePreferNone   = 0/**< Default function cache configuration, no preference */
  cudaFuncCachePreferShared = 1/**< Prefer larger shared memory and smaller L1 cache  */
  cudaFuncCachePreferL1     = 2/**< Prefer larger L1 cache and smaller shared memory */
  cudaFuncCachePreferEqual  = 3/**< Prefer equal size L1 cache and shared memory */
};
```

哪种模式更好取决于您在内核中使用共享内存的数量。 典型的情况是：
➤当内核使用更多共享内存时，建议使用更多共享内存。
➤当内核使用更多寄存器时，首选更多L1缓存。

```
cudaError_t cudaFuncSetCacheConfig(const void* func,enum cudaFuncCacheca cheConfig)
```

只需为每个内核调用一次此函数。 不必在每次内核启动时都重新设置片上存储器配置。

**即使L1高速缓存和共享内存位于同一片上硬件中，它们之间也有几处完全不同。 共享内存通过32个存bank进行访问，而L1缓存为通过缓存行访问。 使用共享内存，您可以完全控制存储内容和存储位置，而使用L1缓存时，数据逐出由硬件完成。**

> 通常，GPU缓存行为比CPU缓存行为更难以推断。 GPU使用不同的启发式算法逐出数据。 **在GPU上，数百个线程共享同一个L1缓存，而数千个线程共享同一个L2缓存**； 因此，数据逐出可能在GPU上更频繁且不可预测地发生。 您可以使用GPU共享内存来显式管理数据并保证SM的本地性。

#### 同步

并行线程之间的同步是任何并行计算语言的关键机制。 顾名思义，共享内存可以由一个线程块中的多个线程同时访问。 如果多个线程在没有同步的情况下修改了相同的共享内存位置，则将导致线程间冲突。 CUDA提供了几种运行时功能来执行块内同步。 通常，有两种基本的同步方法：

- 障碍
- 内存围栏

> 在障碍处，所有调用线程都等待所有其他调用线程到达障碍点。 在内存隔离区，所有调用线程都将停顿，直到对内存的所有修改对所有其他调用线程可见为止。 但是，在研究CUDA的块内屏障和内存围栏之前，了解CUDA采用的弱排序内存模型很重要。

**问题：什么是弱排序内存模型？这意味着内存访问不一定按它们在程序中出现的顺序执行**

##### 弱排序内存模型

内存隔离栅功能可确保隔离栅之前的任何内存对于隔离栅之后的其他线程可见。 根据所需的范围，有三种类型的内存围栏：块，网格或系统。

```c++
void __threadfence_block();
void __threadfence();
void __threadfence_systm();
```

#### 如何更好的使用共享内存

在设计自己的使用共享内存的内核时，您的重点应该放在以下两个概念上：
➤跨bank映射数据元素
➤从线程索引到共享内存偏移量的映射

##### 正方形的共享内存

```c++
_shared_ int tile[N][N];

tile[threadIdx.y][threadIdx.x];//两种方式
tile[threadIdx.x][threadIdx.y];//哪种比较好?
```

如何测量bank冲突?

下面就是nvprof metric

```
shared_load_transactions_per_request
shared_store_transactions_per_request
```

````shell
Invocations                               Metric Name                             Metric Description         Min         Max         Avg
Device "GeForce GTX 1080 (0)"
    Kernel: setRowReadColDynPad(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request    1.000000    1.000000    1.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request    1.000000    1.000000    1.000000
    Kernel: setRowReadCol(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request   32.000000   32.000000   32.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request    1.000000    1.000000    1.000000
    Kernel: setRowReadColDyn(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request   32.000000   32.000000   32.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request    1.000000    1.000000    1.000000
    Kernel: setColReadCol(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request   32.000000   32.000000   32.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request   32.000000   32.000000   32.000000
    Kernel: setRowReadRow(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request    1.000000    1.000000    1.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request    1.000000    1.000000    1.000000
    Kernel: setRowReadColPad(int*)
          1      shared_load_transactions_per_request    Shared Memory Load Transactions Per Request    1.000000    1.000000    1.000000
          1     shared_store_transactions_per_request   Shared Memory Store Transactions Per Request    1.000000    1.000000    1.000000

````

##### 动态共享内存分配

动态共享内存必须声明为未调整大小的1D数组

> 填充数组是避免存储区冲突的一种方法



#### 规约内存访问模式

> 如何判断一个kernel函数它的性能下限以及上限呢?
>
> **下限:写一个一般的实现,这个就是可以作为下限**
>
> **上限:写一个只有数据搬运的kernel的,因为I/O是固定的,所以这个就是最好的上限**

### 常量内存

常量内存驻留在设备DRAM中（例如全局内存），并具有专用的片上缓存。 像L1高速缓存和共享内存一样，从每个SM常量高速缓存中读取的延迟要比直接从常量内存中读取的延迟低得多。 每个SM的恒定内存缓存大小限制为64 KB。

常量内存与本书到目前为止研究的其他任何类型的内存都有不同的最佳访问模式。 最好是，warp中的所有线程都在常量内存中访问相同的位置。 warp中的线程对不同地址的访问被序列化。 因此，恒定内存读取的成本与线程束中线程读取的唯一地址的数量成线性比例。

常量内存变量存在于应用程序的整个生命周期中，可从网格中的所有线程以及主机通过运行时函数进行访问。

>  常量缓存?????

#### warp中的shuffie指令

引入shuffle指令是一种允许线程直接读取另一个线程的寄存器的机制，只要两个线程处于同一warp即可。

shuffle指令使warp中的线程能够直接彼此交换数据，而不是通过共享或全局内存。 随机播放指令的延迟比共享内存低，并且不消耗额外的内存来执行数据交换。 因此，混洗指令为应用程序提供了一种有吸引力的方式，可让应用程序快速在线程束之间交换数据。

> lane:表示warp中的一个线程

![](/images/posts/2020-09-01-09-45-07-cuda专家编程0531.png)

![](/images/posts/2020-09-01-09-49-16-cuda专家编程0504.png)

![](/images/posts/2020-09-01-09-50-24-cuda专家编程0510.png)

![](/images/posts/2020-09-01-09-52-03-cuda专家编程0512.png)

#### 在warp中共享数据

随机播放指令将应用于以下三种整数变量类型：

- 标量变量
- 数组
- 向量型变量

## stream和并发

两种级别的并发:

- kernel级别并发
- grid级别并发

grid并发的两大支柱:

1. 异步函数
2. cudastream

**从软件角度来看，不同流中的CUDA操作可以并发运行； 在物理硬件上可能并非总是如此。 根据PCIe总线争用或每个SM资源的可用性，不同的CUDA流可能仍需要互相等待才能完成。**

流有两种类型：
➤隐式声明的流（NULL流）
➤明确声明的流（非NULL流）

### stream调度

从概念上讲，所有流都可以同时运行。 但是，将流映射到物理硬件时，这并不总是现实。

管Fermi GPU支持16路并发（即一次执行多达16个网格），但最终所有流都复用到一个硬件工作队列中。 选择要执行的网格时，CUDA运行时将调度队列最前面的任务。 运行时检查任务相关性，如果该任务仍在执行，则等待该任务依赖的所有任务完成。 最后，当满足所有依赖性时，新任务将分派到可用的SM。 该单一管道可能导致错误的依赖关系。 

![](/images/posts/2020-09-03-08-52-31-cuda专家编程0602.png)

使用多个硬件工作队列（一种称为Hyper-Q的技术），在Kepler系列GPU中减少了"虚假"依赖关系。 Hyper-Q通过在主机和设备之间维持多个硬件管理的连接，允许多个CPU线程或进程同时在单个GPU上启动工作。 受Fermi虚假依赖关系限制的现有应用程序可以在不更改任何现有代码的情况下获得显着的性能提升。

![](/images/posts/2020-09-03-08-55-42-cuda专家编程0603.png)

### 流的优先级

```c++
int leastPriority(-999);
int greatestPriority(-999);
cudaDeviceGetStreamPriorityRange(&leastPriority,&greatestPriority);
printf("> LeastProiority: %d, greatestPriority: %d\n", leastPriority,greatestPriority);
```

```shell
> LeastProiority: 0, greatestPriority: -1
```

### cuda事件

CUDA中的事件本质上是CUDA流中与该流中的操作流中的某个点关联的标记。

用来测量时间,下面就是使用的模板

```c++
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);
// record start event on the default stream
cudaEventRecord(start);
// execute kernel
kernel<<<grid, block>>>(arguments);
// record stop event on the default stream
cudaEventRecord(stop);
// wait until the stop event completes
cudaEventSynchronize(stop);
// calculate the elapsed time between two events
float time;
cudaEventElapsedTime(&time, start, stop);
// clean up the two events
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

#### stream同步

两种类型:

- 阻塞stream
- 非阻塞stream

两种放入stream的方式:

- 广度优先
- 宽度优先

![](/images/posts/2020-09-04-10-21-45-cuda专家编程0605.png)

![](/images/posts/2020-09-04-10-22-37-cuda专家编程0604.png)

可以使用CUDA_DEVICE_MAX_CONNECTIONS环境变量来调整开普勒设备的并发硬件连接数（最多32个）。

以广度优先的顺序调度内核删除了错误的依赖关系。

> 为什么呢?因为硬件队列的原因

### 默认流的阻止行为

![](/images/posts/2020-09-07-09-15-56-cuda专家编程0613.png)

在图中可以看到默认流阻止后面的非空流的并发操作.

### stream的回调函数

流回调是可以排队到CUDA流的另一种操作类型。 一旦流回调之前的流中的所有操作都完成，CUDA运行时将调用由流回调指定的主机端函数。 该功能由应用程序提供。 这允许将任意主机侧逻辑插入CUDA流中。 流回调是另一种CPU-GPU同步机制。 回调之所以特别强大，是因为回调是在主机系统上创建工作的GPU操作的第一个示例，与本书中到目前为止描述的每个CUDA概念相反。

回调函数有两个限制：

- 无法从回调函数调用CUDA API函数.
- 无法在回调函数中执行同步.

```c++
cudaError_t cudaStreamAddCallback(cudaStream_t stream,cudaStreamCallback_t callback, void *userData, unsigned int flags);
```



## 调整指令级原语

两种类型的函数:

1. I/O限制级
2. 计算限制级

 **处理器的计算吞吐量可以通过其在一段时间内执行的操作数来衡量。**

但是，并非所有指令都是相同的。 如果您未收敛正确的答案或未获得预期的结果，则应用程序的运行速度并不重要。 在优化应用程序的吞吐量和正确性时，了解不同底层指令在性能，数值精度和线程安全性方面的优缺点非常重要。 了解何时将内核代码编译为一个原语或另一个原语，可以调整编译器的代码生成以适应您的要求。



### 介绍cuda指令

指令代表处理器中的单个逻辑单元。 尽管使用CUDA时直接操作指令很少见，但重要的是要了解何时从CUDA内核代码生成不同的指令，以及高级语言功能如何转换为指令。 在两个功能等效的指令之间进行选择会影响各种应用程序特性，包括性能，准确性和正确性。

三个影响指令生成的因素:

- 浮点计算
- 内在以及标准函数
- 原子计算

浮点计算对非整数值进行运算，并且会影响CUDA程序的准确性和性能。 内在函数和标准函数实现了重叠的数学运算集，但提供了不同的准确性和性能保证。 当同时从多个线程对一个变量执行操作时，原子指令可确保正确性。



#### 浮点指令

![](/images/posts/2020-09-08-09-15-56-cuda专家编程0701.png)

使用double和float有两个影响:

-  通信和计算中单精度和双精度浮点运算之间的性能差异是不可忽略的。
- 这些数值差异可能会在迭代应用程序中累积为一次迭代的不精确输出.

#### intrinsic和标准函数

|                          | 在哪里使用       | 指令数据                               |
| ------------------------ | ---------------- | -------------------------------------- |
| 标准函数(sqrt)           | 主机端或者设备端 | 编译出来的指令较多,但是精度高          |
| intrinsic函数(_dsqrt_rn) | 被设备代码调用   | 编译出的指令较少,速度较快,但是精度降低 |

标准函数和固有函数在数值精度和性能上都不同。

```c++
//这个是使用instrinsic函数汇编代码
.visible .entry _Z9intrinsicPf(
	.param .u64 _Z9intrinsicPf_param_0
)
{
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [_Z9intrinsicPf_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.f32 	%f1, [%rd2];
	lg2.approx.f32 	%f2, %f1;
	add.f32 	%f3, %f2, %f2;
	ex2.approx.f32 	%f4, %f3;
	st.global.f32 	[%rd2], %f4;
	ret;
}
```

```c++
//这个是使用标准函数汇编代码
visible .entry _Z8standardPf(
	.param .u64 _Z8standardPf_param_0
)
{
	.reg .pred 	%p<18>;
	.reg .f32 	%f<101>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd2, [_Z8standardPf_param_0];
	cvta.to.global.u64 	%rd1, %rd2;
	mov.f32 	%f17, 0f3F800000;
	cvt.rzi.f32.f32	%f18, %f17;
	add.f32 	%f19, %f18, %f18;
	mov.f32 	%f20, 0f40000000;
	sub.f32 	%f21, %f20, %f19;
	abs.f32 	%f1, %f21;
	ld.global.f32 	%f2, [%rd1];
	abs.f32 	%f3, %f2;
	setp.lt.f32	%p2, %f3, 0f00800000;
	mul.f32 	%f22, %f3, 0f4B800000;
	selp.f32	%f23, 0fC3170000, 0fC2FE0000, %p2;
	selp.f32	%f24, %f22, %f3, %p2;
	mov.b32 	 %r1, %f24;
	and.b32  	%r2, %r1, 8388607;
	or.b32  	%r3, %r2, 1065353216;
	mov.b32 	 %f25, %r3;
	shr.u32 	%r4, %r1, 23;
	cvt.rn.f32.u32	%f26, %r4;
	add.f32 	%f27, %f23, %f26;
	setp.gt.f32	%p3, %f25, 0f3FB504F3;
	mul.f32 	%f28, %f25, 0f3F000000;
	add.f32 	%f29, %f27, 0f3F800000;
	selp.f32	%f30, %f28, %f25, %p3;
	selp.f32	%f31, %f29, %f27, %p3;
	add.f32 	%f32, %f30, 0fBF800000;
	add.f32 	%f16, %f30, 0f3F800000;
	// inline asm
	rcp.approx.ftz.f32 %f15,%f16;
	// inline asm
	add.f32 	%f33, %f32, %f32;
	mul.f32 	%f34, %f15, %f33;
	mul.f32 	%f35, %f34, %f34;
	mov.f32 	%f36, 0f3C4CAF63;
	mov.f32 	%f37, 0f3B18F0FE;
	fma.rn.f32 	%f38, %f37, %f35, %f36;
	mov.f32 	%f39, 0f3DAAAABD;
	fma.rn.f32 	%f40, %f38, %f35, %f39;
	mul.rn.f32 	%f41, %f40, %f35;
	mul.rn.f32 	%f42, %f41, %f34;
	sub.f32 	%f43, %f32, %f34;
	neg.f32 	%f44, %f34;
	add.f32 	%f45, %f43, %f43;
	fma.rn.f32 	%f46, %f44, %f32, %f45;
	mul.rn.f32 	%f47, %f15, %f46;
	add.f32 	%f48, %f42, %f34;
	sub.f32 	%f49, %f34, %f48;
	add.f32 	%f50, %f42, %f49;
	add.f32 	%f51, %f47, %f50;
	add.f32 	%f52, %f48, %f51;
	sub.f32 	%f53, %f48, %f52;
	add.f32 	%f54, %f51, %f53;
	mov.f32 	%f55, 0f3F317200;
	mul.rn.f32 	%f56, %f31, %f55;
	mov.f32 	%f57, 0f35BFBE8E;
	mul.rn.f32 	%f58, %f31, %f57;
	add.f32 	%f59, %f56, %f52;
	sub.f32 	%f60, %f56, %f59;
	add.f32 	%f61, %f52, %f60;
	add.f32 	%f62, %f54, %f61;
	add.f32 	%f63, %f58, %f62;
	add.f32 	%f64, %f59, %f63;
	sub.f32 	%f65, %f59, %f64;
	add.f32 	%f66, %f63, %f65;
	mul.rn.f32 	%f67, %f20, %f64;
	neg.f32 	%f68, %f67;
	fma.rn.f32 	%f69, %f20, %f64, %f68;
	fma.rn.f32 	%f70, %f20, %f66, %f69;
	mov.f32 	%f71, 0f00000000;
	fma.rn.f32 	%f72, %f71, %f64, %f70;
	add.rn.f32 	%f73, %f67, %f72;
	neg.f32 	%f74, %f73;
	add.rn.f32 	%f75, %f67, %f74;
	add.rn.f32 	%f76, %f75, %f72;
	mov.b32 	 %r5, %f73;
	setp.eq.s32	%p4, %r5, 1118925336;
	add.s32 	%r6, %r5, -1;
	mov.b32 	 %f77, %r6;
	add.f32 	%f78, %f76, 0f37000000;
	selp.f32	%f79, %f77, %f73, %p4;
	selp.f32	%f4, %f78, %f76, %p4;
	mul.f32 	%f80, %f79, 0f3FB8AA3B;
	cvt.rzi.f32.f32	%f81, %f80;
	mov.f32 	%f82, 0fBF317200;
	fma.rn.f32 	%f83, %f81, %f82, %f79;
	mov.f32 	%f84, 0fB5BFBE8E;
	fma.rn.f32 	%f85, %f81, %f84, %f83;
	mul.f32 	%f86, %f85, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f87, %f86;
	add.f32 	%f88, %f81, 0f00000000;
	ex2.approx.f32 	%f89, %f88;
	mul.f32 	%f90, %f87, %f89;
	setp.lt.f32	%p5, %f79, 0fC2D20000;
	selp.f32	%f91, 0f00000000, %f90, %p5;
	setp.gt.f32	%p6, %f79, 0f42D20000;
	selp.f32	%f98, 0f7F800000, %f91, %p6;
	setp.eq.f32	%p7, %f98, 0f7F800000;
	@%p7 bra 	BB1_2;

	fma.rn.f32 	%f98, %f98, %f4, %f98;

BB1_2:
	setp.lt.f32	%p8, %f2, 0f00000000;
	setp.eq.f32	%p9, %f1, 0f3F800000;
	and.pred  	%p1, %p8, %p9;
	mov.b32 	 %r7, %f98;
	xor.b32  	%r8, %r7, -2147483648;
	mov.b32 	 %f92, %r8;
	selp.f32	%f100, %f92, %f98, %p1;
	setp.eq.f32	%p10, %f2, 0f00000000;
	@%p10 bra 	BB1_5;
	bra.uni 	BB1_3;

BB1_5:
	add.f32 	%f95, %f2, %f2;
	selp.f32	%f100, %f95, 0f00000000, %p9;
	bra.uni 	BB1_6;

BB1_3:
	setp.geu.f32	%p11, %f2, 0f00000000;
	@%p11 bra 	BB1_6;

	cvt.rzi.f32.f32	%f94, %f20;
	setp.neu.f32	%p12, %f94, 0f40000000;
	selp.f32	%f100, 0f7FFFFFFF, %f100, %p12;

BB1_6:
	add.f32 	%f96, %f3, 0f40000000;
	mov.b32 	 %r9, %f96;
	setp.lt.s32	%p14, %r9, 2139095040;
	@%p14 bra 	BB1_11;

	setp.gtu.f32	%p15, %f3, 0f7F800000;
	@%p15 bra 	BB1_10;
	bra.uni 	BB1_8;

BB1_10:
	add.f32 	%f100, %f2, 0f40000000;
	bra.uni 	BB1_11;

BB1_8:
	setp.neu.f32	%p16, %f3, 0f7F800000;
	@%p16 bra 	BB1_11;

	selp.f32	%f100, 0fFF800000, 0f7F800000, %p1;

BB1_11:
	setp.eq.f32	%p17, %f2, 0f3F800000;
	selp.f32	%f97, 0f3F800000, %f100, %p17;
	st.global.f32 	[%rd1], %f97;
	ret;
}
```

从上面的汇编代码行数来看(虽然代码行数不表示每一个cycle执行的指令数,但是也是作为一个参考值)

**在大多数情况下，CUDA编译器会在后台处理将程序员编写的内核代码转换为GPU指令集的过程。 您很少希望检查或手动修改所生成的指令。 但是，这并不意味着您不能轻易地指示编译器偏向于性能或准确性，或两者兼顾。 两种技术可对CUDA编译器可执行的指令级优化类型进行更严格的控制：编译器标志以及内部或标准函数调用。**

但是，逐个操作手动调整内核的工作量很大。 编译器标志提供了一种更加自动化和全局的方式来操纵编译器指令的生成。 例如，您可能想控制CUDA编译器对浮点MAD（FMAD）指令的生成。 回想一下，MAD是一种简单的编译器优化，它可以将乘法和加法运算融合到一条指令中，从而与使用两条指令相比，该运算所需的时间减少了一半。 但是，此优化是以牺牲一些数字精度为代价的。 因此，某些应用程序可能希望显式限制FMAD指令的使用。

```shell
--fmad=false // 不使用fmad优化
--fmad=true  //使用fmad优化
```

```c++
	ld.param.u64 	%rd1, [_Z3fooPf_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.f32 	%f1, [%rd2];
	fma.rn.f32 	%f2, %f1, %f1, %f1; //使用fmad优化
	st.global.f32 	[%rd2], %f2;
	ret;
```

```c++
	ld.param.u64 	%rd1, [_Z3fooPf_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	ld.global.f32 	%f1, [%rd2];
	mul.rn.f32 	%f2, %f1, %f1;   //未使用fmad
	add.rn.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd2], %f3;
	ret;
```

#### 原子指令

**原子函数可分为三类：算术函数，按位函数和交换功能**。 原子算术函数对目标存储器位置执行简单算术并包括加，减，最大值，最小值，增量和减量之类的常用操作。原子按位函数对目标存储位置执行按位运算，并且包括按位与，按位OR和按位XOR。 原子交换功能有条件地起作用或无条件地用新值交换内存位置的值。 始终具有原子交换功能不管交换是否成功，都将返回最初存储在目标位置的值。atomicExch无条件替换存储的值。 atomicCAS有条件地替换该值如果当前存储的值与调用GPU线程指定的期望值匹配，则存储。

原子操作带来了巨大的性能成本.



## GPU加速的CUDA库和OPENACC

![](/images/posts/2020-09-10-cuda专家编程0801.png)

使用NVIDIA库时的常见工作流程如下：

1.创建一个特定于库的句柄，以管理对库操作有用的上下文信息。
2.将设备存储器分配给库功能的输入和输出。
3.如果输入的格式不是库支持的格式，请将其转换为库可访问的格式。
4.使用支持的格式的输入填充预分配的设备内存。
5.配置要执行的库计算。
6.执行一个库调用，将所需的计算卸载到GPU。
7.可能以库确定的格式从设备存储器中检索该计算的结果。
8.如有必要，将检索到的数据转换为应用程序的本机格式。
9.释放CUDA资源。
10.继续执行应用程序的其余部分。



## 多GPU编程

多gpu的链接方式:

➤在单个节点中通过PCIe总线连接的多个GPU
➤通过集群中的网络交换机连接的多个GPU

![](/images/posts/2020-09-11-08-39-51-cuda专家编程0901.png)

分为两种情况:

1. 无需在多gpu之间进行数据传输
2. 需要在多gpu之间进行数据传输



## 实施注意事项

通用软件开发模式--->异构并行编程模式

![](/images/posts/2020-09-11-09-57-59-cuda专家编程1004.png)



### 显存带宽

> 这里是在<<Professional cuda c programming.pdf>>这个书中,P468

这里很重要







































