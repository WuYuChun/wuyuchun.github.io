
---
layout: post

title:  "cuda的理解"

date:   2020-05-05 14:29

excerpt:

tag:

- GPU
---

[cuda demo]: https://github.com/myurtoglu/cudaforengineers



# 简介

GPU的前身今世

- 并行计算
  - 多CPU
  - 异构平台
- 利用GPU
  - OpenGL\图形学
  - 通用计算（cuda）



gpu的演化过程

![](/images/posts/2020-05-06-08-54-47-cpu-gpu.png)

三个抽象

- 线程组
- 共享内存
- 同步屏障

这些抽象提供了**数据并行性**以及**线程并行性**。把一个大问题变成了很多个的子问题。实际上，可以在任何可用的多处理器上调度每个线程块在GPU中以**任何顺序（并发或顺序）**进行操作，以便编译后的CUDA程序可以在任何数量的多处理器上执行。

![](/images/posts/2020-05-06-09-22-59-调度.png)



## 硬件架构

- CPU的不同硬件设计
- 集成GPU
- 多个GPU
- 在cuda中的地址空间
  - CPU和GPU的地址空间是隔离的，两者不能互相访问
  - 虚拟地址空间
  - 地址空间隔离：Cuda中使用虚拟地址空间
- CPU和GPU的交互
  - 同等主机内存
  - 命令缓冲区
  - CPU/GPU同步

## 硬件术语

- 显卡的类型名/编号
- GPU芯片的名称/编号
- 计算能力
- 架构类型



## cuda并行所需的基本任务

- 使用特定的网格维度加载核函数：线程块和线程的数目
- 明确哪些函数编译后运行在设备上，主机上，还是两者之上
- 访问和运用线程块和线程的计算索引值
- 分配内存和传输数据
- 核函数调用方式
  - __global__：标志核函数，在主机端调用，在设备端执行
  - __host__：主机端调用，主机端执行
  - __device__：在设备端调用，在设备端执行（在核函数中调用的函数需要有_device_限定符）
  - __host___device__：会让系统编译这个函数的主机版本和设备版本





# 编程模型

## kernel

含义：就是一个c函数，能被N个不同的线程执行N次

定义：

```c++
function_name<<<blockx,thriedx>>(params,...)
```

Q：线程数是不是可以设置很多？它依赖于什么呢？若是设置超过了规定，执行起来会是什么样呢？

**每一个kernel还有一个Grid，一个grid含有多个block，一个block含有多个thread**

block的执行是以任何顺序来调度执行。



## 显存的层次

grid中访问的是全局

block访问的是共享内存

thread访问的是自己的内存

还有常量内存以及文本内存



# 编程接口

## cuda编译的流程

### PTX 

指令集架构



### NVCC

---> PTX code

---> cubin object

**use the CUDA driver API (see Driver API) to load and execute the PTX code or cubin object.**



Q：如何编译PTX，如何查看编译过后可执行文件？查看它的反汇编



-code ----> 表示 cubin objec在要在哪个目标下运行，例如：

```c++
-code=sm_35 //produces binary code for devices of compute capability 3.5
```

-arch---->表示PTX的编译情况

```c++
-arch=compute_30
```



### 显存的分配方式

cudaMalloc

cudaMallocPitch

cudaMalloc3D

cudaHostAlloc

使用固定内存的好处？

### streams

这个该如何理解呢？：stream使得这些并行化可行

- 并行化
  - CPU/GPU并行
  - memcpy和内核执行的并行
  - 内核并行
  - 多GPU并行

以宽度优先而不是深度优先，发射指令

### 显示同步

- cudaDeviceSynchronize()
- cudaStreamSynchronize()
- cudaStreamWaitEvent()
- cudaStreamQuery()



### 隐是同步

Two commands from different streams cannot run concurrently。



### 行为并行

如何在stream进行测量，多个stream中的并行呢？

stream也是有优先级的，设置优先级。



## 图

这个很重要，该如何理解呢？

图的优势：

- 减少cpu launch的时间，与stream相比，因为许多操作是预先定义好的
- 在GPU上实现了整个工作流程，能更好的优化，比stream阶段性的提交。

---

使用图，分为三个阶段：

1. 定义图
2. 实例化图
3. 执行图

图的结构：图中一个节点表示一个操作，操作之间的依赖就是连接，这些依赖规定了操作的执行顺序。

节点的类型：

- kernel
- cpu function call
- memroy copy
- memset
- empty node
- child graph

---

创建图的方式：

1. API
2. streeam capture



### 事件

事件的作用是什么？



### 多显卡

显卡之间的访问，

```c++
cudaDeviceCanAccessPeer()
```



### 统一内存访问空间



```c++
nifiedAddressing()
```



### 进程间通信

cuda IPC



### 错误

如何检查GPU的运行时错误？



# 硬件实现



GPU是围绕着多个SMs（Streaming Multiprocessors）来构造体系的。NVIDIA GPU架构使用小端表示。



### **SIMT架构**

warp的含义是什么？32？



每一个block能分为多个warp来调度呢？

> The total number of registers and total amount of shared memory allocated for a block
> are documented in the CUDA Occupancy Calculator provided in the CUDA Toolkit.



cuda引用单指令多线程（SIMT）的并行模式



### Hardware Multithreading



# 性能准则



**三个基本策略**：

1. 最大化并行，提高最大的利用率
2. 优化内存，提高内存的吞吐量
3. 优化指令利用，提高最大的指令吞吐量



## 最大化利用率



**应用层面**



**设备层面**

通过使用stream，让设备尽可能的忙碌起来



**SM级别**

在这个级别中，利用率与常驻warp的数量直接相关。

warp尚未准备好执行其下一条指令的最常见原因是该指令的输入操作数尚不可用。

Warp未准备好执行下一条指令的另一个原因是它正在等待在某些内存栅栏（“内存栅栏功能”）或同步点（“内存”围栏功能）。

**给定内核的每个多处理器上存在的块和warp数调用取决于调用的执行配置（执行配置），多处理器的内存资源，以及内核的资源需求为硬件多线程中描述。 报告寄存器和共享内存使用情况使用-ptxas-options = -v选项进行编译时由编译器执行。**



**占用率计算**

```
cudaOccupancyMaxActiveBlocksPerMultiprocessor()
```

```c++
// Device code
__global__ void MyKernel(int *d, int *a, int *b){
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    d[idx] = a[idx] * b[idx];
}
// Host code
int main(){
    
   int numBlocks; // Occupancy in terms of active blocks
   int blockSize = 32;
   // These variables are used to convert occupancy to warps
   int device;
   cudaDeviceProp prop;
   int activeWarps;
   int maxWarps;
   cudaGetDevice(&device);
   cudaGetDeviceProperties(&prop, device);
   cudaOccupancyMaxActiveBlocksPerMultiprocessor(
                                                 &numBlocks,
                                                  MyKernel,
                                                  blockSize,
                                                 0);
   activeWarps = numBlocks * blockSize / prop.warpSize;
   maxWarps = prop.maxThreadsPerMultiProcessor / prop.warpSize;
   std::cout << "Occupancy: " << (double)activeWarps / maxWarps * 100 << "%" <<
   std::endl;
   return 0;
}
```



**在cuda/tool/CUDA_Occupancy_Calculator.xls**



**显存访问**

显存的访问对于warp的执行有很多的影响

**全局显存的访问**

32\64\128字节的对齐，对于warp中每一个线程访问全局，将会合并成一次或者多次的显存访问。

对于大小和对齐的要求：如果未满足此大小和对齐要求，则访问将编译为具有交错访问模式的多个指令，从而导致这些指令无法完全合并。 因此，建议对驻留在全局内存中的数据使用符合此要求的类型。

对于二维数据的访问，如下所示：

```c++
BaseAddress+width*ty+tx;
```

- [x] **为了使这些访问完全合并，线程块的宽度和数组的宽度都必须是经线大小的倍数。**



**局部内存**

Also, the compiler reports total local memory usage per kernel (lmem) when compiling with the **--ptxas-options=-v **option.

如何在cmake中增加这个选项表示出来呢？



**常量内存**



## 最大化指令吞吐率

在本节中，吞吐量以每个SM每个时钟周期的操作数给出。 对于32的warp大小，一个指令对应32个操作，因此，如果N是每个时钟周期的操作数，则指令吞吐量为每个时钟周期N / 32指令。

**cuobjdump can be used to inspect a particular implementation in a cubin object.**

在if\switch\do\for\while会影响到指令的吞吐率，导致线程执行分叉。最简单的方式的是用**thread.idx/warpsize**来进行控制。

同步指令的影响



# 语言扩展

内建vector类型

**如何使用Tensor Cores**

--Sub-byte WMMA operations provide a way to access the low-precision capabilities of
Tensor Cores. --

cuda core 和 tensor cores 的区别？



## 资源

- http://www.ee.iitm.ac.in/~ee12s008/CUDA_Workshop.pdf
- http://hpc.pku.edu.cn/docs/20170829223804057249.pdf
- http://hustcat.github.io/gpu-architecture/
- http://haifux.org/lectures/267/Introduction-to-GPUs.pdf
