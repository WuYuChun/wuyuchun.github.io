[TOC]



# CUDA c++编程指南--设计指导



## 1 介绍



### 1.1 使用GPU的好处



相比于CPU，在相同的价格和功率范围内，图形处理单元(GPU)1提供了更高的指令吞吐量和内存带宽。许多应用程序利用这些更高的能力在GPU上比在CPU上运行得更快(参见GPU应用程序)。其他计算设备，如fpga，也非常节能，但提供编程灵活性远低于gpu。

GPU和CPU之间存在能力上的差异是因为它们的设计目标不同。虽然CPU设计善于执行的操作序列,称为一个线程,尽可能快,可以并行执行几十这些线程,GPU旨在擅长并行执行成千上万(掩盖了慢单线程性能来实现更高的吞吐量)。

GPU是专门用于高度并行计算的，因此设计成更多的晶体管用于数据处理，而不是数据缓存和流控制。图1显示了CPU与GPU的芯片资源分配示例。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南01.PNG)

将更多的晶体管用于数据处理，例如浮点计算，有利于高度并行计算;GPU可以通过计算隐藏内存访问延迟，而不是依赖于大型数据缓存和复杂的流控制来避免长时间的内存访问延迟，这两者对于晶体管来说都是昂贵的。

一般来说，一个应用程序混合了并行部分和顺序部分，所以为了最大化整体性能，系统被设计为混合gpu和cpu。具有高度并行性的应用程序可以利用GPU的这种大规模并行特性来实现比CPU更高的性能。

### 2.1 CUDA:一个通用的并行计算平台和编程模型

2006年11月，NVIDIA®推出了CUDA®，这是一个通用的并行计算平台和编程模型，利用NVIDIA gpu中的并行计算引擎，以一种比CPU更高效的方式解决许多复杂的计算问题

CUDA提供了一个允许开发者使用c++作为高级编程语言的软件环境。如图2所示，支持其他语言、应用程序编程接口或基于指令的方法，如FORTRAN、DirectCompute、OpenACC。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南02.PNG)

### 1.3 一种可伸缩的编程模型

多核cpu和多核gpu的出现意味着主流处理器芯片现在是并行系统。我们面临的挑战是开发能够透明地扩展并行度以利用不断增加的处理器内核数量的应用软件，就像3D图形应用程序能够透明地扩展它们的并行度到具有不同内核数量的多核gpu。

CUDA并行编程模型旨在克服这一挑战，同时对熟悉C等标准编程语言的程序员保持较低的学习曲线。

其核心是三个关键的抽象——线程组的层次结构、共享内存和障碍同步——它们只是作为最小的语言扩展集公开给程序员。

这些抽象提供细粒度数据并行性和线程并行性，嵌套在粗粒度数据并行性和任务并行性中。它们引导程序员将问题划分为粗的子问题，这些子问题可以由线程块独立并行解决，而每个子问题又可以由块内的所有线程协同并行解决。

通过允许线程在解决每个子问题时进行协作，这种分解保留了语言的表达性，同时还支持自动的可伸缩性。的确,每一块的线程可以安排在任何可用的多处理器GPU,在任何顺序,同时或顺序,以便编译CUDA程序可以执行任意数量的多处理器如图3所示,只有运行时系统需要知道物理多处理器数。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南03.PNG)

这种可扩展的编程模型允许GPU架构通过简单地扩展多处理器和内存分区的数量来跨越广泛的市场范围:从高性能的GeForce图形处理器和专业的Quadro和Tesla计算产品到各种廉价的主流GeForce图形处理器(查看所有cuda支持的图形处理器列表)

*注意:GPU是围绕一个流多处理器阵列(SMs)构建的(更多细节见硬件实现)。一个多线程程序被划分为多个独立执行的线程块，这样多处理器的GPU会比少处理器的GPU自动执行程序的时间更短。*

### 1.4 文档结构

本文档分为以下几章:

- 本章简介是对CUDA的一般介绍。
- Chapter Programming Model概述CUDA编程模型。
- Chapter Programming Interface描述了编程接口。
- 第1章Hardware Implementation描述硬件实现。
- Chapter Performance Guidelines提供了一些关于如何达到最大的指导的性能。
- 附录cuda开启的gpu列表所有cuda开启的设备。
- 附录c++语言扩展，‣所有c++扩展的详细描述语言。
- 附录Cooperative Groups描述各种组的同步原语CUDA线程。
- 附录CUDA Dynamic Parallelism描述了如何启动和同步一个内核从另一个。
- 附录数学函数列出CUDA中支持的数学函数。
- 附录c++语言支持列出设备代码中支持的c++特性。
- 附录纹理抓取提供了纹理抓取的更多细节
- 附录计算能力给出了各种设备的技术规格，如还有更多的架构细节。
- 附录Driver API介绍底层驱动API。
- 附录CUDA环境变量列出所有CUDA环境变量。
- 附录Unified Memory Programming，运行统一内存编程模型。



## 2 编程模型

本章介绍了CUDA编程模型背后的主要概念，概述了它们是如何在c++中公开的。在《编程接口》中对CUDA c++作了详细的描述。

本章和下一章中使用的矢量加法示例的完整代码可以在vectorAdd CUDA示例中找到。



### 2.1 内核

CUDA c++扩展了c++，允许程序员定义c++函数，称为内核，当调用时，由N个不同的CUDA线程并行执行N次，而不是像普通c++函数一样只执行一次。

一个内核是用__global__声明说明符定义的，并且为一个给定的内核调用执行该内核的CUDA线程的数量是用一个新的<<<…>>>执行配置语法(参见c++语言扩展)。每个执行内核的线程都有一个唯一的线程ID，可以在内核中通过内置变量访问这个线程ID。

下面的示例代码使用内置变量threadIdx，将两个大小为N的向量A和B相加，并将结果存储到向量C中:

```
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
int i = threadIdx.x;
C[i] = A[i] + B[i];
}
int main()
{
...
// Kernel invocation with N threads
VecAdd<<<1, N>>>(A, B, C);
...
}
```

这里，每个执行VecAdd()的N个线程都执行一次成对加法。



### 2.2 线程层次结构

为了方便起见，threadadidx是一个三分量向量，因此可以使用一维、二维或三维线程索引来标识线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种跨越域(如向量、矩阵或卷)中的元素调用计算的自然方法。

一个线程的索引和它的线程ID以一种简单的方式相互关联:对于一维块，它们是相同的;对于大小为(Dx, Dy)的二维块，index (x, y)线程的线程ID为(x + y Dx);对于大小为(Dx, Dy, Dz)的三维块，index (x, y, z)线程的线程ID为(x + y Dx + z Dx Dy)。

下面的代码将两个大小为NxN的矩阵A和B相加，并将结果存储到矩阵C中:

```
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
int i = threadIdx.x;
int j = threadIdx.y;
C[i][j] = A[i][j] + B[i][j];
}
int main()
{
...
// Kernel invocation with one block of N * N * 1 threads
int numBlocks = 1;
dim3 threadsPerBlock(N, N);
MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
...
}
```

每个块的线程数是有限制的，因为一个块的所有线程都驻留在同一个处理器核心上，并且必须共享该核心的有限内存资源。在当前gpu上，一个线程块最多可以包含1024个线程。

但是，一个内核可以由多个形状相同的线程块执行，因此线程的总数等于每个块的线程数乘以块的数量。

块被组织成一个一维、二维或三维的线程块网格，如图4所示。网格中的线程块的数量通常由正在处理的数据的大小决定，这通常超过系统中处理器的数量。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南04.PNG)

<<<…中指定的每个块的线程数和每个网格的块数。>>>语法的类型可以是int或dim3。二维块或网格可以在上面的例子中指定。

网格中的每个块都可以通过一个一维、二维或三维的唯一索引来标识，该索引可以通过内置的blockIdx变量在内核中访问。在内核中，可以通过内置的blockDim变量访问线程块的维度。

扩展前面的MatAdd()示例以处理多个块，代码变成如下所示。

```
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
if (i < N && j < N)
C[i][j] = A[i][j] + B[i][j];
}
int main()
{
...
// Kernel invocation
dim3 threadsPerBlock(16, 16);
dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
...
}
```

线程块大小为16x16(256个线程)，虽然在本例中是任意的，但这是一种常见的选择。网格是用足够的块创建的，每个矩阵元素都有一个线程。为了简单起见，本例假设每个维度中的每个网格的线程数能被该维度中的每个块的线程数整除，尽管情况不一定如此。

线程块必须独立执行:必须能够以任何顺序(并行或串行)执行它们。这种独立性要求允许在任意数量的内核上以任意顺序调度线程块，如图3所示，使程序员能够编写随内核数量伸缩的代码。

块中的线程可以通过共享一些共享内存来共享数据，并通过同步它们的执行来协调内存访问。更确切地说，可以通过调用___syncthreads()内在函数来在内核中指定同步点;__syncthreads()充当一个屏障，在这个屏障上，块中的所有线程必须等待，然后才允许任何线程继续。共享内存给出了一个使用共享内存的例子。除了__syncthreads()之外，Cooperative Groups API还提供了一组丰富的线程同步原语。

为了高效的合作，共享内存应该是靠近每个处理器核心的低延迟内存(很像L1缓存)，__syncthreads()应该是轻量级的。

### 2.3 内存层次结构

CUDA线程在执行过程中可以从多个内存空间访问数据，如图5所示。每个线程都有私有的本地内存。每个线程块都有对该块的所有线程可见的共享内存，并且具有与该块相同的生存期。所有线程都可以访问相同的全局内存。

还有两个额外的只读内存空间可供所有线程访问:常量内存空间和纹理内存空间。全局、常量和纹理内存空间针对不同的内存使用进行了优化(请参阅设备内存访问)。纹理内存还为一些特定的数据格式提供了不同的寻址模式和数据过滤(参见纹理和表面内存)。

全局内存空间、常量内存空间和纹理内存空间是由同一个应用程序在内核启动时持久化的。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南05.PNG)

### 2.4 异构计算

如图6所示，CUDA编程模型假设CUDA线程在一个物理上独立的设备上执行，该设备作为运行c++程序的主机的协处理器。例如，当内核在GPU上执行，而c++程序的其余部分在CPU上执行时，就是这种情况。

CUDA编程模型还假设主机和设备都在DRAM中维护各自独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用CUDA运行时(在编程接口中描述)来管理内核可见的全局、常量和纹理内存空间。这包括设备内存的分配和回收，以及主机和设备内存之间的数据传输。

统一内存提供托管内存来桥接主机和设备内存空间。托管内存作为一个具有公共地址空间的单一、一致的内存映像，可以从系统中的所有cpu和gpu访问。该功能支持对设备内存的超额订阅，并且通过消除在主机和设备上显式地镜像数据的需要，可以极大地简化移植应用程序的任务。关于统一内存的介绍，请参见统一内存编程。

*注意:串行代码在主机上执行，而并行代码在设备上执行。*



### 2.5 计算能力

设备的计算能力由版本号表示，有时也称为“SM版本”。这个版本号标识了GPU硬件支持的特性，应用程序在运行时使用这个版本号来确定当前GPU上可用的硬件特性和/或指令。

计算能力由主要修订号X和次要修订号Y组成，由x.y表示

具有相同主版本号的设备具有相同的核心架构。设备的主要修订号是8安培NVIDIA GPU的体系结构的基础上,7基于沃尔塔设备架构,6设备基于帕斯卡架构,5设备基于麦克斯韦架构,3基于开普勒架构的设备,2设备基于费米架构,1为基于特斯拉架构的设备。

次要修订号对应于对核心体系结构的增量改进，可能包括新特性。

图灵是计算能力7.5设备的架构，是基于Volta架构的增量更新。

CUDA-Enabled gpu列出了所有CUDA-Enabled设备及其计算能力。计算能力给出了每个计算能力的技术规格。

注意:特定GPU的计算能力版本不应与CUDA版本混淆(例如，CUDA 7.5, CUDA 8, CUDA 9)，后者是CUDA软件平台的版本。应用程序开发人员使用CUDA平台来创建运行在许多代GPU架构上的应用程序，包括未来尚未发明的GPU架构。CUDA平台的新版本通常通过支持新的GPU架构的计算能力版本来增加对该架构的本地支持，而CUDA平台的新版本通常还包括独立于硬件生成的软件功能。

从CUDA 7.0和CUDA 9.0开始，特斯拉和费米架构就不再受支持



## 3 编程接口

CUDA c++为熟悉c++编程语言的用户提供了一条简单的路径，可以方便地编写设备执行的程序。

它由一个最小的c++语言扩展集和一个运行时库组成。

核心语言扩展已在编程模型中介绍。它们允许程序员将内核定义为c++函数，并在每次调用函数时使用一些新的语法来指定网格和块维。所有扩展的完整描述可以在c++语言扩展中找到。任何包含这些扩展名的源文件都必须使用nvcc编译，如nvcc编译中所述。

CUDA运行时中引入了运行时。它提供在主机上执行的C和c++函数来分配和释放设备内存，在主机内存和设备内存之间传输数据，管理具有多个设备的系统等。完整的运行时描述可以在CUDA参考手册中找到。

运行时是建立在一个较低级的C API之上，CUDA驱动程序API，也可以被应用程序访问。驱动程序API通过暴露低级别的概念提供了额外的控制级别，例如CUDA上下文——设备的宿主进程的模拟——和CUDA模块——设备的动态加载库的模拟。大多数应用程序不使用驱动程序API，因为它们不需要这种额外的控制级别，并且在使用运行时时，上下文和模块管理是隐式的，从而产生更简洁的代码。由于运行时可与驱动程序API互操作，大多数需要某些驱动程序API特性的应用程序可以默认使用运行时API，而只在需要时使用驱动程序API。驱动程序API在驱动程序API中介绍，并在参考手册中详细描述。

### 3.1 使用NVCC编译

内核可以使用CUDA指令集架构(称为PTX)来编写，该架构在PTX参考手册中有描述。然而，通常使用高级编程语言(如c++)更有效。在这两种情况下，内核必须被nvcc编译成二进制代码才能在设备上执行。

nvcc是一个编译器驱动程序，它简化了编译c++或PTX代码的过程:它提供了简单和熟悉的命令行选项，并通过调用实现不同编译阶段的工具集合来执行它们。本节概述nvcc工作流和命令选项。完整的描述可以在nvcc用户手册中找到。



#### 3.1.1 编译工作流

##### 3.1.1.1 离线编译

用nvcc编译的源文件可以包含主机代码(即在主机上执行的代码)和设备代码(即在设备上执行的代码)的混合。Nvcc的基本工作流程是将设备代码从主机代码中分离出来，然后:

- 将设备代码编译为装配形式(PTX代码)和/或二进制形式(立方对象)，
- 并通过替换<<<…来修改主机代码>>>语法在kernel中引入(在执行配置中详细描述)，通过必要的CUDA运行时函数调用来从PTX代码和/或cubin对象加载和启动每个编译过的内核。

修改后的宿主代码要么输出为c++代码，让其他工具编译，要么通过让nvcc在最后编译阶段调用宿主编译器直接作为目标代码。

应用程序可以:

- 任何一个指向编译后的主机代码的链接(这是最常见的情况)，
- 所有修改过的主机代码(如果有的话)，使用CUDA驱动API(参见驱动API)加载并执行PTX代码或cubin对象。

##### 3.1.1.2 即时编译

应用程序在运行时加载的任何PTX代码都被设备驱动程序进一步编译为二进制代码。这称为即时编译。即时编译增加了应用程序加载时间，但允许应用程序从每个新设备驱动程序带来的任何新的编译器改进中受益。这也是使应用程序在编译时还不存在的设备上运行的唯一方法，详细内容见应用程序兼容性。

当设备驱动程序为某些应用程序即时编译某些PTX代码时，它会自动缓存生成的二进制代码的一个副本，以避免在随后的应用程序调用中重复编译。当设备驱动程序升级时，缓存(称为计算缓存)将自动失效，因此应用程序可以从设备驱动程序中内置的新的即时编译器的改进中获益。

环境变量可用于控制实时编译，如CUDA环境变量中所述

作为一种替代使用nvcc编译CUDA c++设备代码，NVRTC可以用来编译CUDA c++设备代码到PTX在运行时。NVRTC是一个运行时编译库CUDA c++;更多信息可以在NVRTC用户指南中找到。

#### 3.1.2 二进制兼容性

二进制代码是特定于体系结构的。使用编译器选项-code生成cubin对象，该选项指定目标体系结构:例如，使用-code=sm_35编译会为具有3.5计算能力的设备生成二进制代码。二进制兼容性可以保证从一个小版本到下一个小版本，但不能保证从一个小版本到前一个或跨多个大版本。换句话说，为计算能力X.y生成的cubin对象只会在具有计算能力X.z的设备上执行，其中z≥y。

*注意:二进制兼容性仅支持桌面。Tegra不支持它。此外，不支持桌面和Tegra之间的二进制兼容性。*

#### 3.1.3 PTX兼容性

一些PTX指令只支持具有更高计算能力的设备。例如，Warp Shuffle功能只支持计算能力3.0及以上的设备。arch编译器选项指定了将c++编译为PTX代码时假定的计算能力。例如，包含warp shuffle的2C代码必须使用-arch=compute_30(或更高)进行编译。

为某些特定计算能力而产生的PTX代码总是可以编译为具有更大或相等计算能力的二进制代码。请注意，从较早的PTX版本编译的二进制文件可能不会使用某些硬件特性。例如，从为计算能力6.0 (Pascal)生成的PTX编译的计算能力7.0 (Volta)的二进制目标设备将不会使用张量核心指令，因为这些指令在Pascal上不可用。因此，最终的二进制文件可能比使用最新版本的PTX生成的二进制文件性能更差。

#### 3.1.4 应用程序兼容性

要在具有特定计算能力的设备上执行代码，应用程序必须加载与该计算能力兼容的二进制或PTX代码，如二进制兼容性和PTX兼容性中所述。特别是，为了能够在未来具有更高计算能力(目前还不能生成二进制代码)的体系结构上执行代码，应用程序必须加载将为这些设备实时编译的PTX代码(参见实时编译)。

哪些PTX和二进制代码嵌入到CUDA c++应用程序中，是由- arch和-code编译器选项或-gencode编译器选项控制的，详见nvcc用户手册。例如,

```
nvcc x.cu
-gencode arch=compute_50,code=sm_50
-gencode arch=compute_60,code=sm_60
-gencode arch=compute_70,code=\'compute_70,sm_70\'
```

嵌入二进制代码兼容计算能力5.0和6.0(第一和第二gencode选项)和PTX和二进制代码兼容计算能力7.0(第三gencode选项)

生成宿主代码是为了在运行时自动选择要加载和执行的最合适的代码，在上面的例子中，将是:

- 5.0二进制代码的设备计算能力5.0和5.2,
- 6.0二进制代码的设备计算能力6.0和6.1,
- 7.0二进制代码的设备计算能力7.0和7.5,
- PTX代码编译成二进制代码在运行时设备的计算能力8.0和8.6。

x.Cu可以有一个使用warp shuffle操作的优化代码路径，例如，只有计算能力为3.0或更高的设备才支持这种操作。__CUDA_ARCH__宏可以用来区分基于计算能力的各种代码路径。它仅为设备代码定义。例如，当使用-arch=compute_35编译时，__CUDA_ARCH__等于350。

使用驱动程序API的应用程序必须编译代码来分离文件，并在运行时显式地加载和执行最适当的文件。

Volta架构引入了独立线程调度，它改变了GPU上线程调度的方式。对于依赖于以前体系结构中SIMT调度的特定行为的代码，独立线程调度可能会改变参与线程的集合，从而导致不正确的结果。为了帮助迁移，同时实现独立线程调度中详细介绍的纠正措施，Volta开发人员可以选择使用编译器选项组合-arch=compute_60 -code=sm_70来实现Pascal的线程调度。

nvcc用户手册列出了-arch、-code和-gencode的各种缩写

编译器选项。例如，-arch=sm_70是-arch=compute_70 - code=compute_70,sm_70的简写(与-gencode arch=compute_70, code=\'compute_70,sm_70\'相同)

#### 3.1.5 c++的兼容性

编译器的前端按照c++语法规则处理CUDA源文件。宿主代码支持全c++。然而，正如c++语言支持中描述的那样，设备代码只完全支持c++的一个子集。

#### 3.1.6 64位的兼容性

64位版本的nvcc以64位模式编译设备代码(即，指针是64位的)。64位模式下编译的设备代码只支持64位模式下编译的主机代码。

类似地，32位版本的nvcc以32位模式编译设备代码，而以32位模式编译的设备代码仅在以32位模式编译的主机代码中得到支持。

32位版本的nvcc也可以使用-m64编译器选项以64位模式编译设备代码。

nvcc的64位版本还可以使用-m32编译器选项以32位模式编译设备代码。



### 3.2 CUDA 运行时

运行时在cudart库中实现，该库通过cudart静态链接到应用程序。自由或libcudart。A，或动态通过cudart.dll或libudart .so。需要cudart.dll和/或cudart的应用程序。因此，对于动态链接，通常将它们作为应用程序安装包的一部分。只有在链接到CUDA运行时相同实例的组件之间传递CUDA运行时符号的地址才是安全的。

所有的函数入口点都有cuda前缀

正如在异构编程中提到的，CUDA编程模型假设一个由主机和设备组成的系统，每个主机和设备都有自己单独的内存。Device Memory概述了用于管理设备内存的运行时函数。

共享内存演示了如何使用线程层次结构中引入的共享内存来最大化性能

主机内存引入了页锁定主机内存，用于将内核执行与主机和设备内存之间的数据传输重叠。

异步并发执行描述了用于在系统的不同级别上启用异步并发执行的概念和API。

Multi-Device System显示了如何将编程模型扩展到具有多个设备连接到同一主机的系统。

错误检查介绍如何正确检查运行时产生的错误

调用栈提到了用于管理CUDA c++调用栈的运行时函数

纹理和表面存储器表示纹理和表面存储器空间，提供了另一种方式来访问设备存储器;它们也暴露了GPU纹理硬件的一个子集。

图形互操作性介绍了运行时提供的与两个主要图形api OpenGL和Direct3D互操作的各种功能

#### 3.2.1 初始化

运行时没有显式的初始化函数;它初始化第一次调用运行时函数(更具体地说，除了参考手册中的错误处理和版本管理部分的函数之外的任何函数)。在对运行时函数调用计时以及将第一次调用的错误代码解释到运行时时，需要记住这一点。

运行时为系统中的每个设备创建一个CUDA上下文(关于CUDA上下文的更多细节请参见上下文)。该上下文是该设备的主要上下文，并在需要该设备上的活动上下文的第一个运行时函数中初始化。它在应用程序的所有宿主线程之间共享。作为此上下文创建的一部分，设备代码将在必要时及时编译(请参阅即时编译)并加载到设备内存中。这一切都是透明的。如果需要，例如驱动程序API互操作性，设备的主要上下文可以从运行时和驱动程序API之间的互操作性中描述的驱动程序API访问。

当宿主线程调用cudaDeviceReset()时，这会破坏宿主线程当前操作的设备的主上下文(即在设备选择中定义的当前设备)。将此设备作为当前设备的任何宿主线程所发出的下一次运行时函数调用将为该设备创建一个新的主上下文。

#### 3.2.2 设备存储

正如在异构编程中提到的，CUDA编程模型假设一个由主机和设备组成的系统，每个主机和设备都有自己单独的内存。内核在设备内存之外运行，因此运行时提供了分配、释放和复制设备内存的函数，以及在主机内存和设备内存之间传输数据。

设备存储器可以被分配为线性存储器或CUDA阵列。

CUDA数组是为纹理获取而优化的不透明内存布局。它们在纹理和表面记忆中被描述。

线性内存是在一个统一的地址空间中分配的，这意味着分别分配的实体可以通过指针彼此引用，例如，在二叉树或链表中。地址空间大小取决于主机系统(CPU)和使用的GPU的计算能力:

|                                          | x86_64(AMD64) | POWER(ppc64le) | ARM64       |
| ---------------------------------------- | ------------- | -------------- | ----------- |
| up to compute capability 5.3 (Maxwell)   | 40bit         | 40bit          | 40bit       |
| compute capability 6.0 (Pascal) or newer | up to 47bit   | up to 49bit    | up to 48bit |

*注意:在计算能力5.3 (Maxwell)和更早的设备上，CUDA驱动创建一个未提交的40位虚拟地址预留，以确保内存分配(指针)落入支持的范围。这个预留显示为预留虚拟内存，但在程序实际分配内存之前不占用任何物理内存。*线性内存通常使用cudaMalloc()分配，并使用cudaFree()释放。

主机内存和设备内存之间的数据传输通常使用cudaMemcpy()完成。在kernel的vector加法代码示例中，vector需要从主机内存复制到设备内存

```c++
// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
int i = blockDim.x * blockIdx.x + threadIdx.x;
if (i < N)
C[i] = A[i] + B[i];
}
// Host code
int main()
{
int N = ...;
size_t size = N * sizeof(float);
// Allocate input vectors h_A and h_B in host memory
float* h_A = (float*)malloc(size);
float* h_B = (float*)malloc(size);
// Initialize input vectors
...
// Allocate vectors in device memory
float* d_A;
cudaMalloc(&d_A, size);
float* d_B;
cudaMalloc(&d_B, size);
float* d_C;
cudaMalloc(&d_C, size);
// Copy vectors from host memory to device memory
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
// Invoke kernel
int threadsPerBlock = 256;
int blocksPerGrid =
(N + threadsPerBlock - 1) / threadsPerBlock;
VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);
// Copy result from device memory to host memory
// h_C contains the result in host memory
cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
// Free device memory
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
// Free host memory
...
}
```

线性内存也可以通过cudaMallocPitch()和cudaMalloc3D()来分配。这些函数被推荐用于2D或3D数组的分配，因为它确保分配被适当填充，以满足设备内存访问中描述的对齐要求，因此，当访问行地址或在2D数组和设备内存的其他区域之间执行复制时，确保最佳性能(使用cudaMemcpy2D()和cudaMemcpy3D()函数)。返回的间距(或步幅)必须用于访问数组元素。下面的代码示例分配了一个宽x高的2D浮点值数组，并演示了如何在设备代码中循环数组元素:

```c++
// Host code
int width = 64, height = 64;
float* devPtr;
size_t pitch;
cudaMallocPitch(&devPtr, &pitch,
width * sizeof(float), height);
MyKernel<<<100, 512>>>(devPtr, pitch, width, height);
// Device code
__global__ void MyKernel(float* devPtr,
size_t pitch, int width, int height)
{
for (int r = 0; r < height; ++r) {
float* row = (float*)((char*)devPtr + r * pitch);
for (int c = 0; c < width; ++c) {
float element = row[c];
}
}
}
```

下面的代码示例分配了一个宽x高x深的浮点值3D数组，并演示了如何在设备代码中循环数组元素:

```c++
// Host code
int width = 64, height = 64, depth = 64;
cudaExtent extent = make_cudaExtent(width * sizeof(float),
height, depth);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&devPitchedPtr, extent);
MyKernel<<<100, 512>>>(devPitchedPtr, width, height, depth);
// Device code
__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,
int width, int height, int depth)
{
char* devPtr = devPitchedPtr.ptr;
size_t pitch = devPitchedPtr.pitch;
size_t slicePitch = pitch * height;
for (int z = 0; z < depth; ++z) {
char* slice = devPtr + z * slicePitch;
for (int y = 0; y < height; ++y) {
float* row = (float*)(slice + y * pitch);
for (int x = 0; x < width; ++x) {
float element = row[x];
}
}
}
}
```

*注意:为了避免分配太多内存从而影响整个系统的性能，请根据问题大小向用户请求分配参数。如果分配失败，你可以回退到其他较慢的内存类型(cudaMallocHost()， cudaHostRegister()等)，或返回一个错误，告诉用户需要多少内存被拒绝。如果您的应用程序由于某些原因不能请求分配参数，我们建议使用cudaMallocManaged()为支持它的平台。*

参考手册列出了所有用于在cudaMalloc()分配的线性内存、cudaMallocPitch()或cudaMalloc3D()分配的线性内存、CUDA数组和在全局或常量内存空间中声明的变量之间复制内存的各种函数。

下面的代码示例演示了通过运行时API访问全局变量的各种方法:

```c++
__constant__ float constData[256];
float data[256];
cudaMemcpyToSymbol(constData, data, sizeof(data));
cudaMemcpyFromSymbol(data, constData, sizeof(data));
__device__ float devData;
float value = 3.14f;
cudaMemcpyToSymbol(devData, &value, sizeof(float));
__device__ float* devPointer;
float* ptr;
cudaMalloc(&ptr, 256 * sizeof(float));
cudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));
```

cudaGetSymbolAddress()用于检索指向为在全局内存空间中声明的变量分配的内存的地址。分配内存的大小是通过cudaGetSymbolSize()获得的。

#### 3.2.3 设备内存L2访问管理

当CUDA内核重复访问全局内存中的数据区域时，这种数据访问可以被认为是持久化的。另一方面，如果数据只被访问一次，这样的数据访问可以被认为是流的。

从CUDA 11.0开始，具有8.0及以上计算能力的设备具有影响L2缓存中数据持久性的能力，可能为全局内存提供更高的带宽和更低的延迟访问。

##### 3.2.3.1. L2缓存预留用于持久化访问

可以留出一部分L2缓存，用于将数据访问持久化到全局内存。持久化访问优先使用L2缓存的这一部分，而普通或流访问全局内存只能使用L2的这一部分，当它通过持久化访问未被使用时。

L2缓存预留大小可以在一定的限制下进行调整:

```c++
cudaGetDeviceProperties(&prop, device_id);
size_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2
cache for persisting accesses or the max allowed*/
```

当GPU配置为MIG (Multi-Instance GPU)模式时，L2 cache set-aside功能将被关闭。

当使用多进程服务(MPS)时，L2缓存设置的大小不能被cudaDeviceSetLimit改变。相反，设置大小只能在MPS服务器启动时通过环境变量CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT指定。

##### 3.2.3.2 L2持久化访问策略

