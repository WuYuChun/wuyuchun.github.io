[TOC]



# CUDA c++编程指南--设计指导



## 1 介绍



### 1.1 使用GPU的好处



相比于CPU，在相同的价格和功率范围内，图形处理单元(GPU)1提供了更高的指令吞吐量和内存带宽。许多应用程序利用这些更高的能力在GPU上比在CPU上运行得更快(参见GPU应用程序)。其他计算设备，如fpga，也非常节能，但提供编程灵活性远低于gpu。

GPU和CPU之间存在能力上的差异是因为它们的设计目标不同。虽然CPU设计善于执行的操作序列,称为一个线程,尽可能快,可以并行执行几十这些线程,GPU旨在擅长并行执行成千上万(掩盖了慢单线程性能来实现更高的吞吐量)。

GPU是专门用于高度并行计算的，因此设计成更多的晶体管用于数据处理，而不是数据缓存和流控制。图1显示了CPU与GPU的芯片资源分配示例。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南01.PNG)

将更多的晶体管用于数据处理，例如浮点计算，有利于高度并行计算;GPU可以通过计算隐藏内存访问延迟，而不是依赖于大型数据缓存和复杂的流控制来避免长时间的内存访问延迟，这两者对于晶体管来说都是昂贵的。

一般来说，一个应用程序混合了并行部分和顺序部分，所以为了最大化整体性能，系统被设计为混合gpu和cpu。具有高度并行性的应用程序可以利用GPU的这种大规模并行特性来实现比CPU更高的性能。

### 2.1 CUDA:一个通用的并行计算平台和编程模型

2006年11月，NVIDIA®推出了CUDA®，这是一个通用的并行计算平台和编程模型，利用NVIDIA gpu中的并行计算引擎，以一种比CPU更高效的方式解决许多复杂的计算问题

CUDA提供了一个允许开发者使用c++作为高级编程语言的软件环境。如图2所示，支持其他语言、应用程序编程接口或基于指令的方法，如FORTRAN、DirectCompute、OpenACC。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南02.PNG)

### 1.3 一种可伸缩的编程模型

多核cpu和多核gpu的出现意味着主流处理器芯片现在是并行系统。我们面临的挑战是开发能够透明地扩展并行度以利用不断增加的处理器内核数量的应用软件，就像3D图形应用程序能够透明地扩展它们的并行度到具有不同内核数量的多核gpu。

CUDA并行编程模型旨在克服这一挑战，同时对熟悉C等标准编程语言的程序员保持较低的学习曲线。

其核心是三个关键的抽象——线程组的层次结构、共享内存和障碍同步——它们只是作为最小的语言扩展集公开给程序员。

这些抽象提供细粒度数据并行性和线程并行性，嵌套在粗粒度数据并行性和任务并行性中。它们引导程序员将问题划分为粗的子问题，这些子问题可以由线程块独立并行解决，而每个子问题又可以由块内的所有线程协同并行解决。

通过允许线程在解决每个子问题时进行协作，这种分解保留了语言的表达性，同时还支持自动的可伸缩性。的确,每一块的线程可以安排在任何可用的多处理器GPU,在任何顺序,同时或顺序,以便编译CUDA程序可以执行任意数量的多处理器如图3所示,只有运行时系统需要知道物理多处理器数。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南03.PNG)

这种可扩展的编程模型允许GPU架构通过简单地扩展多处理器和内存分区的数量来跨越广泛的市场范围:从高性能的GeForce图形处理器和专业的Quadro和Tesla计算产品到各种廉价的主流GeForce图形处理器(查看所有cuda支持的图形处理器列表)

*注意:GPU是围绕一个流多处理器阵列(SMs)构建的(更多细节见硬件实现)。一个多线程程序被划分为多个独立执行的线程块，这样多处理器的GPU会比少处理器的GPU自动执行程序的时间更短。*

### 1.4 文档结构

本文档分为以下几章:

- 本章简介是对CUDA的一般介绍。
- Chapter Programming Model概述CUDA编程模型。
- Chapter Programming Interface描述了编程接口。
- 第1章Hardware Implementation描述硬件实现。
- Chapter Performance Guidelines提供了一些关于如何达到最大的指导的性能。
- 附录cuda开启的gpu列表所有cuda开启的设备。
- 附录c++语言扩展，‣所有c++扩展的详细描述语言。
- 附录Cooperative Groups描述各种组的同步原语CUDA线程。
- 附录CUDA Dynamic Parallelism描述了如何启动和同步一个内核从另一个。
- 附录数学函数列出CUDA中支持的数学函数。
- 附录c++语言支持列出设备代码中支持的c++特性。
- 附录纹理抓取提供了纹理抓取的更多细节
- 附录计算能力给出了各种设备的技术规格，如还有更多的架构细节。
- 附录Driver API介绍底层驱动API。
- 附录CUDA环境变量列出所有CUDA环境变量。
- 附录Unified Memory Programming，运行统一内存编程模型。



## 2 编程模型

本章介绍了CUDA编程模型背后的主要概念，概述了它们是如何在c++中公开的。在《编程接口》中对CUDA c++作了详细的描述。

本章和下一章中使用的矢量加法示例的完整代码可以在vectorAdd CUDA示例中找到。



### 2.1 内核

CUDA c++扩展了c++，允许程序员定义c++函数，称为内核，当调用时，由N个不同的CUDA线程并行执行N次，而不是像普通c++函数一样只执行一次。

一个内核是用__global__声明说明符定义的，并且为一个给定的内核调用执行该内核的CUDA线程的数量是用一个新的<<<…>>>执行配置语法(参见c++语言扩展)。每个执行内核的线程都有一个唯一的线程ID，可以在内核中通过内置变量访问这个线程ID。

下面的示例代码使用内置变量threadIdx，将两个大小为N的向量A和B相加，并将结果存储到向量C中:

```
// Kernel definition
__global__ void VecAdd(float* A, float* B, float* C)
{
int i = threadIdx.x;
C[i] = A[i] + B[i];
}
int main()
{
...
// Kernel invocation with N threads
VecAdd<<<1, N>>>(A, B, C);
...
}
```

这里，每个执行VecAdd()的N个线程都执行一次成对加法。



### 2.2 线程层次结构

为了方便起见，threadadidx是一个三分量向量，因此可以使用一维、二维或三维线程索引来标识线程，形成一个一维、二维或三维的线程块，称为线程块。这提供了一种跨越域(如向量、矩阵或卷)中的元素调用计算的自然方法。

一个线程的索引和它的线程ID以一种简单的方式相互关联:对于一维块，它们是相同的;对于大小为(Dx, Dy)的二维块，index (x, y)线程的线程ID为(x + y Dx);对于大小为(Dx, Dy, Dz)的三维块，index (x, y, z)线程的线程ID为(x + y Dx + z Dx Dy)。

下面的代码将两个大小为NxN的矩阵A和B相加，并将结果存储到矩阵C中:

```
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
int i = threadIdx.x;
int j = threadIdx.y;
C[i][j] = A[i][j] + B[i][j];
}
int main()
{
...
// Kernel invocation with one block of N * N * 1 threads
int numBlocks = 1;
dim3 threadsPerBlock(N, N);
MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
...
}
```

每个块的线程数是有限制的，因为一个块的所有线程都驻留在同一个处理器核心上，并且必须共享该核心的有限内存资源。在当前gpu上，一个线程块最多可以包含1024个线程。

但是，一个内核可以由多个形状相同的线程块执行，因此线程的总数等于每个块的线程数乘以块的数量。

块被组织成一个一维、二维或三维的线程块网格，如图4所示。网格中的线程块的数量通常由正在处理的数据的大小决定，这通常超过系统中处理器的数量。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南04.PNG)

<<<…中指定的每个块的线程数和每个网格的块数。>>>语法的类型可以是int或dim3。二维块或网格可以在上面的例子中指定。

网格中的每个块都可以通过一个一维、二维或三维的唯一索引来标识，该索引可以通过内置的blockIdx变量在内核中访问。在内核中，可以通过内置的blockDim变量访问线程块的维度。

扩展前面的MatAdd()示例以处理多个块，代码变成如下所示。

```
// Kernel definition
__global__ void MatAdd(float A[N][N], float B[N][N],
float C[N][N])
{
int i = blockIdx.x * blockDim.x + threadIdx.x;
int j = blockIdx.y * blockDim.y + threadIdx.y;
if (i < N && j < N)
C[i][j] = A[i][j] + B[i][j];
}
int main()
{
...
// Kernel invocation
dim3 threadsPerBlock(16, 16);
dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);
MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);
...
}
```

线程块大小为16x16(256个线程)，虽然在本例中是任意的，但这是一种常见的选择。网格是用足够的块创建的，每个矩阵元素都有一个线程。为了简单起见，本例假设每个维度中的每个网格的线程数能被该维度中的每个块的线程数整除，尽管情况不一定如此。

线程块必须独立执行:必须能够以任何顺序(并行或串行)执行它们。这种独立性要求允许在任意数量的内核上以任意顺序调度线程块，如图3所示，使程序员能够编写随内核数量伸缩的代码。

块中的线程可以通过共享一些共享内存来共享数据，并通过同步它们的执行来协调内存访问。更确切地说，可以通过调用___syncthreads()内在函数来在内核中指定同步点;__syncthreads()充当一个屏障，在这个屏障上，块中的所有线程必须等待，然后才允许任何线程继续。共享内存给出了一个使用共享内存的例子。除了__syncthreads()之外，Cooperative Groups API还提供了一组丰富的线程同步原语。

为了高效的合作，共享内存应该是靠近每个处理器核心的低延迟内存(很像L1缓存)，__syncthreads()应该是轻量级的。

### 2.3 内存层次结构

CUDA线程在执行过程中可以从多个内存空间访问数据，如图5所示。每个线程都有私有的本地内存。每个线程块都有对该块的所有线程可见的共享内存，并且具有与该块相同的生存期。所有线程都可以访问相同的全局内存。

还有两个额外的只读内存空间可供所有线程访问:常量内存空间和纹理内存空间。全局、常量和纹理内存空间针对不同的内存使用进行了优化(请参阅设备内存访问)。纹理内存还为一些特定的数据格式提供了不同的寻址模式和数据过滤(参见纹理和表面内存)。

全局内存空间、常量内存空间和纹理内存空间是由同一个应用程序在内核启动时持久化的。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南05.PNG)

### 2.4 异构计算

如图6所示，CUDA编程模型假设CUDA线程在一个物理上独立的设备上执行，该设备作为运行c++程序的主机的协处理器。例如，当内核在GPU上执行，而c++程序的其余部分在CPU上执行时，就是这种情况。

CUDA编程模型还假设主机和设备都在DRAM中维护各自独立的内存空间，分别称为主机内存和设备内存。因此，程序通过调用CUDA运行时(在编程接口中描述)来管理内核可见的全局、常量和纹理内存空间。这包括设备内存的分配和回收，以及主机和设备内存之间的数据传输。

统一内存提供托管内存来桥接主机和设备内存空间。托管内存作为一个具有公共地址空间的单一、一致的内存映像，可以从系统中的所有cpu和gpu访问。该功能支持对设备内存的超额订阅，并且通过消除在主机和设备上显式地镜像数据的需要，可以极大地简化移植应用程序的任务。关于统一内存的介绍，请参见统一内存编程。

*注意:串行代码在主机上执行，而并行代码在设备上执行。*



### 2.5 计算能力

设备的计算能力由版本号表示，有时也称为“SM版本”。这个版本号标识了GPU硬件支持的特性，应用程序在运行时使用这个版本号来确定当前GPU上可用的硬件特性和/或指令。

计算能力由主要修订号X和次要修订号Y组成，由x.y表示

具有相同主版本号的设备具有相同的核心架构。设备的主要修订号是8安培NVIDIA GPU的体系结构的基础上,7基于沃尔塔设备架构,6设备基于帕斯卡架构,5设备基于麦克斯韦架构,3基于开普勒架构的设备,2设备基于费米架构,1为基于特斯拉架构的设备。

次要修订号对应于对核心体系结构的增量改进，可能包括新特性。

图灵是计算能力7.5设备的架构，是基于Volta架构的增量更新。

CUDA-Enabled gpu列出了所有CUDA-Enabled设备及其计算能力。计算能力给出了每个计算能力的技术规格。

注意:特定GPU的计算能力版本不应与CUDA版本混淆(例如，CUDA 7.5, CUDA 8, CUDA 9)，后者是CUDA软件平台的版本。应用程序开发人员使用CUDA平台来创建运行在许多代GPU架构上的应用程序，包括未来尚未发明的GPU架构。CUDA平台的新版本通常通过支持新的GPU架构的计算能力版本来增加对该架构的本地支持，而CUDA平台的新版本通常还包括独立于硬件生成的软件功能。

从CUDA 7.0和CUDA 9.0开始，特斯拉和费米架构就不再受支持



## 3 编程接口

CUDA c++为熟悉c++编程语言的用户提供了一条简单的路径，可以方便地编写设备执行的程序。

它由一个最小的c++语言扩展集和一个运行时库组成。

核心语言扩展已在编程模型中介绍。它们允许程序员将内核定义为c++函数，并在每次调用函数时使用一些新的语法来指定网格和块维。所有扩展的完整描述可以在c++语言扩展中找到。任何包含这些扩展名的源文件都必须使用nvcc编译，如nvcc编译中所述。

CUDA运行时中引入了运行时。它提供在主机上执行的C和c++函数来分配和释放设备内存，在主机内存和设备内存之间传输数据，管理具有多个设备的系统等。完整的运行时描述可以在CUDA参考手册中找到。

运行时是建立在一个较低级的C API之上，CUDA驱动程序API，也可以被应用程序访问。驱动程序API通过暴露低级别的概念提供了额外的控制级别，例如CUDA上下文——设备的宿主进程的模拟——和CUDA模块——设备的动态加载库的模拟。大多数应用程序不使用驱动程序API，因为它们不需要这种额外的控制级别，并且在使用运行时时，上下文和模块管理是隐式的，从而产生更简洁的代码。由于运行时可与驱动程序API互操作，大多数需要某些驱动程序API特性的应用程序可以默认使用运行时API，而只在需要时使用驱动程序API。驱动程序API在驱动程序API中介绍，并在参考手册中详细描述。

### 3.1 使用NVCC编译

内核可以使用CUDA指令集架构(称为PTX)来编写，该架构在PTX参考手册中有描述。然而，通常使用高级编程语言(如c++)更有效。在这两种情况下，内核必须被nvcc编译成二进制代码才能在设备上执行。

nvcc是一个编译器驱动程序，它简化了编译c++或PTX代码的过程:它提供了简单和熟悉的命令行选项，并通过调用实现不同编译阶段的工具集合来执行它们。本节概述nvcc工作流和命令选项。完整的描述可以在nvcc用户手册中找到。



#### 3.1.1 编译工作流

##### 3.1.1.1 离线编译

用nvcc编译的源文件可以包含主机代码(即在主机上执行的代码)和设备代码(即在设备上执行的代码)的混合。Nvcc的基本工作流程是将设备代码从主机代码中分离出来，然后:

- 将设备代码编译为装配形式(PTX代码)和/或二进制形式(立方对象)，
- 并通过替换<<<…来修改主机代码>>>语法在kernel中引入(在执行配置中详细描述)，通过必要的CUDA运行时函数调用来从PTX代码和/或cubin对象加载和启动每个编译过的内核。

修改后的宿主代码要么输出为c++代码，让其他工具编译，要么通过让nvcc在最后编译阶段调用宿主编译器直接作为目标代码。

应用程序可以:

- 任何一个指向编译后的主机代码的链接(这是最常见的情况)，
- 所有修改过的主机代码(如果有的话)，使用CUDA驱动API(参见驱动API)加载并执行PTX代码或cubin对象。

##### 3.1.1.2 即时编译

应用程序在运行时加载的任何PTX代码都被设备驱动程序进一步编译为二进制代码。这称为即时编译。即时编译增加了应用程序加载时间，但允许应用程序从每个新设备驱动程序带来的任何新的编译器改进中受益。这也是使应用程序在编译时还不存在的设备上运行的唯一方法，详细内容见应用程序兼容性。

当设备驱动程序为某些应用程序即时编译某些PTX代码时，它会自动缓存生成的二进制代码的一个副本，以避免在随后的应用程序调用中重复编译。当设备驱动程序升级时，缓存(称为计算缓存)将自动失效，因此应用程序可以从设备驱动程序中内置的新的即时编译器的改进中获益。

环境变量可用于控制实时编译，如CUDA环境变量中所述

作为一种替代使用nvcc编译CUDA c++设备代码，NVRTC可以用来编译CUDA c++设备代码到PTX在运行时。NVRTC是一个运行时编译库CUDA c++;更多信息可以在NVRTC用户指南中找到。

#### 3.1.2 二进制兼容性

二进制代码是特定于体系结构的。使用编译器选项-code生成cubin对象，该选项指定目标体系结构:例如，使用-code=sm_35编译会为具有3.5计算能力的设备生成二进制代码。二进制兼容性可以保证从一个小版本到下一个小版本，但不能保证从一个小版本到前一个或跨多个大版本。换句话说，为计算能力X.y生成的cubin对象只会在具有计算能力X.z的设备上执行，其中z≥y。

*注意:二进制兼容性仅支持桌面。Tegra不支持它。此外，不支持桌面和Tegra之间的二进制兼容性。*

#### 3.1.3 PTX兼容性

一些PTX指令只支持具有更高计算能力的设备。例如，Warp Shuffle功能只支持计算能力3.0及以上的设备。arch编译器选项指定了将c++编译为PTX代码时假定的计算能力。例如，包含warp shuffle的2C代码必须使用-arch=compute_30(或更高)进行编译。

为某些特定计算能力而产生的PTX代码总是可以编译为具有更大或相等计算能力的二进制代码。请注意，从较早的PTX版本编译的二进制文件可能不会使用某些硬件特性。例如，从为计算能力6.0 (Pascal)生成的PTX编译的计算能力7.0 (Volta)的二进制目标设备将不会使用张量核心指令，因为这些指令在Pascal上不可用。因此，最终的二进制文件可能比使用最新版本的PTX生成的二进制文件性能更差。

#### 3.1.4 应用程序兼容性

要在具有特定计算能力的设备上执行代码，应用程序必须加载与该计算能力兼容的二进制或PTX代码，如二进制兼容性和PTX兼容性中所述。特别是，为了能够在未来具有更高计算能力(目前还不能生成二进制代码)的体系结构上执行代码，应用程序必须加载将为这些设备实时编译的PTX代码(参见实时编译)。

哪些PTX和二进制代码嵌入到CUDA c++应用程序中，是由- arch和-code编译器选项或-gencode编译器选项控制的，详见nvcc用户手册。例如,

```
nvcc x.cu
-gencode arch=compute_50,code=sm_50
-gencode arch=compute_60,code=sm_60
-gencode arch=compute_70,code=\'compute_70,sm_70\'
```

嵌入二进制代码兼容计算能力5.0和6.0(第一和第二gencode选项)和PTX和二进制代码兼容计算能力7.0(第三gencode选项)

生成宿主代码是为了在运行时自动选择要加载和执行的最合适的代码，在上面的例子中，将是:

- 5.0二进制代码的设备计算能力5.0和5.2,
- 6.0二进制代码的设备计算能力6.0和6.1,
- 7.0二进制代码的设备计算能力7.0和7.5,
- PTX代码编译成二进制代码在运行时设备的计算能力8.0和8.6。

x.Cu可以有一个使用warp shuffle操作的优化代码路径，例如，只有计算能力为3.0或更高的设备才支持这种操作。__CUDA_ARCH__宏可以用来区分基于计算能力的各种代码路径。它仅为设备代码定义。例如，当使用-arch=compute_35编译时，__CUDA_ARCH__等于350。

使用驱动程序API的应用程序必须编译代码来分离文件，并在运行时显式地加载和执行最适当的文件。

Volta架构引入了独立线程调度，它改变了GPU上线程调度的方式。对于依赖于以前体系结构中SIMT调度的特定行为的代码，独立线程调度可能会改变参与线程的集合，从而导致不正确的结果。为了帮助迁移，同时实现独立线程调度中详细介绍的纠正措施，Volta开发人员可以选择使用编译器选项组合-arch=compute_60 -code=sm_70来实现Pascal的线程调度。

nvcc用户手册列出了-arch、-code和-gencode的各种缩写

编译器选项。例如，-arch=sm_70是-arch=compute_70 - code=compute_70,sm_70的简写(与-gencode arch=compute_70, code=\'compute_70,sm_70\'相同)

#### 3.1.5 c++的兼容性

编译器的前端按照c++语法规则处理CUDA源文件。宿主代码支持全c++。然而，正如c++语言支持中描述的那样，设备代码只完全支持c++的一个子集。

#### 3.1.6 64位的兼容性

64位版本的nvcc以64位模式编译设备代码(即，指针是64位的)。64位模式下编译的设备代码只支持64位模式下编译的主机代码。

类似地，32位版本的nvcc以32位模式编译设备代码，而以32位模式编译的设备代码仅在以32位模式编译的主机代码中得到支持。

32位版本的nvcc也可以使用-m64编译器选项以64位模式编译设备代码。

nvcc的64位版本还可以使用-m32编译器选项以32位模式编译设备代码。



### 3.2 CUDA 运行时

运行时在cudart库中实现，该库通过cudart静态链接到应用程序。自由或libcudart。A，或动态通过cudart.dll或libudart .so。需要cudart.dll和/或cudart的应用程序。因此，对于动态链接，通常将它们作为应用程序安装包的一部分。只有在链接到CUDA运行时相同实例的组件之间传递CUDA运行时符号的地址才是安全的。

所有的函数入口点都有cuda前缀

正如在异构编程中提到的，CUDA编程模型假设一个由主机和设备组成的系统，每个主机和设备都有自己单独的内存。Device Memory概述了用于管理设备内存的运行时函数。

共享内存演示了如何使用线程层次结构中引入的共享内存来最大化性能

主机内存引入了页锁定主机内存，用于将内核执行与主机和设备内存之间的数据传输重叠。

异步并发执行描述了用于在系统的不同级别上启用异步并发执行的概念和API。

Multi-Device System显示了如何将编程模型扩展到具有多个设备连接到同一主机的系统。

错误检查介绍如何正确检查运行时产生的错误

调用栈提到了用于管理CUDA c++调用栈的运行时函数

纹理和表面存储器表示纹理和表面存储器空间，提供了另一种方式来访问设备存储器;它们也暴露了GPU纹理硬件的一个子集。

图形互操作性介绍了运行时提供的与两个主要图形api OpenGL和Direct3D互操作的各种功能

#### 3.2.1 初始化

运行时没有显式的初始化函数;它初始化第一次调用运行时函数(更具体地说，除了参考手册中的错误处理和版本管理部分的函数之外的任何函数)。在对运行时函数调用计时以及将第一次调用的错误代码解释到运行时时，需要记住这一点。

运行时为系统中的每个设备创建一个CUDA上下文(关于CUDA上下文的更多细节请参见上下文)。该上下文是该设备的主要上下文，并在需要该设备上的活动上下文的第一个运行时函数中初始化。它在应用程序的所有宿主线程之间共享。作为此上下文创建的一部分，设备代码将在必要时及时编译(请参阅即时编译)并加载到设备内存中。这一切都是透明的。如果需要，例如驱动程序API互操作性，设备的主要上下文可以从运行时和驱动程序API之间的互操作性中描述的驱动程序API访问。

当宿主线程调用cudaDeviceReset()时，这会破坏宿主线程当前操作的设备的主上下文(即在设备选择中定义的当前设备)。将此设备作为当前设备的任何宿主线程所发出的下一次运行时函数调用将为该设备创建一个新的主上下文。

#### 3.2.2 设备存储

正如在异构编程中提到的，CUDA编程模型假设一个由主机和设备组成的系统，每个主机和设备都有自己单独的内存。内核在设备内存之外运行，因此运行时提供了分配、释放和复制设备内存的函数，以及在主机内存和设备内存之间传输数据。

设备存储器可以被分配为线性存储器或CUDA阵列。

CUDA数组是为纹理获取而优化的不透明内存布局。它们在纹理和表面记忆中被描述。

线性内存是在一个统一的地址空间中分配的，这意味着分别分配的实体可以通过指针彼此引用，例如，在二叉树或链表中。地址空间大小取决于主机系统(CPU)和使用的GPU的计算能力:

|                                          | x86_64(AMD64) | POWER(ppc64le) | ARM64       |
| ---------------------------------------- | ------------- | -------------- | ----------- |
| up to compute capability 5.3 (Maxwell)   | 40bit         | 40bit          | 40bit       |
| compute capability 6.0 (Pascal) or newer | up to 47bit   | up to 49bit    | up to 48bit |

*注意:在计算能力5.3 (Maxwell)和更早的设备上，CUDA驱动创建一个未提交的40位虚拟地址预留，以确保内存分配(指针)落入支持的范围。这个预留显示为预留虚拟内存，但在程序实际分配内存之前不占用任何物理内存。*线性内存通常使用cudaMalloc()分配，并使用cudaFree()释放。

主机内存和设备内存之间的数据传输通常使用cudaMemcpy()完成。在kernel的vector加法代码示例中，vector需要从主机内存复制到设备内存

```c++
// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
int i = blockDim.x * blockIdx.x + threadIdx.x;
if (i < N)
C[i] = A[i] + B[i];
}
// Host code
int main()
{
int N = ...;
size_t size = N * sizeof(float);
// Allocate input vectors h_A and h_B in host memory
float* h_A = (float*)malloc(size);
float* h_B = (float*)malloc(size);
// Initialize input vectors
...
// Allocate vectors in device memory
float* d_A;
cudaMalloc(&d_A, size);
float* d_B;
cudaMalloc(&d_B, size);
float* d_C;
cudaMalloc(&d_C, size);
// Copy vectors from host memory to device memory
cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
// Invoke kernel
int threadsPerBlock = 256;
int blocksPerGrid =
(N + threadsPerBlock - 1) / threadsPerBlock;
VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);
// Copy result from device memory to host memory
// h_C contains the result in host memory
cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
// Free device memory
cudaFree(d_A);
cudaFree(d_B);
cudaFree(d_C);
// Free host memory
...
}
```

线性内存也可以通过cudaMallocPitch()和cudaMalloc3D()来分配。这些函数被推荐用于2D或3D数组的分配，因为它确保分配被适当填充，以满足设备内存访问中描述的对齐要求，因此，当访问行地址或在2D数组和设备内存的其他区域之间执行复制时，确保最佳性能(使用cudaMemcpy2D()和cudaMemcpy3D()函数)。返回的间距(或步幅)必须用于访问数组元素。下面的代码示例分配了一个宽x高的2D浮点值数组，并演示了如何在设备代码中循环数组元素:

```c++
// Host code
int width = 64, height = 64;
float* devPtr;
size_t pitch;
cudaMallocPitch(&devPtr, &pitch,
width * sizeof(float), height);
MyKernel<<<100, 512>>>(devPtr, pitch, width, height);
// Device code
__global__ void MyKernel(float* devPtr,
size_t pitch, int width, int height)
{
for (int r = 0; r < height; ++r) {
float* row = (float*)((char*)devPtr + r * pitch);
for (int c = 0; c < width; ++c) {
float element = row[c];
}
}
}
```

下面的代码示例分配了一个宽x高x深的浮点值3D数组，并演示了如何在设备代码中循环数组元素:

```c++
// Host code
int width = 64, height = 64, depth = 64;
cudaExtent extent = make_cudaExtent(width * sizeof(float),
height, depth);
cudaPitchedPtr devPitchedPtr;
cudaMalloc3D(&devPitchedPtr, extent);
MyKernel<<<100, 512>>>(devPitchedPtr, width, height, depth);
// Device code
__global__ void MyKernel(cudaPitchedPtr devPitchedPtr,
int width, int height, int depth)
{
char* devPtr = devPitchedPtr.ptr;
size_t pitch = devPitchedPtr.pitch;
size_t slicePitch = pitch * height;
for (int z = 0; z < depth; ++z) {
char* slice = devPtr + z * slicePitch;
for (int y = 0; y < height; ++y) {
float* row = (float*)(slice + y * pitch);
for (int x = 0; x < width; ++x) {
float element = row[x];
}
}
}
}
```

*注意:为了避免分配太多内存从而影响整个系统的性能，请根据问题大小向用户请求分配参数。如果分配失败，你可以回退到其他较慢的内存类型(cudaMallocHost()， cudaHostRegister()等)，或返回一个错误，告诉用户需要多少内存被拒绝。如果您的应用程序由于某些原因不能请求分配参数，我们建议使用cudaMallocManaged()为支持它的平台。*

参考手册列出了所有用于在cudaMalloc()分配的线性内存、cudaMallocPitch()或cudaMalloc3D()分配的线性内存、CUDA数组和在全局或常量内存空间中声明的变量之间复制内存的各种函数。

下面的代码示例演示了通过运行时API访问全局变量的各种方法:

```c++
__constant__ float constData[256];
float data[256];
cudaMemcpyToSymbol(constData, data, sizeof(data));
cudaMemcpyFromSymbol(data, constData, sizeof(data));
__device__ float devData;
float value = 3.14f;
cudaMemcpyToSymbol(devData, &value, sizeof(float));
__device__ float* devPointer;
float* ptr;
cudaMalloc(&ptr, 256 * sizeof(float));
cudaMemcpyToSymbol(devPointer, &ptr, sizeof(ptr));
```

cudaGetSymbolAddress()用于检索指向为在全局内存空间中声明的变量分配的内存的地址。分配内存的大小是通过cudaGetSymbolSize()获得的。

#### 3.2.3 设备内存L2访问管理

当CUDA内核重复访问全局内存中的数据区域时，这种数据访问可以被认为是持久化的。另一方面，如果数据只被访问一次，这样的数据访问可以被认为是流的。

从CUDA 11.0开始，具有8.0及以上计算能力的设备具有影响L2缓存中数据持久性的能力，可能为全局内存提供更高的带宽和更低的延迟访问。

##### 3.2.3.1. L2缓存预留用于持久化访问

可以留出一部分L2缓存，用于将数据访问持久化到全局内存。持久化访问优先使用L2缓存的这一部分，而普通或流访问全局内存只能使用L2的这一部分，当它通过持久化访问未被使用时。

L2缓存预留大小可以在一定的限制下进行调整:

```c++
cudaGetDeviceProperties(&prop, device_id);
size_t size = min(int(prop.l2CacheSize * 0.75), prop.persistingL2CacheMaxSize);
cudaDeviceSetLimit(cudaLimitPersistingL2CacheSize, size); /* set-aside 3/4 of L2
cache for persisting accesses or the max allowed*/
```

当GPU配置为MIG (Multi-Instance GPU)模式时，L2 cache set-aside功能将被关闭。

当使用多进程服务(MPS)时，L2缓存设置的大小不能被cudaDeviceSetLimit改变。相反，设置大小只能在MPS服务器启动时通过环境变量CUDA_DEVICE_DEFAULT_PERSISTING_L2_CACHE_PERCENTAGE_LIMIT指定。

##### 3.2.3.2 L2持久化访问策略

访问策略窗口指定全局内存的一个连续区域和L2缓存中用于访问该区域的持久性属性。

下面的代码示例展示了如何使用CUDA流设置L2持久化访问窗口。

CUDA Stream Example  

```c++
cudaStreamAttrValue stream_attribute; //
Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr = reinterpret_cast<void*>(ptr); //
Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = num_bytes; //
Number of bytes for persistence access.
//
(Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)
stream_attribute.accessPolicyWindow.hitRatio = 0.6; //
Hint for cache hit ratio
stream_attribute.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting; //
Type of access property on cache hit
stream_attribute.accessPolicyWindow.missProp = cudaAccessPropertyStreaming; //
Type of access property on cache miss.
//Set the attributes to a CUDA stream of type cudaStream_t
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow,
&stream_attribute);
```

当内核随后在CUDA流中执行时，对全局内存区段[ptr. ptr+num_bytes]的内存访问更可能在L2缓存中持久，而不是访问其他全局内存位置。

也可以为CUDA图核节点设置L2持久性，示例如下:

CUDA GraphKernelNode Example  

```c++
cudaKernelNodeAttrValue node_attribute; //
Kernel level attributes data structure
node_attribute.accessPolicyWindow.base_ptr = reinterpret_cast<void*>(ptr); //
Global Memory data pointer
node_attribute.accessPolicyWindow.num_bytes = num_bytes; //
Number of bytes for persistence access.
// (Must
be less than cudaDeviceProp::accessPolicyMaxWindowSize)
node_attribute.accessPolicyWindow.hitRatio = 0.6; // Hint
for cache hit ratio
node_attribute.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting; // Type
of access property on cache hit
node_attribute.accessPolicyWindow.missProp = cudaAccessPropertyStreaming; // Type
of access property on cache miss.
//Set the attributes to a CUDA Graph Kernel node of type cudaGraphNode_t
cudaGraphKernelNodeSetAttribute(node, cudaKernelNodeAttributeAccessPolicyWindow,
&node_attribute);
```

hitRatio参数可用于指定接收hitProp属性的访问的比例。在上面的两个例子中，全局内存区域[ptr. ptr+num_bytes]中60%的内存访问具有持久化属性，40%的内存访问具有流属性。哪些特定的内存访问被分类为持久化(hitProp)是随机的，其概率近似为hitRatio;概率分布取决于硬件架构和存储范围。

例如，如果L2预留缓存大小是16KB，而accessPolicyWindow中的num_bytes是32KB:

- 当hitRatio为0.5时，硬件将随机选择32KB窗口中的16KB指定为持久化并缓存在预留的L2缓存区域中。
- 在hitRatio为1.0的情况下，硬件将尝试在设置好的L2缓存区域中缓存整个32KB窗口。由于set-aside区域比窗口小，缓存行将被逐出，以在L2缓存的set-aside部分保留最近使用的32KB数据中的16KB。

因此，hitRatio可以用来避免缓存线的抖动，并从总体上减少移动到L2缓存和搬出的数据量。

一个低于1.0的hitRatio值可以用来手动控制来自并发CUDA流的不同accessPolicyWindows的数据量。例如，设L2预留缓存大小为16KB;在两个不同的CUDA流中的两个并发内核，每个都有一个16KB的accessPolicyWindow，并且hitRatio值都是1.0，当竞争共享的L2资源时，可能会驱逐彼此的缓存线。但是，如果两个accessPolicyWindows的hitRatio值都是0.5，它们将不太可能逐出自己的或彼此的持久化缓存线。

##### 3.2.3.3 L2访问属性

为不同的全局内存数据访问定义了三种类型的访问属性:

1. cudaAccessPropertyStreaming:与streaming属性一起发生的内存访问不太可能在L2缓存中持久，因为这些访问会优先被逐出。
2. cudaAccessPropertyPersisting:与持久化属性一起发生的内存访问更有可能持久化到L2缓存中，因为这些访问优先保留在L2缓存的预留部分。
3. cudaAccessPropertyNormal:该访问属性强制将以前应用的持久化访问属性重置为正常状态。使用之前CUDA内核的持久化属性的内存访问可能会在L2缓存中保留很长时间。这种使用后持久化减少了不使用持久化属性的后续内核可用的L2缓存数量。使用cudaAccessPropertyNormal属性重置访问属性窗口将删除优先访问的持久化(优先保留)状态，就像先前的访问没有访问属性一样。

##### 3.2.3.4 L2持久性的例子

下面的例子展示了如何为持久访问设置L2缓存，通过CUDA流在CUDA内核中使用setaside L2缓存，然后重置L2缓存。

```c++
cudaStream_t stream;
cudaStreamCreate(&stream);
// Create CUDA stream
cudaDeviceProp prop;
// CUDA device properties variable
cudaGetDeviceProperties( &prop, device_id);
// Query GPU properties
size_t size = min( int(prop.l2CacheSize * 0.75) , prop.persistingL2CacheMaxSize );
cudaDeviceSetLimit( cudaLimitPersistingL2CacheSize, size);
// set-aside 3/4 of L2 cache for persisting accesses or the max allowed
size_t window_size = min(prop.accessPolicyMaxWindowSize, num_bytes);
// Select minimum of user defined num_bytes and max window size.
cudaStreamAttrValue stream_attribute;
// Stream level attributes data structure
stream_attribute.accessPolicyWindow.base_ptr = reinterpret_cast<void*>(data1);
// Global Memory data pointer
stream_attribute.accessPolicyWindow.num_bytes = window_size;
// Number of bytes for persistence access
stream_attribute.accessPolicyWindow.hitRatio = 0.6;
// Hint for cache hit ratio
stream_attribute.accessPolicyWindow.hitProp = cudaAccessPropertyPersisting;
// Persistence Property
stream_attribute.accessPolicyWindow.missProp = cudaAccessPropertyStreaming;
// Type of access property on cache miss
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow,
&stream_attribute); // Set the attributes to a CUDA Stream
for(int i = 0; i < 10; i++) {
cuda_kernelA<<<grid_size,block_size,0,stream>>>(data1);
// This data1 is used by a kernel multiple times
}
// [data1 + num_bytes) benefits from L2 persistence
cuda_kernelB<<<grid_size,block_size,0,stream>>>(data1);
// A different kernel in the same stream can also benefit
// from the persistence of data1
stream_attribute.accessPolicyWindow.num_bytes = 0;
// Setting the window size to 0 disable it
cudaStreamSetAttribute(stream, cudaStreamAttributeAccessPolicyWindow,
&stream_attribute); // Overwrite the access policy attribute to a CUDA Stream
cudaCtxResetPersistingL2Cache();
// Remove any persistent lines in L2
cuda_kernelC<<<grid_size,block_size,0,stream>>>(data2);
// data2 can now benefit from full L2 in normal mode
```

##### 3.2.3.5 将L2访问设置为正常

一个来自以前CUDA内核的持久化L2缓存线可能会在L2被使用后很长时间内持久化。因此，对L2缓存的正常复位对于流或正常内存访问以正常优先级利用L2缓存是很重要的。有三种方法可以将持久化访问重置为正常状态。

1. 用访问属性cudaAccessPropertyNormal重置之前的持久化内存区域。
2. 通过调用cudaCtxResetPersistingL2Cache()将所有持久化的L2缓存线重置为正常。
3. 最终未触及的线路会自动复位到正常状态。强烈建议不要依赖自动复位，因为自动复位所需的时间长度是不确定的。

##### 3.2.3.6 管理L2预留缓存的利用率

在不同的CUDA流中同时执行的多个CUDA内核可能会给它们的流分配不同的访问策略窗口。然而，L2留出的缓存部分在所有这些并发CUDA内核中共享。因此，这个预留缓存部分的净利用率是所有并发内核单独使用的总和。当持久化访问的数量超过了预留的L2缓存容量时，将内存访问指定为持久化访问的好处就会减少。

为了管理备用L2缓存部分的利用率，应用程序必须考虑以下因素:

- L2备用缓存的大小。
- CUDA所有可以并发执行的内核。
- 所有可能并发执行的CUDA内核的访问策略窗口。
- 在什么时候和如何需要L2重置来允许正常或流访问以同等优先级利用之前设置好的L2缓存。

##### 3.2.3.7 查询L2缓存属性

与L2缓存相关的属性是cudaDeviceProp结构的一部分，可以使用CUDA运行时API cudaGetDeviceProperties查询

CUDA设备属性包括:

- l2CacheSize: GPU上可用的L2缓存量。
- persistingL2CacheMaxSize:可以为持久化内存访问留出的最大L2缓存量。
- accessPolicyMaxWindowSize:访问策略窗口的最大大小。

##### 3.2.3.8 控制L2缓存设置留出大小用于持久化内存访问

使用CUDA运行时API cudaDeviceGetLimit查询L2预留缓存大小，并使用CUDA运行时API cudaDeviceSetLimit设置为cudaLimit。设置该限制的最大值是cudaDeviceProp::persistingL2CacheMaxSize。

```c++
enum cudaLimit {
/* other fields not shown */
cudaLimitPersistingL2CacheSize
};
```

#### 3.2.4 共享存储

如变量内存空间说明符中所述，共享内存是使用_ __shared_ __内存空间说明符分配的。

正如线程层次结构中提到的，共享内存比全局内存要快得多，并在共享内存中详细介绍了这一点。它可以用作暂存存储器(或软件管理缓存)，以最小化从CUDA块的全局内存访问，如下面的矩阵乘法示例所示。

下面的代码示例是不利用共享内存的矩阵乘法的简单实现。每个线程读取A的一行和B的一列，并计算C的相应元素，如图7所示。因此，A是从全局内存中读取B的宽度时间，而B是从全局内存中读取A的高度时间。

```c++
// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.width + col)
typedef struct {
int width;
int height;
float* elements;
} Matrix;
// Thread block size
#define BLOCK_SIZE 16
// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);
// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
// Load A and B to device memory
Matrix d_A;
d_A.width = A.width; d_A.height = A.height;
size_t size = A.width * A.height * sizeof(float);
cudaMalloc(&d_A.elements, size);
cudaMemcpy(d_A.elements, A.elements, size,
cudaMemcpyHostToDevice);
Matrix d_B;
d_B.width = B.width; d_B.height = B.height;
size = B.width * B.height * sizeof(float);
cudaMalloc(&d_B.elements, size);
cudaMemcpy(d_B.elements, B.elements, size,
cudaMemcpyHostToDevice);
// Allocate C in device memory
Matrix d_C;
d_C.width = C.width; d_C.height = C.height;
size = C.width * C.height * sizeof(float);
cudaMalloc(&d_C.elements, size);
// Invoke kernel
dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);
// Read C from device memory
cudaMemcpy(C.elements, d_C.elements, size,
cudaMemcpyDeviceToHost);
// Free device memory
cudaFree(d_A.elements);
cudaFree(d_B.elements);
cudaFree(d_C.elements);
}
// Matrix multiplication kernel called by MatMul()
__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
// Each thread computes one element of C
// by accumulating results into Cvalue
float Cvalue = 0;
int row = blockIdx.y * blockDim.y + threadIdx.y;
int col = blockIdx.x * blockDim.x + threadIdx.x;
for (int e = 0; e < A.width; ++e)
Cvalue += A.elements[row * A.width + e]
* B.elements[e * B.width + col];
C.elements[row * C.width + col] = Cvalue;
}
```

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南07.PNG)

下面的代码示例是矩阵乘法的实现，它确实利用了共享内存。在这个实现中，每个线程块负责计算C的一个方子矩阵Csub，块中的每个线程负责计算Csub的一个元素。如图8所示，Csub等于两个矩形矩阵的乘积:维度A (A.width, block_size)的子矩阵与Csub的行索引相同，维度B (B . block_size, A.width)的子矩阵与Csub的列索引相同。为了适应设备的资源，这两个矩形矩阵被分为维数block_size的任意多个方阵，Csub被计算为这些方阵的乘积的和。执行这些产品时，首先将两个相应的方阵从全局内存加载到共享内存，一个线程加载每个矩阵的一个元素，然后让每个线程计算产品的一个元素。每个线程将这些产品的结果累积到一个寄存器中，完成后将结果写入全局内存。

通过这种方式阻塞计算，我们可以利用快速共享内存并节省大量全局内存带宽，因为a只从全局内存读取(B.width / block_size)次，而B只从全局内存读取(a .height / block_size)次。

前面代码示例中的Matrix类型被增加了一个stride字段，这样就可以用相同的类型有效地表示子矩阵。_ _device_ _函数用于获取和设置元素，并从一个矩阵构建任何子矩阵。

```c++
// Matrices are stored in row-major order:
// M(row, col) = *(M.elements + row * M.stride + col)
typedef struct {
int width;
int height;
int stride;
float* elements;
} Matrix;
// Get a matrix element
__device__ float GetElement(const Matrix A, int row, int col)
{
return A.elements[row * A.stride + col];
}
// Set a matrix element
__device__ void SetElement(Matrix A, int row, int col,
float value)
{
A.elements[row * A.stride + col] = value;
}
// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is
// located col sub-matrices to the right and row sub-matrices down
// from the upper-left corner of A
__device__ Matrix GetSubMatrix(Matrix A, int row, int col)
{
Matrix Asub;
Asub.width = BLOCK_SIZE;
Asub.height = BLOCK_SIZE;
Asub.stride = A.stride;
Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row
+ BLOCK_SIZE * col];
return Asub;
}
// Thread block size
#define BLOCK_SIZE 16
// Forward declaration of the matrix multiplication kernel
__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);
// Matrix multiplication - Host code
// Matrix dimensions are assumed to be multiples of BLOCK_SIZE
void MatMul(const Matrix A, const Matrix B, Matrix C)
{
// Load A and B to device memory
Matrix d_A;
d_A.width = d_A.stride = A.width; d_A.height = A.height;
size_t size = A.width * A.height * sizeof(float);
cudaMalloc(&d_A.elements, size);
cudaMemcpy(d_A.elements, A.elements, size,
cudaMemcpyHostToDevice);
Matrix d_B;
d_B.width = d_B.stride = B.width; d_B.height = B.height;
size = B.width * B.height * sizeof(float);
cudaMalloc(&d_B.elements, size);
cudaMemcpy(d_B.elements, B.elements, size,
cudaMemcpyHostToDevice);
// Allocate C in device memory
Matrix d_C;
d_C.width = d_C.stride = C.width; d_C.height = C.height;
size = C.width * C.height * sizeof(float);
cudaMalloc(&d_C.elements, size);
// Invoke kernel
dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);
dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);
MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);
// Read C from device memory
cudaMemcpy(C.elements, d_C.elements, size,
cudaMemcpyDeviceToHost);
// Free device memory
cudaFree(d_A.elements);
cudaFree(d_B.elements);
cudaFree(d_C.elements);
}
// Matrix multiplication kernel called by MatMul()
__global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)
{
// Block row and column
int blockRow = blockIdx.y;
int blockCol = blockIdx.x;
// Each thread block computes one sub-matrix Csub of C
Matrix Csub = GetSubMatrix(C, blockRow, blockCol);
// Each thread computes one element of Csub
// by accumulating results into Cvalue
float Cvalue = 0;
// Thread row and column within Csub
int row = threadIdx.y;
int col = threadIdx.x;
// Loop over all the sub-matrices of A and B that are
// required to compute Csub
// Multiply each pair of sub-matrices together
// and accumulate the results
for (int m = 0; m < (A.width / BLOCK_SIZE); ++m) {
// Get sub-matrix Asub of A
Matrix Asub = GetSubMatrix(A, blockRow, m);
// Get sub-matrix Bsub of B
Matrix Bsub = GetSubMatrix(B, m, blockCol);
// Shared memory used to store Asub and Bsub respectively
__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];
// Load Asub and Bsub from device memory to shared memory
// Each thread loads one element of each sub-matrix
As[row][col] = GetElement(Asub, row, col);
Bs[row][col] = GetElement(Bsub, row, col);
// Synchronize to make sure the sub-matrices are loaded
// before starting the computation
__syncthreads();
// Multiply Asub and Bsub together
for (int e = 0; e < BLOCK_SIZE; ++e)
Cvalue += As[row][e] * Bs[e][col];
// Synchronize to make sure that the preceding
// computation is done before loading two new
// sub-matrices of A and B in the next iteration
__syncthreads();
}
// Write Csub to device memory
// Each thread writes one element
SetElement(Csub, row, col, Cvalue);
}
```

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南08.PNG)

#### 3.2.5 页面锁定的主机内存

运行时提供了一些函数来允许使用页面锁定(也称为固定)主机内存(相对于malloc()分配的常规可分页主机内存):

- cudaHostAlloc()和cudaFreeHost()分配并释放页面锁定的主机内存;


- cudaHostRegister() page-locks malloc()分配的内存范围(限制参见参考手册)。

使用页面锁定的主机内存有几个好处:

- 页面锁定的主机内存和设备内存之间的拷贝可以与一些设备的内核执行并发执行，如异步并发执行中所述。
- 在某些设备上，页锁定的主机内存可以映射到设备的地址空间，而不需要将其复制到设备内存中或从设备内存中复制，详见映射内存。
- 在具有前端总线的系统上，如果主机内存被分配为页面锁定，那么主机内存和设备内存之间的带宽更高，如果另外分配为写组合内存(如写组合内存中描述的)，则带宽更高。

但是，页面锁定的主机内存是一种稀缺资源，因此在页面锁定内存中分配时会比在可分页内存中分配早很久。此外，通过减少操作系统用于分页的可用物理内存数量，消耗过多的页面锁定内存会降低整体系统性能。

*注意:页面锁定的主机内存不会缓存在非I/O相干的Tegra设备上。此外，cudaHostRegister()不支持非I/O相干Tegra设备。*

这个简单的零拷贝CUDA示例附带了关于页面锁定内存api的详细文档。

##### 3.2.5.1  Portable Memory

页面锁定内存块可以与系统中的任何设备一起使用(关于多设备系统的更多细节，请参阅多设备系统)，但默认情况下，使用上面描述的页锁定内存的好处，只有当块被分配时当前的设备(以及所有设备共享相同的统一地址空间，如果有的话，如统一虚拟地址空间中所述)才可用。为了使这些优势对所有设备都可用，块需要通过传递标志cudaHostAlloc()来分配，或者通过传递标志cudaHostRegisterPortable给cudaHostRegister()来分页锁定。

##### 3.2.5.2. Write-Combining Memory  

默认情况下，页面锁定的主机内存被分配为可缓存的。它可以通过传递标志cudahostallocwritecombcombined给cudaHostAlloc()来选择性地分配写组合。写组合内存释放了主机的L1和L2缓存资源，使更多的缓存可用于应用程序的其余部分。此外，在跨越PCI Express总线的传输过程中，写入组合内存不会被窥探，这可以提高传输性能高达40%。

从主机上读取写组合内存的速度非常慢，所以写组合内存通常应该用于主机只写入的内存。

##### 3.2.5.3. Mapped Memory  

一个页面锁定的主机内存块也可以通过传递标志cudahostlocmapped到cudaHostAlloc()或传递标志cudaHostRegisterMapped到cudaHostRegister()来映射到设备的地址空间。因此，这样的块通常有两个地址:一个在主机内存中，由cudaHostAlloc()或malloc()返回，另一个在设备内存中，可以使用cudaHostGetDevicePointer()检索，然后使用它从内核中访问块。唯一的例外是使用cudaHostAlloc()分配的指针，以及在统一虚拟地址空间中提到的主机和设备使用统一地址空间时。

从内核中直接访问主机内存不能提供与设备内存相同的带宽，但确实有一些优点:

1. 不需要在设备内存中分配一个块，并在此块和主机内存中的块之间复制数据;数据传输是内核根据需要隐式执行的;
2. 没有必要使用流(参见并发数据传输)来重叠数据传输与内核执行;源自内核的数据传输会自动与内核执行重叠。

但是，由于映射的页面锁定内存在主机和设备之间共享，应用程序必须使用流或事件同步内存访问(参见异步并发执行)，以避免任何潜在的写后读、读后写或写后写危险。

为了能够检索到任何映射的页锁内存的设备指针，页锁内存映射必须通过调用cudaetdeviceflags()在任何其他CUDA调用被执行之前使用cudaDeviceMapHost标志来启用。否则，cudaHostGetDevicePointer()将返回一个错误。

如果设备不支持，cudaHostGetDevicePointer()也会返回一个错误映射的页锁定主机内存。应用程序可以通过检查canMapHostMemory设备属性(参见设备枚举)来查询此功能，对于支持页面锁定主机内存映射的设备，该属性等于1。

注意，从主机或其他设备的角度来看，在映射的页锁定内存上操作的原子函数(参见原子函数)并不是原子函数。

还要注意，CUDA运行时要求从主机和其他设备的角度来看，1字节、2字节、4字节和8字节自然对齐到主机内存的加载和存储被保存为单次访问。在某些平台上，内存原子可能被硬件分解为独立的加载和存储操作。这些组件加载和存储操作对于保持自然对齐的访问有相同的要求。例如，CUDA运行时不支持PCI Express总线拓扑结构，其中PCI Express桥接将设备和主机之间的8字节自然对齐写入分成两个4字节写入。

#### 3.2.6 异步并发执行

CUDA公开了以下操作作为独立的任务，可以同时操作彼此:

- 主机上的计算;
- 设备上的计算;
- 内存从主机到设备的传输;
- 内存从设备到主机的传输;
- 内存在给定设备的内存中传输;
- 设备间内存传输。

这些操作之间实现的并发级别将取决于特性集和设备的计算能力，如下所述。



##### 3.2.6.1 主机和设备之间的并发执行

通过在设备完成所请求的任务之前将控制权返回给主机线程的异步库函数，可以促进并发主机执行。使用异步调用，当适当的设备资源可用时，许多设备操作可以排队一起由CUDA驱动程序执行。这减轻了主机线程管理设备的大部分责任，让它可以自由地执行其他任务。以下设备操作对于主机是异步的:

- 内核启动;
- 单个设备内存中的内存副本;
- 内存从主机拷贝到设备上一个64kb以下的内存块;
- 由带有Async后缀的函数执行的内存拷贝;
- 内存集函数调用。

程序员可以通过设置CUDA_LAUNCH_BLOCKING环境变量为1来全局禁用运行在系统上的所有CUDA应用程序的内核启动的异步性。此特性仅用于调试目的，不应该用作使生产软件可靠运行的方法。

如果硬件计数器通过分析器(sight, Visual profiler)收集，内核启动是同步的，除非启用了并发内核分析。如果异步内存拷贝涉及的主机内存不是页锁定的，那么异步内存拷贝也是同步的。

##### 3.2.6.2 内核并发执行

一些具有计算能力的设备2。X和更高版本可以同时执行多个内核。应用程序可以通过检查concurrentKernels设备属性(请参阅设备枚举)来查询此功能，对于支持它的设备，该属性等于1。

一个设备可以同时执行的内核启动的最大数量取决于它的计算能力，如表15所示。

来自一个CUDA上下文的内核不能与来自另一个CUDA上下文的内核同时执行.

使用许多纹理或大量本地内存的内核不太可能与其他内核并发执行。

##### 3.2.6.3 数据传输和内核执行的重叠

一些设备可以在内核执行的同时从GPU执行异步内存复制。应用程序可以通过检查asyncEngineCount设备属性(请参阅设备枚举)来查询该功能，对于支持该功能的设备，该属性的值大于零。如果主机内存涉及到该副本，则它必须是页锁定的。

也可以通过内核执行(在支持concurrentKernels设备属性的设备上)和/或从设备进行复制(对于支持asyncEngineCount属性的设备)来同时执行设备内复制。使用标准内存复制函数初始化设备内拷贝，目的地址和源地址位于同一设备上。

##### 3.2.6.4 并行数据传输

一些具有计算能力的设备2。X及以上版本可以重叠设备之间的拷贝。应用程序可以通过检查asyncEngineCount设备属性(请参阅设备枚举)来查询该功能，对于支持该功能的设备，该属性等于2。为了实现重叠，传输中涉及的任何主机内存都必须是页锁定的。

##### 3.2.6.5. Streams  

应用程序通过流管理上面描述的并发操作。流是按顺序执行的命令序列(可能由不同的主机线程发出)。另一方面，不同的流可能会以不同的顺序或并发地执行它们的命令;这种行为是不能保证的，因此不应该依赖于正确性(例如，内核间通信是未定义的)。在流上发出的命令可以在满足命令的所有依赖项时执行。依赖项可以是之前在同一流上启动的命令，也可以是来自其他流的依赖项。同步调用的成功完成保证了启动的所有命令都已完成。

###### 3.2.6.5.1. Creation and Destruction  

定义流的方法是创建一个stream对象，并将它指定为一系列内核启动和主机<->设备内存副本的stream参数。下面的代码示例创建了两个流，并在页锁定内存中分配一个浮点数组hostPtr。

```c++
cudaStream_t stream[2];
for (int i = 0; i < 2; ++i)
cudaStreamCreate(&stream[i]);
float* hostPtr;
cudaMallocHost(&hostPtr, 2 * size);
```

这些流中的每一个都由下面的代码示例定义为一个从主机到设备的内存拷贝、一个内核启动和一个从设备到主机的内存拷贝的序列:

```
for (int i = 0; i < 2; ++i) {
cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
size, cudaMemcpyHostToDevice, stream[i]);
MyKernel <<<100, 512, 0, stream[i]>>>
(outputDevPtr + i * size, inputDevPtr + i * size, size);
cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
size, cudaMemcpyDeviceToHost, stream[i]);
}
```

每个流将输入数组hostPtr的一部分复制到设备内存中的数组inputDevPtr，通过调用MyKernel()在设备上处理inputDevPtr，并将结果outputDevPtr复制回hostPtr的相同部分。重叠行为描述本例中的流如何根据设备的能力重叠。注意，hostPtr必须指向页锁定的主机内存，才能发生重叠。

流通过调用cudaStreamDestroy()来释放。

```c++
for (int i = 0; i < 2; ++i)
cudaStreamDestroy(stream[i]);
```

如果设备在调用cudaStreamDestroy()时仍在流中工作，该函数将立即返回，并且一旦设备完成流中的所有工作，与流相关的资源将自动释放。

###### 3.2.6.5.2. Default Stream

内核启动和主机<->设备内存拷贝没有指定任何流参数，或者等效地将流参数设置为0，将被发送到默认流。因此，他们是按顺序执行的。

对于使用——default-stream每线程编译标志(或者在包含CUDA头之前定义CUDA_API_PER_THREAD_DEFAULT_STREAM宏)编译的代码，默认流是一个常规流，每个主机线程有自己的默认流。

*注意:#define CUDA_API_PER_THREAD_DEFAULT_STREAM 1不能用于启用此行为时，nvcc编译的代码，因为nvcc隐式包含cuda_runtime.h在翻译单元的顶部。在这种情况下，需要使用——default-stream每线程编译标志，或者CUDA_API_PER_THREAD_DEFAULT_STREAM宏需要用-DCUDA_API_PER_THREAD_DEFAULT_STREAM=1编译标志来定义。*

对于使用——default-stream遗留编译标志编译的代码，默认流是一个称为NULL流的特殊流，每个设备都有一个用于所有主机线程的NULL流。NULL流是特殊的，因为它会导致隐式同步，如隐式同步中所述。

对于未指定——default-stream编译标志而编译的代码，将假定——default-stream遗留代码为默认代码。

###### 3.2.6.5.3 显式同步

有多种方法可以显式地使流彼此同步。

cudaDeviceSynchronize()等待所有主机线程的所有流中的所有前面的命令都完成。

cudaStreamSynchronize()接受流作为参数，并等待，直到给定流中的所有前面的命令都完成。它可以用于将主机与特定的流同步，从而允许其他流继续在设备上执行。

cudaStreamWaitEvent()将流和事件作为参数(关于事件的描述，请参阅Events)，并在调用cudaStreamWaitEvent()之后将所有添加到给定流的命令延迟执行，直到给定事件完成。

cudaStreamQuery()为应用程序提供了一种方法来知道流中所有前面的命令是否已经完成

###### 3.2.6.5.4. Implicit Synchronization  

来自不同流的两个命令不能同时运行，如果主机线程在它们之间发出以下任何操作:

- 页面锁定的主机内存分配，
- 一个设备内存分配
- 一个设备内存集
- 两个地址到同一设备内存之间的内存副本
- 任何CUDA命令到NULL流
- 在计算能力3中描述的L1/共享内存配置之间切换。计算能力7.x。

对于支持并发内核执行并具有3.0或更低计算能力的设备，任何需要依赖检查来查看流内核启动是否完成的操作:

- 只有在CUDA上下文中的任何流中所有先前内核启动的所有线程块都已经开始执行的时候才开始执行
- 停止所有以后在CUDA上下文中从任何流启动内核，直到检查内核启动完成。

需要依赖检查的操作包括与被检查的启动相同流中的任何其他命令，以及该流上对cudaStreamQuery()的任何调用。因此，应用程序应该遵循以下准则来提高并发内核执行的潜力:

- 所有独立操作都应该在依赖操作之前发出，
- 任何类型的同步都应该尽可能地延迟。

###### 3.2.6.5.5 重叠行为

两个流之间的执行重叠量取决于命令发送到每个流的顺序，以及设备是否支持数据传输和内核执行的重叠(参见数据传输和内核执行的重叠)，并发内核执行(参见并发内核执行)，和/或并发数据传输(参见并发数据传输)。

例如,在设备不支持并行数据传输,这两个流的代码示例创建和销毁不重叠,因为内存复制从主机到设备发出后流[1]从设备到主机的内存复制发给流[0],因此，它只能在从设备到主机发送到流[0]的内存副本完成后才能启动。如果代码以以下方式重写(并假设设备支持数据传输和内核执行的重叠)

```c++
for (int i = 0; i < 2; ++i)
cudaMemcpyAsync(inputDevPtr + i * size, hostPtr + i * size,
size, cudaMemcpyHostToDevice, stream[i]);
for (int i = 0; i < 2; ++i)
MyKernel<<<100, 512, 0, stream[i]>>>
(outputDevPtr + i * size, inputDevPtr + i * size, size);
for (int i = 0; i < 2; ++i)
cudaMemcpyAsync(hostPtr + i * size, outputDevPtr + i * size,
size, cudaMemcpyDeviceToHost, stream[i]);
```

然后发送到[1]流的从主机到设备的内存拷贝与发送到[0]流的内核启动重叠。

在支持并发数据传输的设备上，创建和销毁的两个代码流是重叠的:发送到[1]流的主机到设备的内存拷贝与发送到[0]流的设备到主机的内存拷贝重叠，甚至与发送到[0]流的内核启动重叠(假设设备支持数据传输和内核执行的重叠)。然而,对于设备的计算能力3.0或更低,内核执行不可能重叠,因为第二个内核启动后发给流[1]从设备到主机的内存复制发给流[0],所以它被阻塞直到第一个内核启动发给流[0]根据隐式同步完成。如果代码像上面那样重写，内核执行重叠(假设设备支持并发内核执行)，因为在从设备到主机的内存副本发送到[0]流之前，第二次内核启动被发送到[1]流。然而，在这种情况下，从设备到主机发出的内存拷贝只与隐式同步发出的内核启动到[1]的最后一个线程块重叠，这只能代表内核总执行时间的一小部分。

###### 3.2.6.5.6 host函数(回调)

运行时提供了一种方法，通过cudaLaunchHostFunc()在流的任何位置插入一个CPU函数调用。在回调完成之前，向流发出的所有命令都会在主机上执行所提供的函数。

下面的代码示例在向每个流发出主机到设备内存副本、内核启动和设备到主机内存副本之后，将主机函数MyCallback添加到每个流。在每个设备到主机的内存副本完成后，该函数将在主机上开始执行。

```
void CUDART_CB MyCallback(cudaStream_t stream, cudaError_t status, void *data){
printf("Inside callback %d\n", (size_t)data);
}
...
for (size_t i = 0; i < 2; ++i) {
cudaMemcpyAsync(devPtrIn[i], hostPtr[i], size, cudaMemcpyHostToDevice,
stream[i]);
MyKernel<<<100, 512, 0, stream[i]>>>(devPtrOut[i], devPtrIn[i], size);
cudaMemcpyAsync(hostPtr[i], devPtrOut[i], size, cudaMemcpyDeviceToHost,
stream[i]);
cudaLaunchHostFunc(stream[i], MyCallback, (void*)i);
}
```

在主机函数之后在流中发出的命令不会在函数完成之前开始执行。

进入流的宿主函数不能(直接或间接)调用CUDA API，因为如果调用导致死锁，它可能会等待自己。

###### 3.2.6.5.7. Stream Priorities  

流的相对优先级可以在创建时使用cudaStreamCreateWithPriority()来指定。允许优先级的范围，排序为[最高优先级，最低优先级]可以通过cudaDeviceGetStreamPriorityRange()函数获得。在运行时，高优先级流中的挂起工作优先于低优先级流中的挂起工作。

下面的代码示例获得当前设备允许的优先级范围，并创建具有最高和最低可用优先级的流。

```c++
// get the range of stream priorities for this device
int priority_high, priority_low;
cudaDeviceGetStreamPriorityRange(&priority_low, &priority_high);
// create streams with highest and lowest available priorities
cudaStream_t st_high, st_low;
cudaStreamCreateWithPriority(&st_high, cudaStreamNonBlocking, priority_high);
cudaStreamCreateWithPriority(&st_low, cudaStreamNonBlocking, priority_low);
```

##### 3.2.6.6. CUDA Graphs  

CUDA Graphs提供了一种新的工作提交模型。图是由依赖关系连接的一系列操作，比如内核启动，依赖关系是与执行分开定义的。这允许一个图形被定义一次，然后重复启动。将图的定义从它的执行中分离出来可以实现许多优化:首先，与流相比，CPU启动成本降低了，因为很多设置都是提前完成的;第二，将整个工作流呈现给CUDA可以实现优化，这可能是不可能的流的分段工作提交机制。

为了看到图形可能的优化，考虑在流中发生的事情:当你将一个内核放入流中，主驱动程序执行一系列操作，为内核在GPU上的执行做准备。这些操作是设置和启动内核所必需的，是必须为每个发布的内核支付的开销。对于执行时间较短的GPU内核，这种开销成本可能是整个端到端执行时间的一个重要部分。

使用图表的工作提交分为三个不同的阶段:定义、实例化和执行。

- 在定义阶段，程序在图中创建操作的描述以及它们之间的依赖关系。
- 实例化获取图形模板的快照，验证它，并执行大量的设置和初始化工作，目的是将启动时需要做的事情最小化。产生的实例称为可执行图。
- 一个可执行图形可以被启动到一个流中，类似于任何其他CUDA工作。它可以在不重复实例化的情况下被启动任意次数。

###### 3.2.6.6.1. Graph Structure  

一个操作在图中形成一个节点。操作之间的依赖关系是边。这些依赖关系约束了操作的执行顺序。

一旦操作所依赖的节点完成，就可以随时调度操作。调度由CUDA系统决定。

#######  3.2.6.6.1.1. Node Types  

一个图形节点可以是:

- 内核
- CPU函数调用
- 内存复制
- memset
- 空节点
- 等待一个事件
- 记录一个事件
- 发出外部信号量的信号
- 等待外部信号量
- 子图:执行一个单独的嵌套图。参见图9。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南09.PNG)

###### 3.2.6.6.2. Creating a Graph Using Graph APIs  

图可以通过两种机制创建:显式API和流捕获。下面是创建和执行下图的示例。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南10.PNG)

```
// Create the graph - it starts out empty
cudaGraphCreate(&graph, 0);
// For the purpose of this example, we'll create
// the nodes separately from the dependencies to
// demonstrate that it can be done in two stages.
// Note that dependencies can also be specified
// at node creation.
cudaGraphAddKernelNode(&a, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&b, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&c, graph, NULL, 0, &nodeParams);
cudaGraphAddKernelNode(&d, graph, NULL, 0, &nodeParams);
// Now set up dependencies on each node
cudaGraphAddDependencies(graph, &a, &b, 1); // A->B
cudaGraphAddDependencies(graph, &a, &c, 1); // A->C
cudaGraphAddDependencies(graph, &b, &d, 1); // B->D
cudaGraphAddDependencies(graph, &c, &d, 1); // C->D
```

###### 3.2.6.6.3. Creating a Graph Using Stream Capture  

流捕获提供了一种从现有的基于流的api创建图表的机制。一段代码将工作启动到流中，包括现有的代码，可以与调用cudaStreamBeginCapture()和cudaStreamEndCapture()括起来。见下文。

```
cudaGraph_t graph;
cudaStreamBeginCapture(stream);
kernel_A<<< ..., stream >>>(...);
kernel_B<<< ..., stream >>>(...);
libraryCall(stream);
kernel_C<<< ..., stream >>>(...);
cudaStreamEndCapture(stream, &graph);
```

调用cudaStreamBeginCapture()将流置于捕获模式。当流被捕获时，流中启动的工作不会排队等待执行。相反，它被附加到一个逐步建立的内部图中。然后调用cudaStreamEndCapture()返回此图，它也结束了流的捕获模式。由流捕获积极构建的图称为捕获图。

流捕获可以用于任何CUDA流除了cudaStreamLegacy(“NULL流”)。注意，它可以在cudaStreamPerThread上使用。如果程序正在使用遗留流，则可以将流0重新定义为每线程流，而不进行任何函数更改。看到默认的流。

流是否被捕获可以通过cudaStreamIsCapturing()查询。

####### 3.2.6.6.3.1. Cross-stream Dependencies and Events  

流捕获可以处理用cudaEventRecord()和cudaStreamWaitEvent()表示的跨流依赖关系，前提是等待的事件被记录到同一个捕获图中。

当在处于捕获模式的流中记录事件时，将导致捕获事件。捕获的事件表示捕获图中的一组节点。

当一个被捕获的事件被流等待时，如果流还没有被捕获，它会将流置于捕获模式，并且流中的下一个项目将对被捕获事件中的节点有额外的依赖关系。然后将这两个流捕获到同一个捕获图中。

当跨流依赖存在于流捕获时，cudaStreamEndCapture()必须在调用cudaStreamBeginCapture()的相同流中被调用;这是原始的溪流。由于基于事件的依赖关系，被捕获到相同捕获图的任何其他流也必须连接回源流。如下所示。所有被捕获到相同捕获图的流都通过cudaStreamEndCapture()从捕获模式中取出。未能重新加入到源流将导致整个捕获操作失败。

```
// stream1 is the origin stream
cudaStreamBeginCapture(stream1);
kernel_A<<< ..., stream1 >>>(...);
// Fork into stream2
cudaEventRecord(event1, stream1);
cudaStreamWaitEvent(stream2, event1);
kernel_B<<< ..., stream1 >>>(...);
kernel_C<<< ..., stream2 >>>(...);
// Join stream2 back to origin stream (stream1)
cudaEventRecord(event2, stream2);
cudaStreamWaitEvent(stream1, event2);
kernel_D<<< ..., stream1 >>>(...);
// End capture in the origin stream
cudaStreamEndCapture(stream1, &graph);
// stream1 and stream2 no longer in capture mode
```

Graph returned by the above code is shown in Figure 10.  

*注意:当流从捕获模式中取出时，流中的下一个非捕获项(如果有的话)将仍然依赖于最近的先前的非捕获项，尽管中间项已经被删除。*

####### 3.2.6.6.3.2。禁止和未处理的操作

同步或查询正在被捕获的流或已捕获事件的执行状态是无效的，因为它们不表示计划执行的项。当任何关联流处于捕获模式时，查询或同步包含活动流捕获的更广泛句柄(如设备句柄或上下文句柄)的执行状态也是无效的。

当同一上下文中的任何流被捕获时，并且它不是用cudaStreamNonBlocking创建的，任何试图使用遗留流的行为都是无效的。这是因为遗留流句柄在任何时候都包含这些其他流;进入遗留流的队列将创建对被捕获流的依赖，而查询它或同步它将查询或同步被捕获的流。

因此，在这种情况下调用同步api也是无效的。同步api，如cudaMemcpy()，将工作排队到遗留流，并在返回之前同步它。

*注意:一般来说，当一个依赖关系将被捕获的东西与未被捕获的东西连接起来，而不是排队等待执行时，CUDA宁愿返回一个错误而不是忽略这个依赖。将流放入或退出捕获模式时会出现异常;这切断了在模式转换之前和之后立即添加到流中的项之间的依赖关系。*

通过等待正在被捕获的流中的捕获事件来合并两个单独的捕获图是无效的，该流与与该事件相关联的不同捕获图是相关联的。等待正在被捕获的流中未捕获的事件是无效的。

一小部分api将异步操作排队到流中，目前在图形中不支持，如果被捕获的流调用将返回一个错误，例如cudaStreamAttachMemAsync()。

####### 3.2.6.6.3.3 失效

当在流捕获期间尝试无效操作时，任何关联的捕获图都将无效。当捕获图无效时，进一步使用任何正在被捕获的流或与图相关的捕获事件是无效的，并将返回一个错误，直到使用cudaStreamEndCapture()结束流捕获。这个调用将使相关的流脱离捕获模式，但也将返回一个错误值和一个NULL图。

###### 3.2.6.6.4 更新实例化图

使用图表的工作提交分为三个不同的阶段:定义、实例化和执行。在工作流没有变化的情况下，定义和实例化的开销可以在多次执行中分摊，并且图提供了比流明显的优势。

图是工作流的快照，包括内核、参数和依赖项，以便尽可能快速和有效地重放它。在工作流发生变化的情况下，图表就会过时，必须进行修改。对图结构(如拓扑或节点类型)的主要更改将需要重新实例化源图，因为必须重新应用各种与拓扑相关的优化技术。

重复实例化的代价可能会降低图形执行的整体性能收益，但通常只有节点参数(如内核参数和cudaMemcpy地址)发生变化，而图形拓扑保持不变。在这种情况下，CUDA提供了一种称为“图更新”的轻量级机制，它允许在不重建整个图的情况下对某些节点参数进行修改。这比重新实例化高效得多。

更新将在下次图形启动时生效，因此它们不会影响以前的图形启动，即使它们在更新时正在运行。一个图可以重复更新和重新启动，因此多个更新/启动可以在一个流上排队。

CUDA提供了两种更新实例化图的机制，即整个图的更新和单个节点的更新。整个图形更新允许用户提供一个拓扑相同的cudaGraph_t对象，其节点包含更新的参数。单个节点更新允许用户显式地更新单个节点的参数。使用更新的cudaGraph_t在大量节点被更新时更方便，或者当图形拓扑对调用者是未知的(即，图形来自库调用的流捕获)。当更改的数量很小且用户拥有需要更新的节点句柄时，首选使用单个节点更新。单个节点更新会跳过未更改节点的拓扑检查和比较，因此在许多情况下会更高效。下面的部分将更详细地解释每种方法。

####### 3.2.6.6.4.1 图更新限制

内核节点:

- 该函数的所属上下文不能更改。
- 一个函数最初没有使用CUDA动态并行度的节点，不能够更新为使用CUDA动态并行度的函数

cudaMemset和cudaMemcpy节点:

- 所有操作数分配/映射到的CUDA设备不能更改。
- 源/目标内存必须从与原始源/目标内存相同的上下文中分配。
- 只有1D cudaMemset/cudaMemcpy节点可以被修改。

memcpy节点的其他限制:

- 所有改变源内存类型或目标内存类型(即cudaPitchedPtr, cudaArray_t等)，或传输类型(即cudaMemcpyKind)都不支持。

外部信号量等待节点和记录节点:

- 不支持修改信号量的数量。

对主机节点、事件记录节点或事件等待节点的更新没有限制。

####### 3.2.6.6.4.2 整个图更新

cudaGraphExecUpdate()允许实例化的图形(“原始图形”)使用拓扑相同的图形(“更新”图形)的参数进行更新。更新图的拓扑必须与用于实例化cudaGraphExec_t的原始图相同。此外，向原始图中添加或从原始图中删除节点的顺序必须与向更新图中添加(或从更新图中删除)节点的顺序相匹配。因此，当使用流捕获时，必须以相同的顺序捕获节点，当使用显式图形节点创建api时，必须以相同的顺序添加和/或删除所有节点。

下面的例子展示了API如何被用来更新一个实例化的图形:

```c++
cudaGraphExec_t graphExec = NULL;
for (int i = 0; i < 10; i++) {
cudaGraph_t graph;
cudaGraphExecUpdateResult updateResult;
cudaGraphNode_t errorNode;
// In this example we use stream capture to create the graph.
// You can also use the Graph API to produce a graph.
cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);
// Call a user-defined, stream based workload, for example
do_cuda_work(stream);
cudaStreamEndCapture(stream, &graph);
// If we've already instantiated the graph, try to update it directly
// and avoid the instantiation overhead
if (graphExec != NULL) {
// If the graph fails to update, errorNode will be set to the
// node causing the failure and updateResult will be set to a
// reason code.
cudaGraphExecUpdate(graphExec, graph, &errorNode, &updateResult);
}
// Instantiate during the first iteration or whenever the update
// fails for any reason
if (graphExec == NULL || updateResult != cudaGraphExecUpdateSuccess) {
// If a previous update failed, destroy the cudaGraphExec_t
// before re-instantiating it
if (graphExec != NULL) {
cudaGraphExecDestroy(graphExec);
}
// Instantiate graphExec from graph. The error node and
// error message parameters are unused here.
cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0);
}
cudaGraphDestroy(graph);
cudaGraphLaunch(graphExec, stream);
cudaStreamSynchronize(stream);
}
```

一个典型的工作流是使用流捕获或图形API创建初始的cudaGraph_t。cudaGraph_t然后被实例化并正常启动。初始启动后，使用与初始图形相同的方法创建一个新的cudaGraph_t，并调用cudaGraphExecUpdate()。如果图形更新成功(由上面示例中的updateResult参数表示)，则启动更新后的cudaGraphExec_t。如果更新由于任何原因失败，cudaGraphExecDestroy()和cudaGraphInstantiate()被调用来销毁原来的cudaGraphExec_t并实例化一个新的。

也可以直接更新cudaGraph_t节点(例如，使用cudaGraphKernelNodeSetParams())，然后更新cudaGraphExec_t，但是使用下一节介绍的显式节点更新api会更有效。

关于使用和当前限制的更多信息，请参阅Graph API。

####### 3.2.6.6.4.3 单个节点更新

实例化的图形节点参数可以直接更新。这消除了实例化的开销以及创建一个新的cudaGraph_t的开销。如果需要更新的节点数量相对于图中节点的总数量较少，那么最好单独更新节点。以下方法可用于更新cudaGraphExec_t节点:

```c++
‣ cudaGraphExecKernelNodeSetParams()
‣ cudaGraphExecMemcpyNodeSetParams()
‣ cudaGraphExecMemsetNodeSetParams()
‣ cudaGraphExecHostNodeSetParams()
‣ cudaGraphExecChildGraphNodeSetParams()
‣ cudaGraphExecEventRecordNodeSetEvent()
‣ cudaGraphExecEventWaitNodeSetEvent()
‣ cudaGraphExecExternalSemaphoresSignalNodeSetParams(
‣ cudaGraphExecExternalSemaphoresWaitNodeSetParams()
```

关于使用和当前限制的更多信息，请参阅Graph API。

###### 3.2.6.6.5 使用graph api

cudaGraph_t对象不是线程安全的。确保多个线程不同时访问同一个cudaGraph_t是用户的责任。

cudaGraphExec_t不能与自身同时运行。cudaGraphExec_t的启动将在同一可执行图之前的启动之后进行。

图的执行是在流中与其他异步工作进行排序。然而，流仅用于订购;它不限制图的内部并行性，也不影响图节点执行的位置。

参见Graph API



##### 3.2.6.7 事件

运行时还提供了一种方法来密切监控设备的进程，以及执行准确的定时，通过让应用程序异步记录程序中的任何点的事件，并查询这些事件何时完成。当事件之前的所有任务(或指定流中的所有命令)都完成时，事件就完成了。在所有流中的所有任务和命令完成后，流0中的事件将被完成。

###### 3.2.6.7.1 构造和析构

下面的代码示例创建了两个事件:

```
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);
```

它们是这样被析构的:

```
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

###### 3.2.6.7.2 运行时间

在创建和销毁中创建的事件可以用以下方式为创建和销毁的代码样本计时:

```c++
cudaEventRecord(start, 0);
for (int i = 0; i < 2; ++i) {
cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,
size, cudaMemcpyHostToDevice, stream[i]);
MyKernel<<<100, 512, 0, stream[i]>>>
(outputDev + i * size, inputDev + i * size, size);
cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,
size, cudaMemcpyDeviceToHost, stream[i]);
}
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float elapsedTime;
cudaEventElapsedTime(&elapsedTime, start, stop);
```

##### 3.2.6.8 同步调用

当调用同步函数时，在设备完成所请求的任务之前，控制不会返回给宿主线程。在宿主线程执行任何其他CUDA调用之前，可以通过调用带有特定标志的cudaSetDeviceFlags()来指定宿主线程是yield, block，还是spin。

#### 3.2.7 多设备系统

##### 3.2.7.1 设备的枚举

一个主机系统可以有多个设备。下面的代码示例展示了如何枚举这些设备、查询它们的属性以及确定支持cuda的设备的数量。

```
int deviceCount;
cudaGetDeviceCount(&deviceCount);
int device;
for (device = 0; device < deviceCount; ++device) {
cudaDeviceProp deviceProp;
cudaGetDeviceProperties(&deviceProp, device);
printf("Device %d has compute capability %d.%d.\n",
device, deviceProp.major, deviceProp.minor);
}
```

##### 3.2.7.2 设备的选择

宿主线程可以通过调用cudaSetDevice()来设置它在任何时候操作的设备。设备内存分配和内核启动是在当前设置的设备上进行的;流和事件是与当前设置的设备关联创建的。如果没有调用cudaSetDevice()，则当前设备为设备0。

下面的代码示例说明了设置当前设备如何影响内存分配和内核执行。

```
size_t size = 1024 * sizeof(float);
cudaSetDevice(0); // Set device 0 as current
float* p0;
cudaMalloc(&p0, size); // Allocate memory on device 0
MyKernel<<<1000, 128>>>(p0); // Launch kernel on device 0
cudaSetDevice(1); // Set device 1 as current
float* p1;
cudaMalloc(&p1, size); // Allocate memory on device 1
MyKernel<<<1000, 128>>>(p1); // Launch kernel on device 1
```

##### 3.2.7.3 流和事件行为

如果内核启动被发送到一个与当前设备没有关联的流，则内核启动将失败，如下面的代码示例所示。

```c++
cudaSetDevice(0); // Set device 0 as current
cudaStream_t s0;
cudaStreamCreate(&s0); // Create stream s0 on device 0
MyKernel<<<100, 64, 0, s0>>>(); // Launch kernel on device 0 in s0
cudaSetDevice(1); // Set device 1 as current
cudaStream_t s1;
cudaStreamCreate(&s1); // Create stream s1 on device 1
MyKernel<<<100, 64, 0, s1>>>(); // Launch kernel on device 1 in s1
// This kernel launch will fail:
MyKernel<<<100, 64, 0, s0>>>(); // Launch kernel on device 1 in s0
```

即使将内存副本发出给与当前设备没有关联的流，它也将成功。

如果输入事件和输入流关联到不同的设备，cudaEventRecord()将失败。

如果两个输入事件关联到不同的设备，cudaEventElapsedTime()将失败

即使输入事件与当前设备不同，cudaEventSynchronize()和cudaEventQuery()也会成功。

即使输入流和输入事件关联到不同的设备，cudaStreamWaitEvent()也会成功。因此，可以使用cudaStreamWaitEvent()来同步多个设备。

每个设备都有自己的默认流(参见默认流)，因此发送到设备的默认流的命令可能会无序地执行，或者与发送到任何其他设备的默认流的命令并发执行。

##### 3.2.7.4 点对点的内存访问

根据系统属性，特别是PCIe和/或NVLINK拓扑，设备能够寻址彼此的内存(例如，在一个设备上执行的内核可以解引用指向另一个设备内存的指针)。如果cudaDeviceCanAccessPeer()对这两个设备返回true，则支持这两个设备之间的点对点内存访问特性。

点对点内存访问仅支持64位应用程序，必须通过调用cudaDeviceEnablePeerAccess()在两个设备之间启用，如下面的代码示例所示。在非nvswitch开启的系统中，每个设备最多支持8个对端连接。

统一地址空间用于两个设备(请参见统一虚拟地址空间)，因此可以使用相同的指针从两个设备寻址内存，如下面的代码示例所示。

```cuda
cudaSetDevice(0); // Set device 0 as current
float* p0;
size_t size = 1024 * sizeof(float);
cudaMalloc(&p0, size); // Allocate memory on device 0
MyKernel<<<1000, 128>>>(p0); // Launch kernel on device 0
cudaSetDevice(1); // Set device 1 as current
cudaDeviceEnablePeerAccess(0, 0); // Enable peer-to-peer access
// with device 0
// Launch kernel on device 1
// This kernel launch can access memory on device 0 at address p0
MyKernel<<<1000, 128>>>(p0);
```

###### 3.2.7.4.1 IOMMU on Linux

仅在Linux上，CUDA和显示驱动程序不支持iommu启用的裸金属PCIe对等内存拷贝。然而，CUDA和显示驱动程序通过虚拟机支持IOMMU。因此，Linux上的用户在本地裸金属系统上运行时，应该禁用IOMMU。IOMMU应该被启用，VFIO驱动程序应该被用作虚拟机的PCIe直通。

在Windows上不存在上述限制。

请参见在64位平台上分配DMA缓冲区。

##### 3.2.7.5 点对点内存复制

存储器拷贝可以在两个不同设备的存储器之间执行。

当为两个设备使用统一地址空间(请参见统一虚拟地址空间)时，使用设备内存中提到的常规内存复制功能来实现。

否则，这是使用cudaMemcpyPeer()， cudaMemcpyPeerAsync()， cudaMemcpy3DPeer()，或cudaMemcpy3DPeerAsync()，如下面的代码示例所示。

```
cudaSetDevice(0); // Set device 0 as current
float* p0;
size_t size = 1024 * sizeof(float);
cudaMalloc(&p0, size); // Allocate memory on device 0
cudaSetDevice(1); // Set device 1 as current
float* p1;
cudaMalloc(&p1, size); // Allocate memory on device 1
cudaSetDevice(0); // Set device 0 as current
MyKernel<<<1000, 128>>>(p0); // Launch kernel on device 0
cudaSetDevice(1); // Set device 1 as current
cudaMemcpyPeer(p1, 1, p0, 0, size); // Copy p0 to p1
MyKernel<<<1000, 128>>>(p1); // Launch kernel on device 1
```

两个不同设备内存之间的拷贝(在隐式NULL流中):

- 直到之前发出给任何一个设备的所有命令都完成并开始运行
- 直到所有命令(参见异步并发执行)在拷贝到任意一台设备可以启动之后发出为止。

与流的正常行为一致，两个设备内存之间的异步复制可能与另一个流中的副本或内核重叠

注意，如果在两个设备之间通过cudaDeviceEnablePeerAccess()启用点对点访问，如在点对点内存访问中描述的，这两个设备之间的点对点内存复制不再需要通过主机分段，因此更快。

#### 3.2.8 统一虚拟地址空间

当应用程序作为64位进程运行时，主机和所有具有2.0或更高计算能力的设备将使用单个地址空间。所有通过CUDA API调用进行的主机内存分配和所有支持设备上的设备内存分配都在这个虚拟地址范围内。结果:

- 通过CUDA分配给主机上的任何内存位置，或者使用统一地址空间的任何设备上的任何内存位置，都可以使用cudaPointerGetAttributes()从指针的值确定。
- 当拷贝到或从任何使用统一地址空间的设备的内存时，cudaMemcpy*()的cudaMemcpyKind参数可以设置为cudaMemcpyDefault来确定指针的位置。只要当前设备使用统一寻址，这也适用于未通过CUDA分配的主机指针。
- 通过cudaHostAlloc()分配的内存是自动可移植的(参见可移植内存)
- 在使用统一地址空间的所有设备上，cudaHostAlloc()返回的指针可以直接从运行在这些设备上的内核中使用(即，没有必要通过cudaHostGetDevicePointer()获取设备指针，如映射内存中所述。)

应用程序可以通过检查unifiedAddressing设备属性(参见设备枚举)是否等于1来查询统一地址空间是否用于特定的设备。

#### 3.2.9 进程间通信

主机线程创建的任何设备内存指针或事件句柄都可以被同一进程内的任何其他线程直接引用。然而，它在这个进程之外是无效的，因此不能被属于不同进程的线程直接引用。

为了跨进程共享设备内存指针和事件，应用程序必须使用进程间通信API，该API在参考手册中有详细描述。IPC API仅支持Linux上的64位进程和计算能力2.0及更高的设备。注意，cudaMallocManaged分配不支持IPC API。

使用该API,应用程序可以获得给定设备的IPC处理内存指针使用cudaIpcGetMemHandle(),将其传递给另一个进程使用标准的IPC机制(例如,进程间共享内存或文件),并使用cudaIpcOpenMemHandle()来检索一个设备指针的IPC处理这个过程中是一个有效的指针。可以使用类似的入口点共享事件句柄。

注意，由于性能原因，cudaMalloc()所做的分配可以从更大的内存块中进行子分配。在这种情况下，CUDA IPC api将共享整个底层内存块，这可能会导致其他子分配被共享，这可能会导致进程之间的信息泄露。为了防止这种行为，建议只共享2MiB对齐大小的分配。

使用IPC API的一个例子是，单个主进程生成一批输入数据，使这些数据对多个辅助进程可用，而不需要重新生成或复制。

使用CUDA IPC进行通信的应用程序应该使用相同的CUDA驱动程序和运行时进行编译、链接和运行。

*注意:在Tegra设备上不支持CUDA IPC呼叫。*

#### 3.2.10 错误检查

所有运行时函数都会返回错误代码，但对于异步函数(参见异步并发执行)，这个错误代码不可能报告任何可能发生在设备上的异步错误，因为函数在设备完成任务之前返回;错误代码只报告在执行任务之前发生在主机上的错误，通常与参数验证有关;如果发生异步错误，将由随后的一些不相关的运行时函数调用报告。

检查异步错误的唯一方法后一些异步函数调用是同步调用通过调用后cudaDeviceSynchronize()(或使用任何其他同步机制中描述异步并发执行)和检查返回的错误代码cudaDeviceSynchronize()。

运行时为每个初始化为cudassuccess的宿主线程维护一个错误变量，每次出现错误(无论是参数验证错误还是异步错误)时，该变量都会被错误代码覆盖。cudaPeekAtLastError()返回这个变量。cudaGetLastError()返回这个变量并将其重置为cudassuccess。

内核启动不会返回任何错误代码，所以必须在内核启动之后调用cudaPeekAtLastError()或cudaGetLastError()来检索任何预启动错误。确保任何错误返回cudaPeekAtLastError()或cudaGetLastError()并不源自调用内核启动前,必须确保运行时错误变量设置为cudaSuccess内核启动之前,例如,通过调用cudaGetLastError()就在内核启动。内核启动是异步的，所以为了检查异步错误，应用程序必须在内核启动和调用cudaPeekAtLastError()或cudaGetLastError()之间进行同步。

```c++
注意，cudaErrorNotReady可能会被cudaStreamQuery()和cudaEventQuery()返回，它不会被认为是一个错误，因此cudaPeekAtLastError()或cudaGetLastError()不会报告。
```

#### 3.2.11 调用堆栈

对于计算能力为2的设备。x及以上，调用堆栈的大小可以使用cudaDeviceGetLimit()查询，并使用cudaDeviceSetLimit()设置。

当调用堆栈溢出时，如果应用程序是通过CUDA调试器(CUDA -gdb, Nsight)运行或一个未指定的启动错误，则内核调用失败并产生堆栈溢出错误，否则。

#### 3.2.12 纹理和表面记忆

CUDA支持GPU用于图形访问纹理和表面内存的纹理硬件子集。如设备内存访问所述，从纹理或表面内存而不是全局内存读取数据有几个性能优势。

有两种不同的api来访问纹理和表面内存:

- 所有设备都支持的纹理引用API
- 纹理对象API仅在具有计算能力3的设备上支持。x和更高的

纹理引用API有纹理对象API没有的限制。它们在纹理参考API中被提到。

#### 3.2.12.1 纹理内存

纹理内存是使用纹理函数中描述的设备函数从内核中读取的。读取纹理并调用其中一个函数的过程称为纹理获取。每个纹理获取都为纹理对象API指定一个名为纹理对象的参数，或者为纹理引用API指定一个纹理引用。

纹理对象或纹理引用指定:

- 纹理，它是被获取的纹理内存块。纹理对象是在运行时创建的，纹理是在创建纹理对象时指定的，如纹理对象API所述。纹理引用是在编译时创建的，纹理是在运行时通过运行时函数将纹理引用绑定到纹理来指定的，如Texture reference API所述;几个不同的纹理引用可能绑定到相同的纹理或内存中重叠的纹理。纹理可以是线性存储器的任何区域或CUDA数组(在CUDA数组中描述)。
- 它的维度指定纹理是使用一个纹理坐标的一维数组、使用两个纹理坐标的二维数组还是使用三个纹理坐标的三维数组。数组的元素称为texel，纹理元素的简称。纹理的宽度、高度和深度指的是每个维度中数组的大小。根据设备的计算能力不同，纹理的最大宽度、高度和深度如表15所示。
- texel的类型，它被限制为基本整数和单精度浮点类型，以及从基本整数和单精度浮点类型派生而来的内置向量类型中定义的任何1-、2-和4-组件向量类型。
- 读取模式，它等于cudareadmodennormalizedfloat或cudaReadModeElementType。如果是cudaReadModeNormalizedFloat和特塞尔绵羊的类型是一个16位或8位整数类型,返回的值纹理获取实际上是作为浮点类型和返回整数类型映射到的全部为无符号整数类型[0.0,1.0]和[-1.0,1.0]符号整型;例如，一个值为0xff的无符号8位纹理元素读取为1。如果它是cudaReadModeElementType，则不执行转换。
- 纹理坐标是否规范化。默认情况下，纹理被引用(由Texture functions的函数)使用[0,N-1]范围内的浮点坐标，其中N是坐标对应维度内的纹理大小。例如，一个大小为64x32的纹理将被x和y维度的坐标分别引用为[0,63]和[0,31]。归一化纹理坐标使得坐标被指定在[0.0,1.0-1/N]范围内，而不是[0,N-1]，所以同样的64x32纹理将被归一化坐标在[0,1-1/N]范围内处理。在x和y维度上。如果纹理坐标独立于纹理大小更可取，规范化纹理坐标自然适合一些应用程序的要求。
- 寻址模式。使用超出范围的坐标来调用Section B.8的设备函数是有效的。寻址模式定义了在这种情况下会发生什么。默认的寻址模式是将坐标夹在有效范围内:非标准化坐标为[0,N)，标准化坐标为[0.0,1.0)。如果指定了边界模式，那么使用超出范围的纹理坐标获取纹理将返回零。对于规范化坐标，wrap模式和mirror模式也是可用的。当使用wrap模式时，每个坐标x被转换为frac(x)=x floor(x)，其中floor(x)是最大的当使用镜像模式时，如果floor(x)是偶数，则每个坐标x转换为frac(x)，如果floor(x)是奇数，则1-frac(x)。寻址模式被指定为一个大小为3的数组，其第一个、第二个和第三个元素分别指定了第一个、第二个和第三个纹理坐标的寻址模式;寻址模式为cudaAddressModeBorder、cudaAddressModeClamp、cudaAddressModeWrap cudaAddressModeMirror;cudaAddressModeWrap和cudaAddressModeMirror只支持规范化的纹理坐标
- 过滤模式指定在获取纹理时如何根据输入纹理坐标计算返回的值。线性纹理过滤只能对配置为返回浮点数据的纹理进行。它执行相邻texels之间的低精度插值。当启用时，围绕纹理获取位置的texels被读取，纹理获取的返回值根据纹理坐标在texels之间的位置被插值。一维纹理采用简单线性插值，二维纹理采用双线性插值，三维纹理采用三线性插值。纹理抓取提供了纹理抓取的更多细节。过滤模式等于cudaFilterModePoint或cudaFilterModeLinear。如果是cudaFilterModePoint，则返回值为纹理坐标最接近输入纹理坐标的Texel。如果它是cudaFilterModeLinear，返回值是两个(一维纹理)、四个(二维纹理)或八个(三维纹理)纹理的线性插值，它们的纹理坐标最接近输入纹理坐标。cudaFilterModeLinear只对浮点类型的返回值有效。


纹理对象API引入了纹理对象API。

Texture Reference API引入了纹理参考API。

16位浮点纹理解释了如何处理16位浮点纹理。

纹理也可以像分层纹理中描述的那样分层。

Cubemap纹理和Cubemap分层纹理描述了一种特殊的纹理类型，即Cubemap纹理。

Texture Gather描述了一个特殊的纹理获取，纹理收集。

###### 3.2.12.1.1 纹理对象API

纹理对象是使用cudaCreateTextureObject()从struct cudaResourceDesc类型的资源描述中创建的，该资源描述指定了纹理，纹理描述定义如下:

```c++
struct cudaTextureDesc
{
enum cudaTextureAddressMode addressMode[3];
enum cudaTextureFilterMode filterMode;
enum cudaTextureReadMode readMode;
int sRGB;
int normalizedCoords;
unsigned int maxAnisotropy;
enum cudaTextureFilterMode mipmapFilterMode;
float mipmapLevelBias;
float minMipmapLevelClamp;
float maxMipmapLevelClamp;
};
```

- addressMode指定寻址模式;
- filterMode指定过滤模式;
- readMode表示读取模式;
- normalizedcoors指定纹理坐标是否被规格化;
- 参见sRGB、maxAnisotropy、mipmapFilterMode、mipmapLevelBias、minMipmapLevelClamp和maxMipmapLevelClamp的参考手册。

下面的代码示例将一些简单的转换内核应用于纹理。

```c++
// Simple transformation kernel
__global__ void transformKernel(float* output,
cudaTextureObject_t texObj,
int width, int height,
float theta)
{
// Calculate normalized texture coordinates
unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
float u = x / (float)width;
float v = y / (float)height;
// Transform coordinates
u -= 0.5f;
v -= 0.5f;
float tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
float tv = v * cosf(theta) + u * sinf(theta) + 0.5f;
// Read from texture and write to global memory
output[y * width + x] = tex2D<float>(texObj, tu, tv);
}
// Host code
int main()
{
// Allocate CUDA array in device memory
cudaChannelFormatDesc channelDesc =
cudaCreateChannelDesc(32, 0, 0, 0,
cudaChannelFormatKindFloat);
cudaArray* cuArray;
cudaMallocArray(&cuArray, &channelDesc, width, height);
// Copy to device memory some data located at address h_data
// in host memory
cudaMemcpyToArray(cuArray, 0, 0, h_data, size,
cudaMemcpyHostToDevice);
// Specify texture
struct cudaResourceDesc resDesc;
memset(&resDesc, 0, sizeof(resDesc));
resDesc.resType = cudaResourceTypeArray;
resDesc.res.array.array = cuArray;
// Specify texture object parameters
struct cudaTextureDesc texDesc;
memset(&texDesc, 0, sizeof(texDesc));
texDesc.addressMode[0] = cudaAddressModeWrap;
texDesc.addressMode[1] = cudaAddressModeWrap;
texDesc.filterMode = cudaFilterModeLinear;
texDesc.readMode = cudaReadModeElementType;
texDesc.normalizedCoords = 1;
// Create texture object
cudaTextureObject_t texObj = 0;
cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);
// Allocate result of transformation in device memory
float* output;
cudaMalloc(&output, width * height * sizeof(float));
// Invoke kernel
dim3 dimBlock(16, 16);
dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x,
(height + dimBlock.y - 1) / dimBlock.y);
transformKernel<<<dimGrid, dimBlock>>>(output,
texObj, width, height,
angle);
// Destroy texture object
cudaDestroyTextureObject(texObj);
// Free device memory
cudaFreeArray(cuArray);
cudaFree(output);
return 0;
}
```

###### 3.2.12.1.2 纹理参考API

纹理引用的一些属性是不可变的，必须在编译时知道;它们在声明纹理引用时被指定。纹理引用在文件作用域被声明为一个纹理类型的变量:

```c++
texture<DataType, Type, ReadMode> texRef;
```

地点:

- DataType表示texel的类型;
- 类型指定纹理引用的类型，它等于cudaTextureType1D， cudaTextureType2D，或cudaTextureType3D，用于一维，二维， 或三维纹理，或cudaTextureType1DLayered或 cudaTextureType2DLayered用于一维或二维分层纹理分别;Type是一个可选参数，默认为cudaTextureType1D;
- ReadMode表示读取模式;它是一个可选参数，默认为 cudaReadModeElementType。

纹理引用只能声明为静态全局变量，不能作为参数传递给函数。

纹理引用的其他属性是可变的，可以在运行时通过宿主运行时更改。如参考手册中所述，运行时API具有低级C风格接口和高级c++风格接口。在高级API中，纹理类型被定义为一个公开派生自低级API中定义的textureReference类型的结构，如下所示:

```
struct textureReference {
int normalized;
enum cudaTextureFilterMode filterMode;
enum cudaTextureAddressMode addressMode[3];
struct cudaChannelFormatDesc channelDesc;
int sRGB;
unsigned int maxAnisotropy;
enum cudaTextureFilterMode mipmapFilterMode;
float mipmapLevelBias;
float minMipmapLevelClamp;
float maxMipmapLevelClamp;
}
```

- normalized表示纹理坐标是否被规格化;

- filterMode表示过滤模式;

- addressMode表示寻址模式;

- channelDesc描述texel的格式;它必须匹配纹理引用声明的DataType参数;channelDesc的类型如下:

- ```
  struct cudaChannelFormatDesc {
  int x, y, z, w;
  enum cudaChannelFormatKind f;
  };
  ```

- where x, y, z, and w are equal to the number of bits of each component of the returned
  value and f is:  

  - cudaChannelFormatKindSigned如果这些组件是带符号整数类型，
  - cudaChannelFormatKindUnsigned如果它们是无符号整数类型
  - 如果它们是浮点类型的cudaChannelFormatKindFloat。

- 参见sRGB、maxAnisotropy、mipmfiltermode、mipmapLevelBias、minMipmapLevelClamp和maxMipmapLevelClamp的参考手册。

normalize, addressMode和filterMode可以在主机代码中直接修改。

在内核可以使用纹理引用从纹理内存中读取之前，纹理引用必须通过cudaBindTexture()或cudaBindTexture2D()(线性内存)或cudaBindTextureToArray() (CUDA数组)绑定到纹理。cudaUnbindTexture()用于解除一个纹理引用的绑定。一旦纹理引用被解除绑定，它就可以安全地返回到另一个数组，即使使用之前绑定的纹理的内核还没有完成。推荐使用cudaMallocPitch()在线性内存中分配二维纹理，并使用cudaMallocPitch()返回的pitch作为cudaBindTexture2D()的输入参数。

以下代码示例将2D纹理引用绑定到devPtr所指向的线性内存:

- Using the low-level API:  

- ```
  texture<float, cudaTextureType2D,
  cudaReadModeElementType> texRef;
  textureReference* texRefPtr;
  cudaGetTextureReference(&texRefPtr, &texRef);
  cudaChannelFormatDesc channelDesc =
  cudaCreateChannelDesc<float>();
  size_t offset;
  cudaBindTexture2D(&offset, texRefPtr, devPtr, &channelDesc,
  width, height, pitch);
  ```

- Using the high-level API:  

- ```
  texture<float, cudaTextureType2D,
  cudaReadModeElementType> texRef;
  cudaChannelFormatDesc channelDesc =
  cudaCreateChannelDesc<float>();
  size_t offset;
  cudaBindTexture2D(&offset, texRef, devPtr, channelDesc,
  width, height, pitch);
  ```

  以下代码示例将2D纹理引用绑定到CUDA数组cuArray:

- Using the low-level API:  

- ```
  texture<float, cudaTextureType2D,
  cudaReadModeElementType> texRef;
  textureReference* texRefPtr;
  cudaGetTextureReference(&texRefPtr, &texRef);
  cudaChannelFormatDesc channelDesc;
  cudaGetChannelDesc(&channelDesc, cuArray);
  cudaBindTextureToArray(texRef, cuArray, &channelDesc);
  ```

- Using the high-level API:  

- ```
  texture<float, cudaTextureType2D,
  cudaReadModeElementType> texRef;
  cudaBindTextureToArray(texRef, cuArray);
  ```

将纹理绑定到纹理引用时指定的格式必须与声明纹理引用时指定的参数匹配;否则，纹理获取的结果是未定义的。

可以绑定到内核的纹理数量有一个限制，如表15所示。

下面的代码示例将一些简单的转换内核应用于纹理。

```
// 2D float texture
texture<float, cudaTextureType2D, cudaReadModeElementType> texRef;
// Simple transformation kernel
__global__ void transformKernel(float* output,
int width, int height,
float theta)
{
// Calculate normalized texture coordinates
unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;
unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;
float u = x / (float)width;
float v = y / (float)height;
// Transform coordinates
u -= 0.5f;
v -= 0.5f;
float tu = u * cosf(theta) - v * sinf(theta) + 0.5f;
float tv = v * cosf(theta) + u * sinf(theta) + 0.5f;
// Read from texture and write to global memory
output[y * width + x] = tex2D(texRef, tu, tv);
}
// Host code
int main()
{
// Allocate CUDA array in device memory
cudaChannelFormatDesc channelDesc =
cudaCreateChannelDesc(32, 0, 0, 0,
cudaChannelFormatKindFloat);
cudaArray* cuArray;
cudaMallocArray(&cuArray, &channelDesc, width, height);
// Copy to device memory some data located at address h_data
// in host memory
cudaMemcpyToArray(cuArray, 0, 0, h_data, size,
cudaMemcpyHostToDevice);
// Set texture reference parameters
texRef.addressMode[0] = cudaAddressModeWrap;
texRef.addressMode[1] = cudaAddressModeWrap;
texRef.filterMode = cudaFilterModeLinear;
texRef.normalized = true;
// Bind the array to the texture reference
cudaBindTextureToArray(texRef, cuArray, channelDesc);
// Allocate result of transformation in device memory
float* output;
cudaMalloc(&output, width * height * sizeof(float));
// Invoke kernel
dim3 dimBlock(16, 16);
dim3 dimGrid((width + dimBlock.x - 1) / dimBlock.x,
(height + dimBlock.y - 1) / dimBlock.y);
transformKernel<<<dimGrid, dimBlock>>>(output, width, height,
angle);
// Free device memory
cudaFreeArray(cuArray);
cudaFree(output);
return 0;
}
```

###### 3.2.12.1.3 16位浮点纹理

CUDA阵列所支持的16位浮点或半格式与IEEE 754-2008 binary2格式相同。

CUDA c++不支持匹配的数据类型，但是提供了一些内在的函数来通过unsigned short类型转换32位浮点格式:__ _float2half_rn(float)___ 和__half2float(unsigned short)。这些函数只在设备代码中支持。例如，可以在OpenEXR库中找到与宿主代码等价的函数。

在执行任何过滤之前，在纹理获取过程中，16位浮点组件被提升为32位浮点。

16位浮点格式的通道描述可以通过调用一个cudaCreateChannelDescHalf*()函数来创建。

###### 3.2.12.1.4 分层的纹理

一维或二维分层纹理(在Direct3D中称为纹理数组，在OpenGL中称为纹理数组)是由一系列层组成的纹理，这些层都是相同维度、大小和数据类型的规则纹理。

一维分层纹理使用整数索引和浮点纹理坐标进行处理;索引表示序列中的一个层，而坐标则指向该层中的一个texel。使用一个整数索引和两个浮点纹理坐标来处理二维分层纹理;索引表示序列中的一个层，而坐标则指向该层中的一个texel。

一个分层纹理只能是一个CUDA数组通过调用cudaMalloc3DArray()与cudaArrayLayered标志(和高度为零的一维分层纹理)。

使用tex1DLayered()、tex1DLayered()、tex2DLayered()和tex2DLayered()中描述的设备函数来获取分层纹理。纹理过滤(见纹理抓取)只在一个层内完成，而不是跨层。

层状纹理只支持计算能力为2.0或更高的设备。

###### 3.2.12.1.5 Cubemap纹理

cubemap纹理是一种特殊的二维分层纹理，它有6个层来表示一个立方体的面:

- 一个层的宽度等于它的高度。
- 立方体地图使用三个纹理坐标x、y和z来处理，它们被解释为一个方向矢量，从立方体的中心发出，指向立方体的一个面和对应该面层中的一个texel。更具体地说，面由最大m的坐标选择，对应层使用坐标(s/m+1)/2和(t/m+1)/2处理，其中s和t定义在表2中。

![](D:\github\wuyuchun.github.io\images\posts\2021-05-06-cuda编程指南11.PNG)

一个分层的纹理只能是一个CUDA数组，通过调用cudaMalloc3DArray()和cudaArrayCubemap标志

使用texCubemap()和texCubemap中描述的设备函数来获取Cubemap纹理

Cubemap纹理只在计算能力2.0或更高的设备上支持。

###### 3.2.12.1.6 Cubemap分层的纹理

 b 



## 4. 杂论

[How to Implement Performance Metrics in CUDA C/C++ ]: https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/
[How to Optimize Data Transfers in CUDA C/C++]: https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/

