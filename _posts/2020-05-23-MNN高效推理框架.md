[TOC]



# MNN:高效推理框架



## 简介

对阿里巴巴的MNN的paer进行读一下，在论文中，MNN主要优势：

1. 提出一种可以进行运行时优化的称为推理的机制
2. 为运算符提供全面的内核优化，以实现最佳计算性能
3. 引入后端抽象模块，该模块支持混合调度并保持引擎轻巧。将MNN集成到应用程序中只会使二进制文件大小增加400〜600KB。

给出目前在移动端部署深度学习的问题：

- 模型兼容性
- 设备多样性
- 资源限制



- [ ] 半自动搜索框架



## 架构

![](/home/dji/messi/notes/ 2020-05-23 16-33-52-MNN-architecture.png)



### 离线转换

对于离线转换，转换器首先将来自不同深度学习框架的模型作为输入，并将其转换为我们自己的模型格式（.mnn）。 同时，执行了一些基本的图形优化，例如算子融合（Ashari等人，2015），替换和模型量化（Rastegari等人，2016）



### 设备上推理

对于设备上的推断，涉及三个模块：预处理，操作优化和后端抽象。 对于每个操作，预推断模块提供一种成本评估机制，该机制将信息（例如，输入大小，内核形状）与后端属性（例如，内核数，硬件可用性）结合起来，以解决方案池从中动态确定最佳计算解决方案。 然后，操作级优化模块利用先进的算法和技术如SIMD（单指令多数据）一样，通过流水线技术可以进一步提高性能。

此外，MNN把支持的各种硬件架构，称为后端。 由于没有统一的标准符合所有硬件规范，因此MNN支持不同的软件解决方案，例如OpenCL，OpenGL，Vulkan和Metal。 **所有后端均实现为独立组件，并且后端抽象模块提供了一组统一的接口，以隐藏原始细节（例如，异构后端上的内存管理）。**



#### 预推理

预推理是所提出的半自动搜索体系结构的基础部分。 它利用了一种常见的现象，即在许多深度学习应用程序中，输入大小通常是固定的（或可以预处理为目标大小）。基于此，可以在正式推理之前确定内存使用情况和计算力成本。 因此，一些优化技术（例如内存可以进行预分配和重用），可以进一步提高性能。 预推理的细节可以分为两部分：计算方案选择和预准备执行分离。



##### 计算方案选择

$$
C_t = C_a + C_b
$$

> t:total  a:algorithm b:backend

C_algorithm: 卷积计算一般有两种实现方式：

1. 滑窗
2. winograd

**通过不同的卷积方式来选择不同的卷积实现**

C_backend:通过操作的计算量和后端的gflop的比值来评估耗时



##### 预先执行

在程序执行期间，计算通常与内存分配和释放交织。 对于移动应用程序，花在内存管理上的时间
不容忽视。 由于输入大小是确定的或可以是预先处理成目标大小，MNN可以推断出确切的
虚步遍历整个图所需的内存通过所有操作并汇总所有分配和释放。 从这个意义上说，MNN预分配了所需的内存在预推断阶段用作内存池并重用。

![](/home/dji/messi/notes/2020-05-23-17-07-18-memory_optimization.png)



#### kernel优化



##### Winograd optimization  

步骤是什么？

##### Large matrix multiplication optimization  

- [ ] Strassen algorithm  ？？？



#### 后端

引入后端抽象模块，使所有的硬件平台(如GPU、CPU、TPU)和软件解决方案(如OpenCL、OpenGL、Vulkan)封装而成一个统一的后端类。通过后端类，资源管理、内存分配和调度都有与具体的操作符实现分离。

```c++
//后端类的代码
class XPUBackend final : public Backend {
	XPUBackend(MNNForwardType type, MemoryMode mode);
	virtual ~XPUBackend();
	virtual Execution* onCreate(const vector<Tensor*>& inputs,
	const vector<Tensor*>& outputs, const MNN::Op* op);
	virtual void onExecuteBegin() const;
	virtual void onExecuteEnd() const;
	virtual bool onAcquireBuffer(const Tensor* tensor, StorageType storageType);
	virtual bool onReleaseBuffer(const Tensor* tensor, StorageType storageType);
	virtual bool onClearBuffer();
	virtual void onCopyBuffer(const Tensor* srcTensor, const Tensor* dstTensor) const;
}
```

1. 降低复杂性

   大量的操作符和无数的设备使得操作符优化成为一项重要的任务。主要的挑战在于，异构后端通常有不同的方法来管理资源，例如，分配/释放内存和调度数据。处理这些问题会使实现更容易出错和麻烦。后端类一致管理GPU着色器等资源加载，根据运算符声明完成最优内存分配。使用后端抽象，MNN设法将任务划分为两个独立的部分。“前端操作员”开发人员可以专注于操作执行的高效实现，并隐藏所有不相关的后端细节。“后端”开发人员可以致力于开发不同的后端规范并提供更方便的api。这种任务的分离在实践中是很有意义的，因为在开源项目中，降低贡献壁垒是非常受欢迎的。

2. 混合调度

   异构计算主要涉及后端选择和不同后端之间的数据传输。在MNN中创建推理会话时，可以配置目标后端。如果有多个后端可用，MNN可以根据上述后端评估(第3.2节)为操作确定最优后端。因此，MNN支持操作符在不同后端上执行的灵活组合，甚至在单个推理中也是如此。例如，卷积可以在CPU上运行，下面的ReLU激活可以在GPU上运行。在后端类的帮助下，开发人员不需要担心调度和传输，这将自动地在幕后进行。

3. 轻量级

   每个后端实现都可以作为独立组件工作，同时维护MNN中的统一接口。只要有后端在某些设备上不可用，相应的具体实现可以很容易地从整个框架中剥离出来。例如，Metal (Apple, 2014)在Android平台上是不受支持的，因此我们可以直接去掉Metal模块，而不需要触及特定的操作实现。通过分离操作符和后端，MNN可以成为具有竞争力的轻量级产品。这一点至关重要，因为移动应用程序对二进制大小有严格的限制。

- [ ] 这里和其他厂家的主流推理框架进行比对



#### 技术总结

和其他方案的对比，这里主要是figure6

还有可维护性、可伸缩性和部署成本的高低都对企业的长期增长有很大的影响



## 性能评测

通过对不同平台，不同网络，对比不同的主流框架





