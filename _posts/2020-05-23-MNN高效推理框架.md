[TOC]



# MNN:高效推理框架



## 简介

对阿里巴巴的MNN的paer进行读一下，在论文中，MNN主要优势：

1. 提出一种可以进行运行时优化的称为推理的机制
2. 为运算符提供全面的内核优化，以实现最佳计算性能
3. 引入后端抽象模块，该模块支持混合调度并保持引擎轻巧。将MNN集成到应用程序中只会使二进制文件大小增加400〜600KB。

给出目前在移动端部署深度学习的问题：

- 模型兼容性
- 设备多样性
- 资源限制



- [ ] 半自动搜索框架



## 架构

![](/home/dji/messi/notes/ 2020-05-23 16-33-52-MNN-architecture.png)



### 离线转换

对于离线转换，转换器首先将来自不同深度学习框架的模型作为输入，并将其转换为我们自己的模型格式（.mnn）。 同时，执行了一些基本的图形优化，例如算子融合（Ashari等人，2015），替换和模型量化（Rastegari等人，2016）



### 设备上推理

对于设备上的推断，涉及三个模块：预处理，操作优化和后端抽象。 对于每个操作，预推断模块提供一种成本评估机制，该机制将信息（例如，输入大小，内核形状）与后端属性（例如，内核数，硬件可用性）结合起来，以解决方案池从中动态确定最佳计算解决方案。 然后，操作级优化模块利用先进的算法和技术如SIMD（单指令多数据）一样，通过流水线技术可以进一步提高性能。

此外，MNN把支持的各种硬件架构，称为后端。 由于没有统一的标准符合所有硬件规范，因此MNN支持不同的软件解决方案，例如OpenCL，OpenGL，Vulkan和Metal。 **所有后端均实现为独立组件，并且后端抽象模块提供了一组统一的接口，以隐藏原始细节（例如，异构后端上的内存管理）。**



#### 预推理

预推理是所提出的半自动搜索体系结构的基础部分。 它利用了一种常见的现象，即在许多深度学习应用程序中，输入大小通常是固定的（或可以预处理为目标大小）。基于此，可以在正式推理之前确定内存使用情况和计算力成本。 因此，一些优化技术（例如内存可以进行预分配和重用），可以进一步提高性能。 预推理的细节可以分为两部分：计算方案选择和预准备执行分离。



##### 计算方案选择

$$
C_t = C_a + C_b
$$

> t:total  a:algorithm b:backend

C_algorithm: 卷积计算一般有两种实现方式：

1. 滑窗
2. winograd

**通过不同的卷积方式来选择不同的卷积实现**

C_backend:通过操作的计算量和后端的gflop的比值来评估耗时



##### 预先执行

在程序执行期间，计算通常与内存分配和释放交织。 对于移动应用程序，花在内存管理上的时间
不容忽视。 由于输入大小是确定的或可以是预先处理成目标大小，MNN可以推断出确切的
虚步遍历整个图所需的内存通过所有操作并汇总所有分配和释放。 从这个意义上说，MNN预分配了所需的内存在预推断阶段用作内存池并重用。

![](/home/dji/messi/notes/2020-05-23-17-07-18-memory_optimization.png)