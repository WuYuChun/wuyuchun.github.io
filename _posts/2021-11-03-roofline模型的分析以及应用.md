# roofline模型的应用 评测深度学习模型理论以及相关概念

-----------
> 以下文章装载自 https://zhuanlan.zhihu.com/p/34204282 

>最近在不同的计算平台上验证几种经典深度学习模型的训练和预测性能时，经常遇到模型的实际测试性能表现和自己计算出的复杂度并不完全吻合的现象，令人十分困惑。机缘巧合听了Momenta的技术分享后，我意识到问题的答案其实就在于 Roof-line Model 这个理论，于是认真研究了一下相关论文。现在把自己的心得总结出来，分享给大家。
在真实世界中，任何模型（例如 VGG / MobileNet 等）都必须依赖于具体的计算平台（例如CPU / GPU / ASIC 等）才能展现自己的实力。此时，模型和计算平台的"默契程度"会决定模型的实际表现。Roofline Model 提出了使用 Operational Intensity（计算强度）进行定量分析的方法，并给出了模型在计算平台上所能达到理论计算性能上限公式。
有了 Roofline Model，我就可以知道模型在机器上能跑多快喽～做梦都会笑出声来～

> ## 1. 计算平台的两个指标：算力 [公式] 与带宽 [公式]
算力 [公式] ：也称为计算平台的性能上限，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是 FLOPS or FLOP/s。
[公式]

>带宽 [公式] ：也即计算平台的带宽上限，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是Byte/s。
[公式]

>计算强度上限 [公式] ：两个指标相除即可得到计算平台的计算强度上限。它描述的是在这个计算平台上，单位内存交换最多用来进行多少次计算。单位是FLOPs/Byte。
[公式]

>注：这里所说的“内存”是广义上的内存。对于CPU计算平台而言指的就是真正的内存；而对于GPU计算平台指的则是显存。
> ## 2. 模型的两个指标：计算量 与 访存量


> 计算量：指的是输入单个样本（对于CNN而言就是一张图像），模型进行一次完整的前向传播所发生的浮点运算个数，也即模型的时间复杂度。单位是 #FLOP or FLOPs。其中卷积层的计算量公式如下（具体分析详见我的另一篇文章）
[公式]



> 访存量：指的是输入单个样本，模型完成一次前向传播过程中所发生的内存交换总量，也即模型的空间复杂度。在理想情况下（即不考虑片上缓存），模型的访存量就是模型各层权重参数的内存占用（Kernel Mem）与每层所输出的特征图的内存占用（Output Mem）之和。单位是Byte。由于数据类型通常为float32 ，因此需要乘以四。
[公式]



> 模型的计算强度 [公式] ：由计算量除以访存量就可以得到模型的计算强度，它表示此模型在计算过程中，每Byte内存交换到底用于进行多少次浮点运算。单位是FLOPs/Byte。可以看到，**模计算强度越大，其内存使用效率越高**。


> 模型的理论性能 [公式] ：我们最关心的指标，即模型在计算平台上所能达到的每秒浮点运算次数（理论值）。单位是 FLOPS or FLOP/s。下面我们即将介绍的 Roof-line Model 给出的就是计算这个指标的方法。终于可以进入正题了。
> ## 3. Roof-line Model


> 其实 Roof-line Model 说的是很简单的一件事：模型在一个计算平台的限制下，到底能达到多快的浮点计算速度。更具体的来说，Roof-line Model 解决的，是“计算量为A且访存量为B的模型在算力为C且带宽为D的计算平台所能达到的理论性能上限E是多少”这个问题。



> ### 3.1 Roof-line 的形态


> 所谓“Roof-line”，指的就是由计算平台的算力和带宽上限这两个参数所决定的“屋顶”形态，如下图所示。

> 算力决定“屋顶”的高度（绿色线段）
> 带宽决定“房檐”的斜率（红色线段）

> ### 3.2 Roof-line 划分出的两个瓶颈区域

> **计算瓶颈区域 Compute-Bound**

>不管模型的计算强度 [公式] 有多大，它的理论性能 [公式] 最大只能等于计算平台的算力 [公式] 。当模型的计算强度 [公式] 大于计算平台的计算强度上限 [公式] 时，模型在当前计算平台处于 Compute-Bound状态，即模型的理论性能 [公式] 受到计算平台算力 [公式] 的限制，无法与计算强度 [公式] 成正比。但这其实并不是一件坏事，因为从充分利用计算平台算力的角度上看，此时模型已经 [公式] 的利用了计算平台的全部算力。可见，计算平台的算力 [公式] 越高，模型进入计算瓶颈区域后的理论性能 [公式] 也就越大。



> **带宽瓶颈区域 Memory-Bound**

> 当模型的计算强度 [公式] 小于计算平台的计算强度上限 [公式] 时，由于此时模型位于“房檐”区间，因此模型理论性能 [公式] 的大小完全由计算平台的带宽上限 [公式] （房檐的斜率）以及模型自身的计算强度 [公式] 所决定，因此这时候就称模型处于 Memory-Bound 状态。可见，在模型处于带宽瓶颈区间的前提下，计算平台的带宽 [公式] 越大（房檐越陡），或者模型的计算强度 [公式] 越大，模型的理论性能 [公式] 可呈线性增长。

> ## 4. 模型实例分析


> 下面让我们先分别给出 VGG16 和 MobileNet 的计算量和访存量统计表格，然后再用 Roof-line Model 理论来对比分析一下下。很有意思哦！


> ### 4.1 VGG16

> VGG-16
VGG 可以说是在计算强度上登峰造极的一个模型系列，简约不简单。以 VGG16 为例，从上表可以看到，仅包含一次前向传播的计算量就达到了 15 GFLOPs，如果包含反向传播，则需要再乘二。访存量则是 Kernel Mem 和 Output Mem 之和再乘以四，大约是 600MB。因此 VGG16 的计算强度就是 25 FLOPs/Byte。

>另外如果把模型顶端那两个硕大无比的全链接层（其参数量占整个模型的80%以上）替换为GAP以降低访存量（事实证明这样修改并不会影响准确率），那么它的实际计算强度可以再提升四倍以上，简直突破天际。

>注：以上分析仅限于前向传播计算过程（即模型预测）。如果涵盖反向传播（即模型训练），则计算量和访存量都要考虑梯度更新的具体方式，例如计算 Momentum 几个变量时引入的时间和空间复杂度。


> ### 4.2 MobileNet

> MobileNet 是以轻量著称的小网络代表（最近新出了V2版本，分析见这里）。相比简单而庞大的 VGG16 结构，MobileNet 的网络更为细长，加入了大量的BN，每一层都通过 DW + PW 的方式降低了计算量，同时也付出了计算效率低的代价。从上面超级长的表格就能有一个感性的的认识。

> MobileNet 的计算量只有大约 0.5 GFLOPs（VGG16 则是 15 GFLOPs），其访存量也只有 74 MB（VGG16 则是约 600 MB）。这样看上去确实轻量了很多，但是由于计算量和访存量都下降了，而且相比之下计算量下降的更厉害，因此 MobileNet 的计算强度只有 7 FLOPs/Byte。

> ### 4.3 两个模型在 1080Ti 上的对比

> 作为性价比之王的 1080Ti，我们的两个模型 VGG16 和 MobileNet 的性能将分别位于这个计算平台 Roof-line Model 的什么位置呢？

1080Ti 的算力:11.3TFlop/s
1080Ti 的带宽:484GB/s
因此 1080Ti 计算平台的最大计算强度I=24
VGG16 的计算强度:25
MobileNet 的计算强度:7 

Roof-line Model : VGG16 vs MobileNet


> 由上图可以非常清晰的看到，

> MobileNet 处于 Memory-Bound 区域。在 1080Ti 上的理论性能只有 3.3 TFLOP/s。
VGG16 刚好迈入 Compute-Bound 区域。完全利用 1080Ti 的全部算力。


>虽然 MobileNet 进行前向传播的计算量只有 VGG 的三十分之一，但是由于计算平台的带宽限制，它不能像 VGG 那样完全利用 1080Ti 这个计算平台的全部算力，因此它在 1080Ti 上每秒钟可以进行的浮点运算数只能达到 VGG 的 30%，因此理论上的运行速度大约是 VGG 的十倍（实际上会因为各方面其他因素的限制而使得差别更小）。



> MobileNet 这类小型网络更适合运行在嵌入式平台之上。首先这类轻量级的计算平台根本就放不下也运行不起来 VGG 这种大模型。更重要的是，由于这类计算平台本身的计算强度上限就很低，可能比 MobileNet 的计算强度还要小，因此 MobileNet 运行在这类计算平台上的时候，它就不再位于 Memory-Bound 区域，而是农奴翻身把歌唱的进入了 Compute-Bound 区域，此时 MobileNet 和 VGG16 一样可以充分利用计算平台的算力，而且内存消耗和计算量都小了一两个数量级，同时分类准确率只下降了1%，所以大家才愿意用它。



> 所以说，屠龙时用屠龙刀，日常吃鸡用小刀就可以了，否则只会弄巧成拙。



感谢 
@谭伟
 所指出的：Roofline 模型讲的是程序在计算平台的算力和带宽这两个指标限制下，所能达到的理论性能上界，而不是实际达到的性能，因为实际计算过程中还有除算力和带宽之外的其他重要因素，它们也会影响模型的实际性能，这是 Roofline Model 未考虑到的。例如矩阵乘法，会因为 cache 大小的限制、GEMM 实现的优劣等其他限制，导致你几乎无法达到 Roofline 模型所定义的边界（屋顶）。

> ## 5. 结语


> 本文系统的介绍了：
计算平台的两个指标：算力和带宽。
模型的两个指标：计算量和访存量。
使用 Roof-line Model 分析模型在计算平台上所能达到的理论计算性能，并分析模型在计算平台上的两种互斥状态：计算受限状态和带宽受限状态。
以 VGG16 和 MobileNet 为例，在 1080Ti 计算平台上分析对比它们的计算性能。
欢迎大家指正。

这个roofline图左侧的斜率是峰值带宽，实际的程序跑起来，因为代码实现方式差异，带宽利用率不同，即-你并不是在以峰值带宽出现在这个图上，实际带宽利用率很低很低，即斜率很低，那当然，根据图就可以看出来，斜率小了，计算能力利用率下降了。所以，正确的理解方式应该是，对于给定的workload，在计算强度固定的情况下（其实有的也不固定），【努力提升带宽】才是正途，即-各种并行化手段，ILP之类的。

---

当这个应用程序的计算密度大于一定值之后，将会变成一个受算术逻辑单元的计算量所限制的程序；而这个计算密度如果小于一定值，将会变成一个受存储器带宽所限制的程序。
[monenta让深度模型高效运行的两个视角](https://zhuanlan.zhihu.com/p/33693725)

```
举两个具体的例子，第一个是矩阵乘矩阵，矩阵C等于A乘B，而A跟B分别是一千乘一千的矩阵。假设存储和计算都是用float 32位来表示，这样一个计算将会做1000乘1000乘1000的浮点乘加，也就是2G FLOPS的运算。我们要读取A和B，然后计算出来C，把它写回去，最少的存储器访问就是三个矩阵的大小，也就是12个MB。

另外一个是矩阵乘向量，也就是矩阵A乘向量B，等于向量C，这时候维度还是1000的情况下，它的计算量就是1000乘1000的浮点乘加，也就是2M。而存储器访问的话最少大约是1000乘于1000个浮点数，也就是4MB。

可以明显地看到上面乘矩阵的操作，它的计算量是2G，访存量是12M，那么它的这个计算量除以访存量，也就是刚刚提到的计算密度，大概是200左右。下面这个矩阵和向量中，它的计算量是2M，访存量是4M，那它的计算量除以访存量大约就只有0.5，显然这两个就是非常不同的程序。
```
**从中可以看出，针对某一个具体操作，可以根据它的计算量以及访存量，可以确定是那种受限操作**

小模型部署在这些硬件上，通常都是被存储带宽所限制住了，而不是被计算量所限制住。


---

计算密度降低，本质就是说 带宽资源增大，计算资源减少。
这样的操作其实本质上是用存储器的带宽资源去换取一些计算资源来提升模型性能。

**计算密度从感性上来理解，就是加载一个数字，我们可以用它来进行多少次运算。**

FFT/Winograd的变换，它本质上是一个由精度换取速度的方法。
---

> 以下根据上面所介绍的理论，去思考以及实践
## 概念
下面介绍一些需要重点理解的概念

### MACC
$$a\leftarrow b \times c + a $$
multiply-accumulate operations：**先乘起来再加起来的运算次数**, 也就是说MACC是Flops的两倍

设有全连接层为：
$$y = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + ... + w[n8]*x[8]$$

此处， w 和 x 是两个向量，结果 y 是标量。对于卷积层或完全连接的层。我们将 w[0]*x[0] +... 计数为一个乘法累加或1个MACC，这里的“累加”运算是加法运算，因为我们将所有乘法的结果相加，所以上式具有 n 个这样的MACC（从技术上讲，上式中只有n-1 个加法，比乘法数少一个，所以这里知识认为MACC的数量是一个近似值）。就FLOPS而言，因为有 n 个乘法和 n-1 个加法，所以点积执行 2n-1 FLOPS，因此，**MACC大约是两个FLOPS**。



### FLOPs
是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。


### FLOPS
是loating point operations per second

flops：在计算中，每秒的浮点运算数（FLOPS、flops或flop/s）是衡量计算机性能的指标，在需要浮点计算的科学计算领域很有用。对于这种情况，它比测量每秒指令数（MIPS）更准确。

在NIVDIA GPU中，对于Flops,**每秒所有执行的单精度操作的单一加权总和。CUDA 设备的峰值单精度浮点性能定义为 *CUDA 核心数乘以图形时钟频率乘以 2*。因为使用融合乘加 ( FFMA) 指令同时执行两个运算的能力。**

如何将 GHz 转换为 FLOPS？
处理器时钟频率（MHz 或 GHz）和每秒浮点运算数 (FLOPS) 之间没有通用的直接转换。这两个数字可能是相关的，但影响处理器实际性能的因素有很多。

对于给定的处理器，您可以计算每个时钟周期的理论峰值浮点运算。计算完成后，您可以乘以时钟速率来确定理论峰值 FLOPS。这个数字纯粹是理论值，代表处理器可以达到的最大性能。如果你错过了我说的最后三遍，这是理论上的峰值性能。

例如，考虑具有以下特征的深度流水线处理器：

- 每个周期可以发出 1 个新的浮点加法
- 每个周期可以发出 1 个新的浮点乘法
- 每个周期可以从内存中加载 1 个新值
- 每个周期可以存储 1 个新值到内存中
- 浮点加法有 4 个周期的延迟
- 浮点乘法有 3 个周期的延迟
- 1GHz 时钟频率

因为这台机器每个周期可以发出 1 个新的浮点加法和 1 个新的浮点乘法，所以这台机器具有 **2 GFLOPS 的峰值理论性能：每个周期 2 个浮点运算，乘以 1GHz 时钟速率**。

它们是对两种不同事物的测量。当然，处理器的时钟速度（以 GHz 或千兆赫兹为单位）会影响它可以执行多少 FLOPS（每秒浮点运算），但这不是唯一的因素。

汽车的马力对于它的加速速度很重要，但其他因素，如最终传动比、轮胎牵引系数、​​车辆重量和许多其他因素也会影响结果。例如，一辆重量为 4,000 磅的 500 马力汽车将比一辆重量为 2,000 磅的 500 马力汽车慢得多。

类似地，通常安全的假设是，具有较高时钟速度的处理器通常比具有较慢时钟速度的类似 CPU 执行更多 FLOPS，但这不是普遍真理，而且并非所有 CPU 都相似。



### IPS
指令每秒（IPS）是一个的度量计算机的处理器速度。对于复杂指令集计算机(CISC)，不同指令占用的时间不同，因此测量值取决于指令组合；即使在比较同一系列的处理器时，IPS 测量也可能存在问题。许多报告的 IPS 值代表了人工指令序列的“峰值”执行率，很少分支且没有缓存争用，而实际工作负载通常会导致 IPS 值显着降低。内存层次结构也极大地影响处理器性能，这是 IPS 计算中很少考虑的问题。由于这些问题，现在一般使用Dhrystone等综合基准来估计常用应用程序中的计算机性能，而原始 IPS 已被废弃。该术语通常与度量前缀（k、M、G、T、P 或 E）结合使用，以形成每秒千条指令( kIPS )、每秒百万条指令( MIPS ) 和每秒十亿条指令( GIPS)） 等等。

**FLOPS 和MIPS是计算机数值计算性能的度量单位。浮点运算通常用于科学计算研究等领域。单位 MIPS 衡量计算机的整数性能。**


### TOPS
(Tera FLoating-point Operations Per-second) ：描述某种操作的计算密度

tops:指每一秒中一次张量计算，每秒万亿次操作，大多数操作都是MAC（乘法/累加）
$$TOPS=(MAC单元的数量）\times（MAC操作的频率）\times 2$$

> TOPS 是整数，FLOPS 是浮点数? 这里的讲解是否正确？
> 它基本上取决于GPU架构。在帕斯卡架构中 1TFLOP = 4TOPS。

> 高TOPS都是运算单元（PE）的理论值，而非整个硬件系统的真实值。**真实值更多取决于内部的SRAM、外部DRAM、指令集和模型优化程度。最糟糕的情况下，真实值是理论值的1/10算力甚至更低。**





### 各个神经网络中的flops以及带宽

| 模型 | 计算量 flops | 访存量byte | 计算强度 flops/byte | 1080实际耗时 ms | Xavier实际耗时 ms|
|---- |-----         |-----     | -----   |----| ----|
| VGG16             | 31G   | 675M  | 45.9 |  2.96692 |   19.0317  |
| Resnet152         | 22.6G | 472M  | 47.9 |  3.00223 | 20.4719   |
| Resnet 50         | 7.72G | 211M  | 36.6 |  2.98112 | 7.48487   |
| Resnet 18         | 3.63G | 72.5M | 50.1 |   |    |
|Inception V2       | 4.07G | 100M  | 40.7 | 2.98328  | 5.19214   |
|MobileNet          | 1.15G | 57.8M | 19.9 | 2.98601  | 1.94529   |
|ShuffleNet-0.5x-g3 | 68.5M | 19.0M | 3.61 |   |    |
|shuffleNet-0.5x-g8 | 76.9M | 29.4M | 2.91 |   |    |

> - 1080Ti 的算力:11.3TFlop/s
> - 1080Ti 的带宽:484GB/s
> - 1080Ti 计算平台的最大计算强度I=24
> - 1080   的算力：9TFlop/s
> - 1080   的带宽：320.3GB/s
> - 1080   计算平台的最大计算强度I= 28
> - xavier 的算力：11Tflop/s
> - xavier 的带宽：137GB/s
> - xavier 计算平台的最大计算强度I= 80.29

**这里引申出来一个问题，为啥1080的计算强度要比1080Ti的强度大？为什么呢？对模型的执行时间有什么影响呢？**

### 计算神经网络在硬件有效利用率

Mac efficiency:指的是硬件针对于该网络，能有效利用的算力。

计算公式： 

$${\frac{网络理论算力Gflops}{设备最大算力Gflops \times 网络运行耗时latency}}$$

例如下面就是使用Xavier 11Tflops来做计算

|模型|bath_size|MACs(G,Bathc)|Latency(ms,bathc)|mac_efficiency|
|---- |---- |---- |---  |---   |
|vgg19| 1 | 19.67 | 11.9 | 15.03%|
|     |   |       |       |     |


### 如何通过硬件性能指标计算算力FLOPS

[自动驾驶算力TOPS的谎言](https://picture.iczhiku.com/weixin/message1590477454606.html)

算力理论值取决于运算精度、MAC的数量和运行频率,大概可以简化为这样子，INT8精度下的MAC数量在FP16精度下等于减少了一半。FP32再减少一半，依次类推。其计算相当简单，假设有512个MAC运算单元，运行频率为1GHz，INT8的数据结构和精度（自动驾驶推理领域常见精度），算力为$512 x 2 x 1 Gigahertz = 1000 Billion Operations/Second = 1 TOPS(Tera-Operations/second)$。如果是FP16精度那么就是0.5TOPS。例如英伟达的Tesla V100有640个Tensor核，每核有64个MAC运算单元，运行频率大约1.480GHz，那么INT8下算力为$640*64*2*1.480Gigahertz=121TOPS$。但是Tesla V100的训练就使用CUDA核，有5120个CUDA核，双精度（FP64）下算力是另一种算法了。这个月刚发布的A100，有432个三代Tensor核，每个核包含512个MAC运算单元（等同于64个双精度MAC），运行频率为1.41Gigahertz，INT8下算力为$432*512*2*1.41Gigahertz=624TOPS$。特斯拉的FSD是9216个MAC运算单元，运行频率是2GHz，INT8算力为$9216*2*2GHz=36.86TOPS$。

真实值和理论值差异极大。决定算力真实值最主要因素是内存（ SRAM和DRAM)带宽，还有实际运行频率（即供电电压或温度），还有算法的batch尺寸。例如谷歌第一代TPU，理论值为90TOPS算力，最差真实值只有1/9，也就是10TOPS算力，因为第一代内存带宽仅34GB/s。而第二代TPU下血本使用了HBM内存，带宽提升到600GB/s（单一芯片，TPU V2板内存总带宽2400GB/s）。最新的英伟达的A100使用40GB的2代HBM，带宽提升到1600GB/s，比V100提升大约73%。特斯拉是128 bitLPDDR4-4266 ，那么内存的带宽就是：$$2133MHz*2DDR*128bit/8/1000=68.256GB/s。$$ 比第一代TPU略好（这些都是理论上的最大峰值带宽）其性能最差真实值估计是2/9。也就是大约8TOPS。16GB版本的Xavier内存峰值带宽是137GB/s。

为什么会这样？这就牵涉到MAC计算效率问题。如果你的算法或者说CNN卷积需要的算力是1TOPS，而运算平台的算力是4TOPS，那么利用效率只有25%，运算单元大部分时候都在等待数据传送，特别是batch尺寸较小时候，这时候存储带宽不足会严重限制性能。但如果超出平台的运算能力，延迟会大幅度增加，存储瓶颈一样很要命。效率在90-95%情况下，存储瓶颈影响最小，但这并不意味着不影响了，影响依然存在。然而平台不会只运算一种算法，运算利用效率很难稳定在90-95%。这就是为何大部分人工智能算法公司都想定制或自制计算平台的主要原因，计算平台厂家也需要推出与之配套的算法，软硬一体，实难分开。

那么每瓦TOPS有没意义呢？抱歉，也没多大意义的。首先是因为算力值本身就有很多种可能，厂家肯定只选数值最大的那个给你看。其次这只是运算单元芯片本身的功耗与算力比，没有考虑DRAM。在深度学习计算中，数据频繁存取，极端情况下，功耗可能不低于运算单元。

### 评论nvidia GPU
[Nvidia—xavier整理](https://zhuanlan.zhihu.com/p/71984335)

[端侧soc算力的计算公式](https://www.zangcq.com/2019/11/16/%E5%85%B3%E4%BA%8Env%E7%AB%AF%E4%BE%A7soc%E7%AE%97%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F/)


|   | xavier module |
|---|---------|
|cpu| 8-core 64bit ARMv8.2 @2265MHZ|
|GPU| 512-core Volta@1377MHZ whit TensorCores|
|DL| DLA|
|Memory|16GB 256-bit LPDDR4x@2133Mhz 137GB/s|


我们看到NV或者其他芯片公司常常说芯片提供了多少算力，从几T到几百T不等，一会儿时FLOPS，一会儿是OPS，令我们常人看了费解，只是看着数字说句NB。实际上呢，也不难理解，那么我们就来详细算一下这些TFLOPS是怎么来的。

我们知道GPU常用的数据类型实际上float，我们常称之为 FP32，那么FP就float的缩写，而FLOPS实际上就是

float operation per second，每秒浮点数运算数，来作为处理器的性能。如果一个处理器每秒钟只能计算一次浮点数的加法，那么他的性能就是1FLOPS；

OPS常见的就是INT8的类型，就是8位的整数，-128到127的范围。我们知道数据位数越长，那么对应每次运算时间就会越长，我们可以看到FP16（16位浮点数）常常是INT8类型的二分之一算力。而fp32就是INT8类型的四分之一了。所以为了发布会效果，我们常常看到皮衣黄教主用INT8数据的算力来忽悠大家。当然，也是很高的了。

#### cuda core的计算方式
TX2的主频为1.3GHz；

TX2 共有2 个Pascal 的SMs；每个SM呢又有128个小SP，

每个SP在一个时钟周期内可以做一次FMA（fused multiply add）乘法和加法，那么也就是2次FP32类型数据FLOPS，那么总共算力就可以得到：

总算力 = 主频 x SMs数 x 每个SM可以计算的最高乘加 x 2 （fma）

1.3GHz x 2 （SMs）x 128 x 2 FLOPS = 665.6G FLOPS


根据之前的残留的表上可以看到，

TX2对应计算能力6.2时，，fp16的算力，即为

1.3GHz(Max Freq) x 2 （SMs）x 256 x 2 FLOPS = 1.3T FLOPS了，证明还是可以算对的。

同样如果想得到 INT8类型的数据的话，也就是FP16两倍的算力罢了。

#### tensor core的计算方式
tensor core第一次引入是在volta架构，那么我们从[白皮书](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf)可以看到对应的计算方式。

在volta架构中，每个SM中有8个Tensor Core，每两个tensor core在一个处理单元里。

每个Tensor Core 每个clock可以处理 64个FP16浮点的乘加，那么这就比每个CUDA core提升了64倍计算效率了。

那么我们同样用算力公式来计算一下。

**Xavier 的算力 512-core NVIDIA Volta @ 1377MHz with 64 TensorCores**

总算力=cuda core算力 + tensor core 算力

公式如下：
$$cudacore算力= 主频 \times SMs数 \times 每一个SM可以计算的最高乘加 \times 2FLOPS$$
$$tensorcore算力 = 主频 \times SMs数 \times Tensor Core Num Per SM \times 64 \times 2FLOPS$$

> 这里竟然使用fp16作为表示？

$$ 1377Mhz \times 8SMs \times 128 FP16 \times 2 FLOPS = 2.689TFLOS $$
$$ 1377Mhz \times 512 \times 2 \times 2 FLOPS $$

$$1377MHz \times 8SMS \times 8TensNums \times 64 \times 2FLOPS = 10.758TFLOPS $$

总的total算力：**13.4FLOPS(Fp16)**  乘以2 --》 26.8TOPS

Xavier实际上还集成2 x DeepLearning Accelerator，提供了5TOPS的算力，这样与 13.4×2 = 26.8 + 5 与声称的32TOPS就可以对齐了。

