---
layout: post

title:  "cuda的最佳实践"

date:   2020-05-09 14:29

excerpt:

tag:

- GPU
---




[TOC]



# 简介

这个是用来指导如何从gpu中获取最好的性能。

一般的优化是按照下面的流程来的：

![](/images/posts/2020-05-09-08-31-34-APOD.png)

评估：识别算法的瓶颈，评估热点函数，确定工作量

并行化：开发cuda代码或者调用现有的cuda相关的库：cuBLAS等

优化：慢慢来

部署：



## 异构计算

主机和设备之间的区别：主要是线程模型以及物理存储

- 线程资源
- 线程：cpu切换线程代价比gpu上切换线程要大
- RAM

考虑下面的情况，两个N*N的矩阵

1. 加法运算：NxN的计算，搬运数据是3NxN
2. 乘法运算：N×NxN的计算



# 优化的建议



**最高优先级：为了最大程度地提高开发人员的生产力，请对应用程序进行性能分析，以确定热点和瓶颈。**

**最高优先级: To get the maximum benefit from CUDA, focus first on finding ways to
parallelize sequential code.**



## Profile

**先用gprof来识别出c++代码（cpu）中的热点**

**利用Amdahl's or Gustafson's Law来确定算法加速的上界**



## 并行化

几种把代码并行化的策略



### 并行库

- cuBLAS
- cuFFT
- Thrust：扫描、排序、规约

### 设置并行编译选项

通过设置编译选项来使得代码并行化

- [ ] **OpenACC？这个是标准是什么？**

### 编写cuda代码



# 获取正确的结果

## Debug

- [ ] cuda-gdb 这个要用起来



# 性能指标（Metrics）

在优化cuda代码的时候，需要了解如何准确地测量性能以及性能在整体中扮演的角色。



## 时间



**cpu时间**

- [ ] 如何测量cpu的时间




**gpu时间**

- [ ] 如何测量gpu的时间






## 带宽



数据的排布影响数据传输

为了衡量性能，必须进行理论带宽统计以及实际带宽统计

**高优先级：在衡量性能和优化收益时，将计算的有效带宽用作度量标准。**



### 理论带宽的计算

例子：Tesla 使用GDDR5（双数据），1.85GHz，384-bit带宽

177.6GB/s=(1.85*10"9*(348/8)*2/10"9)---其中“表示幂

**公式：**
$$
带宽=(GHz*带宽/8*2)
$$


**注意：启用ECC时，有效的最大带宽会因内存校验和的额外流量而减少大约20％，尽管ECC对带宽的确切影响取决于内存访问模式。**



### 实际带宽的计算

公式：


$$
有效带宽=(b_r+b_w)/time
$$
Br is the number of bytes read per kernel, Bw is the number of bytes written per kernel, and time is given in seconds.



### 通过Profiler测量吞吐

- Requested Global Load Throughput
- Requested Global Store Throughput
- Global Load Throughput、
- Global Store Throughput
- DRAM Read Throughput
-  DRAM Write Throughput

Requested Global Load Throughput 和 Requested Global Store Throughput 表示内核请求的全局显存的吞吐量，和有效带宽计算的值对应。



# 显存优化

内存优化是提高性能的最重要领域，通过最大化带宽来最大程度利用硬件。



## 在主机和设备之间的数据传输

主机和设备之间传输的带宽（通过PCIE总线 8GB/s）和设备传输的代码差距比较大

**高优先级：最大限度地减少主机与设备之间的数据传输，即使这意味着在设备上运行某些内核（与在主机CPU上运行这些内核相比）也未显示出性能提升。**

临时数据结构应在设备内存中创建，由设备操作并销毁，而不要被主机映射或复制到主机内存中。

将每一次小的传输，变成批量的

当使用页面锁定（或固定）内存时，可以在主机和设备之间获得更高的带宽。

但是如何设定固定内存？设定太多的固定内存会影响整体的性能。



### 传输和计算异步

传输和计算重叠需要固定内存以及stream

顺序执行

```c++
cudaMemcpy(a_d, a_h, N*sizeof(float), dir);
kernel<<<N/nThreads, nThreads>>>(a_d);
```

异步执行

```c++
size=N*sizeof(float)/nStreams;
for (i=0; i<nStreams; i++) {
	offset = i*N/nStreams;
	cudaMemcpyAsync(a_d+offset, a_h+offset, size, dir, stream[i]);
	kernel<<<N/(nThreads*nStreams), nThreads, 0,stream[i]>>>(a_d+offset);
}
```



![](/images/posts/2020-05-11-09-31-01_execute.png)



### 0拷贝

**低优先级: Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later.**



## 设备显存空间

![](/images/posts/2020-05-11-09-39-03-spaces_on_device.png)



![](/images/posts/2020-05-12-09-21-55-存储.png)



![](/images/posts/2020-05-11-09-41-14-salient_feature_device_memory.png)



### 全局访问

**最重要的性能考虑因素可能是全局内存访问的合并。**

**高优先级：确保在可能的情况下合并全局内存访问。**

这里的全局访问需要考虑cache L1 和L2

注意：For devices of compute capability 3.x, accesses to global memory are cached only in L2;
L1 is reserved for local memory accesses. Some devices of compute capability 3.5, 3.7, or 5.2 allow opt-in caching of globals in L1 as well.

针对于启用ECC的，也会有影响

顺序对齐的访问模式，如下所示

![](/images/posts/2020-05-11_09-52-38-address_warp.png)

如果任何线程未请求该行的某些单词（例如，如果多个线程访问了同一单词，或者某些线程未参与访问），则无论如何都将获取高速缓存行中的所有数据。

![](/images/posts/2020-05-11-09-55-43-misaligned_access.png)

这里使用了两个128byte L1 cache lines

- [ ] 如何做下面的实验呢？并且测量相应的带宽？

```c++
A copy kernel that illustrates misaligned accesses
__global__ void offsetCopy(float *odata, float* idata, int offset)
{
	int xid = blockIdx.x * blockDim.x + threadIdx.x + offset;
	odata[xid] = idata[xid];
}
```

- [ ] strided 访问

```c++
A kernel to illustrate non-unit stride data copy
__global__ void strideCopy(float *odata, float* idata, int stride)
{
	int xid = (blockIdx.x*blockDim.x + threadIdx.x)*stride;
	odata[xid] = idata[xid];
}
```

![](/images/posts/2020-05-11-10-16-44-stride_access.png)

跨度为2导致负载/存储效率为50％，因为未使用事务中的一半元素，这代表了浪费的带宽。 随着步幅的增加，有效带宽减小，直到为经线中的32个线程加载32行缓存为止。如图8所示。



### 共享内存

[]: https://devblogs.nvidia.com/using-shared-memory-cuda-cc/	" NVIDA"



**每个SM都有与其关联的相当少量的共享内存，通常为16KB的内存**。如果每个SM具有16KB的共享内存，并且在一个SM上同时运行4个线程块，则每个线程块可用的最大共享内存量将为16KB / 4（4KB）。

![](/images/posts/2020-05-12-09-24-17-共享内存.png)

在kernel中声明共享内存的方式有多种，具体取决于在编译时还是在运行时知道内存量。



共享内存被分为大小一样的内存banks，并且能同时访问。

- [ ] 什么是banks冲突

![](/images/posts/2020-05-12-10-09-01-bancks-confliks.png)

为了最大程度地减少banks冲突，重要的是要了解存储器地址如何映射到banks以及如何优化调度存储请求。

**so bank conflicts can occur between any threads in the warp**



写一个这样的例子进行运行

```c++
__global__ void simpleMultiply(float *a, float* b, float *c,int N)
{
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	float sum = 0.0f;
	for (int i = 0; i < TILE_DIM; i++) {
		sum += a[row*TILE_DIM+i] * b[i*N+col];
	}
	c[row*N+col] = sum;
}
```

it is necessary to consider how warps access global memory in the for loop. 



```c++
__global__ void coalescedMultiply(float *a, float* b, float *c, int N)
{
    __shared__ float aTile[TILE_DIM][TILE_DIM];
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	float sum = 0.0f;
	aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
	for (int i = 0; i < TILE_DIM; i++) {
		sum += aTile[threadIdx.y][i]* b[i*N+col];
	}
	c[row*N+col] = sum;
}
```

each element in a tile of A is read from global memory only once, in a fully coalesced fashion (with no wasted bandwidth), to shared memory. 

**这说明了当硬件L1高速缓存逐出策略与应用程序的需求不完全匹配时，或者当L1高速缓存不用于从全局内存中读取数据时，将共享内存用作用户管理的高速缓存。**

![](/images/posts/2020-05-12-10-13-13-冲突.png)

![](/images/posts/2020-05-12 10-14-31-foure-confile.png)

```c++
__global__ void sharedABMultiply(float *a, float* b, float *c,int N)
{
	__shared__ float aTile[TILE_DIM][TILE_DIM],
	bTile[TILE_DIM][TILE_DIM];
	int row = blockIdx.y * blockDim.y + threadIdx.y;
	int col = blockIdx.x * blockDim.x + threadIdx.x;
	float sum = 0.0f;
	aTile[threadIdx.y][threadIdx.x] = a[row*TILE_DIM+threadIdx.x];
	bTile[threadIdx.y][threadIdx.x] = b[threadIdx.y*N+col];
	__syncthreads();
	for (int i = 0; i < TILE_DIM; i++) {
		sum += aTile[threadIdx.y][i]* bTile[i][threadIdx.x];
	}
	c[row*N+col] = sum;
}
```

- [ ] 这里需要把这个写成一个例子进行验证对比（P48)

使用共享内存的三大原因：

- 允许合并访问全局内存
- 从全局内存中消除冗余负载
- 为了避免浪费带宽



### 局部存储

局部存储并不是代表访问它是很快



### 文本内存

文本存储是cached，在kernel调用中，纹理缓存与全局内存写入之间的一致性并不保持一致。



### 常量内存

64KB

**The constant memory space is cached**





# 执行优化

这里关键的是让处理器（sm)尽可能的忙起来，这里的关键概念是并发率

**中等优先级。为了隐藏由寄存器依赖性引起的延迟，每个多处理器保持足够的活动线程数量（即足够的占用率）。**

**中等优先级。每个区块的线程数应该是32个线程的倍数，因为这样可以提供最佳的计算效率，并且有利于凝聚。**





## 并发率（Occupancy）

**Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. This metric is occupancy.**

更高的占用率并不总是等同于更高的性能，在这一点上，额外的占用率并不能提高性能。 但是，低占用率总是会干扰隐藏内存延迟的能力，从而导致性能下降。



影响占用率的因素：

- 寄存器：So, if each thread block uses many registers, the number of thread blocks that can be resident
  on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor。



数据传输和kener执行的重叠



## 多个上下文（context）

如果多个CUDA应用进程同时访问同一个GPU，这几乎总是意味着多个上下文，因为除非使用**CUDA多进程服务**，否则一个上下文被绑定到一个特定的主机进程。

- [ ] cuda多进程服务指的是什么？

当上下文切换的频繁的时候，会降低利用率

- [ ] 主上下文（primary context)



## block以及threads的数量

每个网格的块的尺寸和大小以及每个块的warp的尺寸和大小都是重要的因素。

When choosing the first execution configuration parameter-the number of blocks per grid, or grid size the primary concern is keeping the entire GPU busy. The number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. 

Notify：**例如，将占用率从66%提高到100%，一般情况下并不会带来相同类比的性能提升。占用率较低kenerl的每个线程的可用寄存器数会比高占用率的内核多。这可能导致更少的寄存器溢出到本地内存。通常情况下，一旦占用了达到了50%，额外增加的占用率并不会转化为提高了性能。在某些情况下，即使是在某些情况下，也可以完全覆盖延时，甚至是 更少的翘曲，特别是通过指令级并行化(ILP)。**



经验法则：

- 每个块的线程应该是warp大小的倍数，以避免浪费计算时间。
- 每个块至少应使用64个线程，并且只有在有多个每个多处理器的并发块。
- 每块128到256个线程之间是一个比较好的选择，也是一个很好的初始范围。使用不同尺寸的线块进行实验。
- 使用几个（3到4个）小线块，而不是每个大线块。如果延迟影响到性能，多处理器的性能。这对以下几点特别有利经常调用__syncthreads()的内核。



## 共享内存

确定性能对占用率的敏感度的一个有用的技术是通过试验动态分配的共享内存的数量来确定性能对占用率的敏感度，这在执行配置的第三个参数中规定了。通过简单地增加这个参数（不修改内核），可以有效地降低内核的占用率，并测量其对性能的影响。





# 指令优化

对于指令的优化，最好的做法就是在高优先级的优化之后在进行



## 算术指令

**低优先级：使用移位操作，避免昂贵的除法和模数计算。**

**中等优先级:只要速度高于精度，就使用快速数学库。**

____sinf(x) and ____expf(x)). Functions following functionName() naming convention are slower but have higher accuracy (e.g., sinf(x) and expf(x)). "

**The -use_fast_math compiler option of nvcc coerces every functionName() call to the equivalent __functionName() call..**



## 内存指令

**高优先级：尽量减少对全局内存的使用。尽可能选择共享内存访问。**



# 控制流

**高优先级:避免同一warp内的不同执行路径。**

**低优先级：使编译器能够很容易地使用分支预测来代替循环或控制语句。**

**中低优先级：使用有符号整数而不是无符号整数作为循环计数器。**

**高优先级：避免在分支代码内部使用__syncthreads()。**



# 了解编程环境

![](/images/posts/2020-05-14-08-39-36-platformPackage.png)



# 总结

三个基本策略：

-  Maximizing parallel execution
-  Optimizing memory usage to achieve maximum memory bandwidth
-  Optimizing instruction usage to achieve maximum instruction throughput

