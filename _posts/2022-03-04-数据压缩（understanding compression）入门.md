# 数据压缩（understanding compression）入门

## 前言

- [ ] 
- [ ] 无损方法：

- 去掉重复数据（LZ算法）
- 熵压缩（哈夫曼编码，算术编码）

- [ ] 有损方法：

- 降低精度（裁断或降采样）
- 图像/视频压缩
- 音频压缩

数据每年的增长速度很快，但是网络带宽的增加、压缩算法的性能提升、存储容量增长的速度就很慢

数据压缩的基础就是数学，早期，数据压缩领域就是寻找并发现了操纵数据集中符号的**概率分布**的不同方法，并利用这些方法来生成包含同样的信息但更小的数据集

> youtube  comressor head

数据压缩无非是用最紧凑的方式来表示数据

--------------------------------

数据压缩算法有5大类：

1. 变长编码
2. 统计压缩
3. 字典编码
4. 上下文模型
5. 多上下文模型

信息熵：一种度量消息所携带信息内容的方法，数据压缩就是这样的应用：“在保证信息能恢复的前提下，我们能将消息变得多么紧凑？”

使得数据压缩程度超过香农发明的公式计算出来的信息熵

-------------

对数据压缩，通常有两个思路：

1. 减少数据中不同符号的数量（即让“字母表”尽可能小）
2. 用更少的位数对更常见的符号进行编码（即常见的“字母”所用的位数最少）

----

理解二进制

根据信息论的观点，一个数值所包含的信息内容等于，为了在一个集合唯一地确定这个数值，需要做出的二选一决定的次数

**熵：表示一个数所需要的最少二进制位数**
$$
lb(x)=ceil(log(x+1)/log(2))
$$
数值LOG2表示形式虽然高效，但是有问题：

- 用最少的二进制位数来表示一个数，在解码相应的二进制字符串时会产生混乱（因为不知道该数对应的LOG2长度），会与硬件的执行性能相冲突

-----------

信息熵
$$
H(S)=-\sum_{i=1}^np_ilb(p_i)
$$
下面用[A,B,B,C,C,C,D,D,D,D]来说明这个公式

| 下标 | 概率p_i | lb(p_i) | p_ixlb(p_i) |
| ---- | ------- | ------- | ----------- |
| 1    | 0.1     | -3.32   | -0.3321     |
| 2    | 0.2     | -2.23   | -0.4642     |
| 3    | 0.3     | -1.73   | -0.5208     |
| 4    | 0.4     | -1.32   | -0.5284     |
|      |         |         | -1.8455     |

结论：表示这组数据每个符号平均约需要1.85个二进制位，也就是，这个数组，一共需要 2*10 = 20（bit)进行表示

--------------------

利用真实数据的两个性质：

1. 排序
2. 符号之间的关系

[1,2,3,4]和[1,3,2,4]两个集合，按照香农的定义，这两个的信息熵相同，但是发现，其中一个有排序的关系。

**突破熵的关键在于，通过利用数据集的结构信息将其转换为一种新的表示形式，而这种新表示的熵比源信息的熵小**



## VLC（变长编码）

目的：减少传输信息过程中所需要的总工作量

数据集中一个符号出现的概率越大，整个数据集的熵也就越小，数据集也就越容易压缩

*如何在应用程序运行VLC进行压缩？给定的数据集，怎样构造VLC*

对数据进行VLC通常有3个步骤：

- 遍历数据集中所有符号并计算每个符号的出现概率
- 根据概率为每一个符号分配码字，一个符号出现的概率越大，所分配的码字越短
- 再次遍历数据集，对每一个符号进行编码，并将对应的码字输出到压缩后的数据流中

以TOBEORNOTTOBEORTOBEORNOT为例

```
# 计算符号的频率，统计出数据集中所有符号的直方图，遍历数据集，并计算每个符号出现的次数
# T-》5，O->8，B-》3，E-》3，R-》3，N-》2

#为每一个字符分配码字，O-》11，T-》00，B-》011，E-》101，R-》0100，N-》0101

#从数据流中一一读出各个符号，然后从码字表中查找出当前符号所对应的码字，添加到输出流，处理完所有的流，再把符号码对应表添加到输出流的前面

```

前缀的性质

类别：

- [ ] 二进制编码
- [ ] 一元码
- [ ] elias gamma编码
- [ ] varint编码

人们创造了数百种VLC算法

**VLC编码方法是根据某个数值期望的出现频率来为该值分配码字符，因此每种VLC编码方法，对于数据集中的各个符号如何分布，都有相应的期望，因此，为了数据集选择适当的VLC编码方法，关键在于使VLC背后的概率模型与该数据集匹配，如果偏离这个方向，最终得到的可能会是更大的数据流**





## 统计编码

统计编码算法通过数据集符号出现的概率来进行编码使结果尽可能与熵接近



### 哈夫曼编码

一个输入是概率列表，输出是码字的算法，创建哈夫曼树





### 算术编码

算术压缩的神奇之处：它将转换应用到整个源数据上以生成一个输出值，而表示这个输出值所需要的二进制位数比源数据本身少



### ANS：非对称数学系统

新的精确熵编码方法



### FSE 有限状态熵



## 自适应统计编码

**自适应统计编码的关键在于其符号码字对应表并非一成不变，相反，可以根据读到符号动态地生成VLC，这个过程的动态性质，让我们可以根据需要对VLC表进行修改，比如对其重置**

- [ ] 自适应算术编码
- [ ] 自适应哈夫曼编码

动态改进的优点：

- 有生成符号码字对应表的能力，无须将符号码字对应表显示地存储在数据流中，数据流变小后，计算性能就能有所提供
- 有实时压缩数据的能力，无须在将整个数据集作为一个整体来处理



## 字典转换

事实上今天所有的主流压缩算法（比如GZIP或者7-Zip）都会在核心转换步骤中使用字典转换

工作方式：

- [ ] 给定源数据流
- [ ] 构建单词字典（而不是符号字典）
- [ ] 将统计压缩应用到字典中的单词上

阶段一：找出单词并进行切分-------》阶段2：计算频次并对数据流编码

### LZ算法

找出最佳分词方式

工作原理：

- [ ] 搜索缓冲区
- [ ] 找出匹配
- [ ] 滑动窗口
- [ ] 用记号标记匹配
- [ ] 没有找到匹配时

存在很多LZ算法的变体



## 上下文数据转换

给定一组相邻的符号集，对它们进行某种方式的变换使其更容易压缩，包括行程编码，增量编码 和 伯罗斯-惠勒变换



### RLE

通常认为RLE是单字符上下文模型，任何给定的符号，在编码时都只考虑它的前一个符号



### 增量编码

将一组数据转换为各个相邻数据之间的相对差值（即增量）的过程

- [ ] XOR增量编码
- [ ] 参照系增量编码



### BWT

工作原理：通过打乱数据流次序来让重复的字串聚集在一起，这一操作本身并不能压缩数据，却可以为后续的压缩系统提供转换好的数据流，方便压缩



## 多媒体压缩

- [ ] 有损压缩

### 选择正确的图像格式

- [ ] PNG
  - [ ] 无损图像格式，允许文件中存在云数据块，可以将而外的数据附加到文件中（APP可以对PNG图像打标签）
- [ ] JPG
  - [ ] 包含元数据，有损
  - [ ] 大多数移动设备闲置有系统可用的JPG编码和解码的硬件，比解码一个PNG文件的速度要快
- [ ] GIF
  - [ ] 支持动画
- [ ] WebP
  - [ ] 介于PNG和JPN之间的地带

*GPU纹理方式DXT，ETC，PVR*

SVG生成格式



## 通用压缩

- [ ] GZIP
- [ ] BZIP2
- [ ] PAQ



## 性能评测



- [ ] 更好的压缩率：内容压缩后的大小与压缩前大小之比
- [ ] 更小的内存需求：
- [ ] 更快的解压速度：
- [ ] 压缩性能：将数据转换为压缩后的形式需要多长时间
  - [ ] CPU速度
  - [ ] 内存



目前所有的压缩算法陷入回报率递减的困境



使用场景

- [ ] 线下压缩，客户端解压
- [ ] 客户端压缩，云端解压
- [ ] 云端压缩，客户端解压
- [ ] 客户端压缩，客户端解压



**gzip成为世界上使用较多的通用文档压缩算法，解码性能是其中的主要原因之一，GZIP算法生成的压缩文件大小合适且解压速度很快**



# 序列化数据

除了图像数据，序列化内容是网络应用程序处理第二多的数据格式

序列化是将高级数据对象转换为二进制字符串的过程（与之相反的过程则称为反序列化）







