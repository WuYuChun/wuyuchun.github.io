



异构架构的模型训练



三种流行的GPU并行性

- 模型并行性:将模型分为几个部分,其中不同的部分分配给不同的设备,在反向传播模式中，前向设备的中间输出在前向步骤中传输到下一设备，而反向传播期间将下一设备的梯度传输到前一设备。 模型并行性是必不可少的，特别是对于无法容纳单个GPU有限内存的大型模型而言。 但是，繁重的中间输出和梯度通信可能会导致高延迟。
- 管道并行性:管道并行性类似于模型并行性，而不是将模型拆分为多个步骤，管道并行性将单个步骤拆分为多个部分。
- 数据并行性:  数据并行性方法将训练数据分为不同的部分，以分发到不同的设备中。 每个设备都有其自己的模型，数据批处理和优化器，从而分别执行正向，反向，参数更新。

这些先进的分布式并行框架(tensorflow pytoch)在基础结构是同构的情况下可以很好地工作，它们具有相同的内存限制，GPU容量和CPU吞吐量，并且具有较高的节点间通信速度。



 训练可分为四个主要步骤：正向步骤，反向传播，梯度更新和参数更新。 每个GPU都有各自的模型，优化器（opt）和学习率调度器（lr_sch）。 模型，优化器和学习率调度器均以相同的状态初始化，而不同的GPU处理训练数据的不同部分。 在前向传递中，加载分配给不同GPU的数据批，执行前向传递并计算单个损失函数。 来自不同GPU的不同损失函数被汇总以计算平均损失。 在反向传播步骤中，平均损耗被分配回每个GPU，以计算各个梯度。 然后应用渐变更新以将渐变传达回每个GPU。 最后，每个GPU的优化器和学习速率调度器分别执行参数更新。



 在这些典型情况下，**数据加载器是训练大型模型的瓶颈**。 我们的解决方案是将数据集分成碎片，以便可以并行加载数据集。 除了数据集大小和数据加载器速度之外，索引采样器还必须能够容纳具有不同数据类型的多个相关张量。 简而言之，我们的目标是找到一种通用的输入和输出机制，可以快速处理存储在多个分片上的任意相关张量。