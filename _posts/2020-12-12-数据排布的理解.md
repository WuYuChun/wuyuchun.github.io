# 数据排布的理解

在深度学习,很多时候,数据都是多维度的,如下图

![](/images/posts/2020-12-12-数据布局.png)

NCHW其实代表的是[WHCN]，第一个元素是000，第二个元素是沿着w方向的，即001，这样下去002 003，再接着呢就是沿着H方向，即004 005 006 007. ..这样到09后，沿C方向，轮到了020，之后021 022 ...一直到319，然后再沿N方向。

NHWC的话以此类推，代表的是[CWHN]，第一个元素是000，第二个沿C方向，即020，040, 060..一直到300，之后沿W方向，001 021 041 061.. .301..到了303后，沿H方向，即004 024 .。 。 304.。最后到了319，变成N方向，320,340....

**为了定义此4D张量中的数据在内存中的布局方式，我们需要定义如何通过偏移量函数将其映射到1D张量，该偏移量函数以逻辑索引（n，c，h，w）作为输入 并将地址位移返回到该值的位置：**
$$
offset : (int, int, int, int) --> int
$$
![](/images/posts/2020-12-12-数据排布02.png)

## NCHW

$$
offsetnchw(n, c, h, w) = n * CHW + c * HW + h * W + w
$$



## NHWC

$$
offsetnhwc(n, c, h, w) = n * HWC + h * WC + w * C + c
$$



## CHWN

$$
offsetchwn(n, c, h, w) = c * HWN + h * WN + w * N + n
$$



data_format 预设值为"NHWC。其中N 表示这批影象有几张，H 表示影象在竖直方向有多少画素，W 表示水平方向画素数，C 表示通道数（例如黑白影象的通道数C = 1，而RGB 彩色影象的通道数C = 3）。为了便于演示，我们后面作图均使用RGB 三通道影象。

![img](https://img-blog.csdnimg.cn/20181108214227390.png)

NCHW 中，C 排列在外层，每个通道内画素紧挨在一起，即 'RRRRRRGGGGGGBBBBBB' 这种形式。

NHWC 格式，C 排列在最内层，多个通道对应空间位置的画素紧挨在一起，即 'RGBRGBRGBRGBRGBRGB' 这种形式。

如果我们需要对影象做彩色转灰度计算，NCHW 计算过程如下：

![img](https://img-blog.csdnimg.cn/20181108214351573.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTg0NzExNQ==,size_16,color_FFFFFF,t_70)

即 R 通道所有画素值乘以 0.299，G 通道所有画素值乘以 0.587，B 通道所有画素值乘以 0.114，最后将三个通道结果相加得到灰度值。

相应地，NHWC 资料格式的彩色转灰度计算过程如下：

![img](https://img-blog.csdnimg.cn/20181108214443767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTg0NzExNQ==,size_16,color_FFFFFF,t_70)

输入资料分成多个(R, G, B) 画素组，每个画素组中R 通道画素值乘以0.299，G 通道画素值乘以0.587，B 通道画素值乘以0.114 后相加得到一个灰度输出画素。将多组结果拼接起来得到所有灰度输出画素。

 

以上使用两种资料格式进行 RGB -> 灰度计算的复杂度是相同的，**区别在于访存特性**。通过两张图对比可以发现，**NHWC 的访存区域性性更好**（每三个输入画素即可得到一个输出画素），**NCHW** 则必须等所有通道输入准备好才能得到最终输出结果，**需要占用较大的临时空间**。

在CNN 中常常见到1x1 卷积（例如：[用于移动和嵌入式视觉应用的MobileNets](https://mp.weixin.qq.com/s?__biz=MzI2MzYwNzUyNg==&mid=2247483973&idx=1&sn=b0b9aa4190f5ac9a34421beaa92eb932&chksm =eab807ccddcf8edaa798098c73b82ee35f4b22e159ddcd4ffb0d0cd6cae77a170a59a5c441e4&scene=21#wechat_redirect)），也是每个输入channel 乘一个权值，然后将所有channel 结果累加得到一个输出channel。如果使用 NHWC 资料格式，可以将卷积计算简化为矩阵乘计算，即 **1x1 卷积核实现了每个输入画素组到每个输出画素组的线性变换**。

 

TensorFlow 为什么选择 NHWC 格式作为预设格式？因为早期开发都是基于 CPU，使用 NHWC 比 NCHW 稍快一些（不难理解，NHWC 区域性性更好，cache（快取） 利用率高）。

NCHW 则是 Nvidia cuDNN 预设格式，使用 GPU 加速时用 NCHW 格式速度会更快（也有个别情况例外）。

 

**最佳实践**：设计网路时充分考虑两种格式，最好能灵活切换，在 GPU 上训练时使用 NCHW 格式，在 CPU 上做预测时使用 NHWC 格式。

在不同的硬体加速的情况下，选用的型别不同，在**intel GPU加速**的情况下，因为GPU对于影象的处理比较多，希望在访问同一个channel的画素是连续的，一般储存选用**NCHW**，这样在做CNN的时候，在访问记忆体的时候就是连续的了，比较方便。